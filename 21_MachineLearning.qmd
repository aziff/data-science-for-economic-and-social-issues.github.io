---
title: "Machine Learning Basics"
---

Machine learning (sometimes abbreviated to ML) is a class of statistical tools that help analysts find patterns in data. Basically, ML helps with prediction problems. Many core methods in ML are modern, with new developments everyday, but there are also fundamental components that have been around for a long time and now can be applied thanks to the larger availability of large datasets and computing power. Academic researchers use ML but it is also commonly used in the public and private sectors, and is a core part of recent advances in AI tools. We will go through some ML basics and see how to implement them in R. You will have practice doing so with an activity.

There are two broad categories of ML.

-   **Supervised learning**. Here there is an explicit outcome the analyst uses for prediction. For example, if the analyst observes historic house prices and housing characteristics and they want to predict future house prices. The outcome "house prices" is observed in some of the data and used in estimation.
-   **Unsupervised learning**. In this case, the outcome is not used for estimation. For example, the analyst may want to group the houses in the data into clusters of similar houses. They do not have any idea of what these clusters are and there is no indication of them in the data. ML tools can still provide a way to do this!

## Supervised Learning

### Linear Regression

You already are familiar with a method that can be used for prediction. Before, we were using linear regression to understand the relationship between the outcome variable ($Y$) and covariates ($X_1, X_2, \ldots$). Now, instead of focusing on interpreting the coefficients, we focus on minimizing **prediction error**. Specifically, linear regression minimizes the mean squared error (MSE) between the predicted and observed outcomes. If the fitted value is $\hat{Y}_i$, the MSE is

\begin{equation*}
\text{MSE} = \frac{1}{N} \sum_{i =1}^N (Y_i - \hat{Y}_i)^2.
\end{equation*}

Consider two datasets. One is the **training dataset**. We use this dataset to estimate the linear regression. From this dataset, we get the estimated coefficients. In the **testing dataset** we predict the outcome but using the estimated coefficients from the training dataset. Because we actually observe the outcome in tthe testing dataset as well, we can compute the squared prediction error. Suppose we have one dataset that we partition into training $(i = 1, 2, \ldots, N)$ and testing $(i = N+1, N+2, \ldots, M)$ datasets.

Let us see how we implement this in R. We will use a dataset from the `ggplot2` package called `diamonds`. This is a very clean dataset with the price and attribute of diamonds. We will try to predict the price of diamonds based on their attributes. You can think of this exercise extending to any situation where you want to predict prices.

```{r, eval = TRUE}
library(ggplot2)
library(dplyr)
library(magrittr)

data(diamonds)
summary(diamonds)

# Make the factors unordered (makes the regression easier)
diamonds <- diamonds %>%
  mutate(across(c(cut, color, clarity), ~ factor(.x, ordered = FALSE)))
```

Let us first define the training and testing datasets. We can randomly divide the data into the two datasets. We use 70% for the training dataset because we have a really big sample here.

```{r, eval = TRUE}


set.seed(470500)

# Create an ID variable
diamonds <- diamonds %>%
  mutate(id = 1:nrow(diamonds))

# Randomly select 70% to the training dataset
train_ids <- sample(diamonds$id, size = 0.7 * nrow(diamonds))

train <- diamonds[(diamonds$id %in% train_ids), ]
test <- diamonds[!(diamonds$id %in% train_ids), ]
```

Now, we can use the training dataset to estimate the regression of price on characteristics.

```{r, eval = TRUE}
train_lm <- lm(price ~ carat + cut + color + 
                 clarity + depth + table + x + y + z,
               data = diamonds)

summary(train_lm)
```

To test our model, we can use `predict()` to predict the price in the testing data with our estimated coefficients.

```{r, eval = TRUE}
test_pred <- predict(train_lm, newdata = test)
summary(test_pred)

(test$price - test_pred)^2 %>%
  mean()
```

This does not mean too much to us unless we compare to another model.

```{r, eval = TRUE}
train_lm2 <- lm(price ~ carat + cut + color + clarity,
               data = diamonds)

summary(train_lm2)

test_pred2 <- predict(train_lm2, newdata = test)

(test$price - test_pred2)^2 %>%
  mean()

```

Comparing the MSE between these two models, the first model has better predictive power. But what about other models? What about all the possible combinations of covariates? What about adding polynomials of those covariates? ML methods can help with that too (below).

### Logistic Regression

We can apply the same logic when the outcome is binary, but we use logistic regression. The model predicts the probability of success, when the outcome is one. Suppose we want to predict if a diamond has a price below someone's budget of \$1,000.

```{r, eval = TRUE}
train <- train %>%
  mutate(inbudget = (price <= 1000))

test <- test %>%
  mutate(inbudget = (price <= 1000))


# Train
ib_train_lm <- glm(inbudget ~ carat + cut + color + clarity + depth + table + price + x + y + z,
    data = train,
    family = "binomial")

summary(ib_train_lm)

# Predict in testing data
ib_test_pred <- predict(ib_train_lm, newdata = test, type = "response")

summary(ib_test_pred)

ib_test_pred_binary <- (ib_test_pred > 0.5)

# Confusion matrix 
table(observed = test$inbudget, predicted = ib_test_pred_binary)

# Accuracy
mean(test$inbudget == ib_test_pred_binary)
```

### LASSO

Under that intuition, ML methods can help us find a model (what to put on the right-hand side) for the best prediction (what is on the left-hand side). We will go through the workflow of LASSO, which is one method you can use.

1.  Split the cleaned data into training and testing. We can use the `tidymodels` library to split the data into training and testing datsaets.

```{r, eval = TRUE}
library(tidymodels)

d_split <- initial_split(diamonds) # Creates the split (default is 75/25)

d_train <- training(d_split) # Define training data based on split
d_test <- testing(d_split) # Definet testing data based on split

```

2.  Build a "recipe." This tells R what is the outcome variable and helps format the possible predictors.

```{r, eval = TRUE}

recipe(price ~ ., data = d_train) %>%
  summary()

# Account for ID variable and factor variables

d_rec <- recipe(price ~ ., data = d_train) %>%
  step_dummy(all_nominal_predictors())  %>% 
  update_role(id, new_role = "ID") 

summary(d_rec)

```

3.  "Prep" the recipe. Given the recipe, R then puts together the data inputs. You can explore the outputs of recipe and prep to better understand these steps.

```{r, eval = TRUE}
d_prep <- d_rec %>%
  prep()

summary(d_rec)
```

4.  Specify parameters of the model. The basic idea of model selection is that we have an objective function with a penalty term. This penalty term depends on the number of predictors. If we include more predictors, the penalty increases. This prevents us just using every predictor available. Then the question is what parameters do we choose for our objective function? Here is the general format for LASSO:

\begin{equation}
\arg\min_{\beta} \sum_{i = 1}^N (Y_i - \beta ' X_i)^2 + \lambda \sum_{k=1}^K \vert \beta_k \vert
\end{equation}

We can dissect this term by term. The first term in Equation (1) is the usual OLS estimator. The second term is the penalty. The parameter $\lambda$ is the first parameter we need to set. This itself is called the penalty term. Large values of $\lambda$ results in a more parsimonious model (fewer predictors) because the penalty is large. If $\lambda$ is zero, then the model is the same as OLS. In practice, they can cover a large range. See methods in cross-validation to select $\lambda$ in a more principled way than just setting it. Then, there is the function on the parameters $\beta$. Suppose there are $K$ possible predictors. LASSO involves taking the absolute value of the parameters. Other methods, like Ridge regression (not in these notes), use other functions rather than the absolute value (e.g., square the parameters). This function is the second part that the analyst sets.

In the code below, the penalty argument is $\lambda$ and the mixture argument set to 1 means that we are using LASSO. We use `set_engine()` to tell R which algorithm to use. Because we are using LASSO, we use "glmnet." This also works for Ridge or similar methods. Other engines include "lm" for OLS or "keras" for Neural Nets.

```{r, eval = TRUE}

d_lasso <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")
```

5.  Create the workflow. This puts together the model and the recipe.

```{r, eval = TRUE}
d_wf <- workflow() %>%
  add_recipe(d_rec)
```

6.  Fit the model and investigate the coefficients. If a coefficient is 0, that means that LASSO dropped it because it did not add sufficient predictive power. The interpretation of the coefficients is the same as for standard linear regression.

```{r, eval = TRUE}

# Run LASSO!
d_fit <- d_wf %>%
  add_model(d_lasso) %>%
  fit(data = d_train)

# Check out coefficients
d_fit %>%
  extract_fit_parsnip() %>%
  tidy() 
```

7.  Apply to testing data and calculate the MSE (or other metric). If we were to use other models or tuning parameters, then this MSE can be used to compare the models.

```{r, eval = TRUE}

# Apply recipe and model to test data
d_results <- predict(d_fit, new_data = d_test) %>%
  bind_cols(d_test)

# Assess MSE
mean((d_results$.pred - d_results$price)^2)
```

Here is a visualization of how the coefficients change with different parameters $\lambda$.

```{r, eval = TRUE, fig.alt="Line plot showing the paths of LASSO regression coefficients for different terms as log(Lambda) increases. Each term is represented by a colored line. The x-axis shows log(Lambda); the y-axis shows the coefficient value. The plot title is `LASSO Coefficient Paths (Workflow Model)`. A legend for the terms appears at the bottom."}

# Raw glmnet object
glmnet_fit <- d_fit %>%
  extract_fit_engine() 

# Clean coefficient paths
# Install broom!
library(broom)
coefs_long <- tidy(glmnet_fit, return_zeros = TRUE, matrix = "beta")  

# Plot
ggplot(coefs_long, aes(x = log(lambda), y = estimate, color = term)) +
  geom_line(alpha = 0.8) +
  labs(title = "LASSO Coefficient Paths (Workflow Model)",
       x = "log(Lambda)",
       y = "Coefficient Value") +
  theme_minimal() +
  theme(legend.position = "bottom")

```

### Other Tools

Decision trees and the more general random forest methods provide other ways to predict. There are various packages to implement these methods in R, including `rpart`. One issue that arises in all these methods is overfitting. This means that the model performs very well in the training data, but not so well in the testing data. Cross-validation is a tool that addresses this issue. The basic idea of cross-validation is that the split of the data is not so simplistic into training/testing. Instead, the data is repeatedly divided into multiple subsets (or “folds”), so that each part of the data is used for both training and testing at different stages. For example, in 5-fold cross-validation, the data is split into five parts: the model is trained on four and tested on the fifth, and this process repeats five times. The average performance across all folds gives a more reliable estimate of how the model is likely to perform on new, unseen data.

## Unsupervised Learning

A common application of unsupervised learning is "clustering" or "pattern recognition." The most foundational approach is the k-means algorithm, which attempts to group observations into $k$ clusters based on similarity. The intuition behind k-means is simple: the algorithm assigns each observation to the nearest cluster center, then updates the cluster centers to be the average of the observations assigned to them. This process repeats until the assignments stop changing. The result is a division of the data into groups that are internally similar but distinct from one another. While it is fast and widely used, it does require the user to specify the number of clusters in advance. K-means and similar tools are useful for summarizing large datasets, detecting outliers, or identifying natural groupings when no outcome variable is available.

```{r, eval = TRUE, fig.alt="Scatter plot of iris dataset clustered with k-means (3 clusters). Points are plotted by petal length (x-axis) and petal width (y-axis). Each point is colored by its cluster label. No species labels are shown."}
# Load built-in iris dataset
data(iris)
head(iris)

# Remove the species label for unsupervised learning
iris_data <- iris[, 1:4]

# Run k-means with 3 clusters (we know there are 3 species)
set.seed(470500)
iris_kmeans <- kmeans(iris_data, centers = 3, nstart = 20)

# Add the cluster labels to the original data
iris_clustered <- iris %>%
  mutate(cluster = as.factor(iris_kmeans$cluster))

# Plot the clusters (e.g., using petal length vs. width)
library(ggplot2)
ggplot(iris_clustered, aes(x = Petal.Length, y = Petal.Width, color = cluster)) +
  geom_point() 
```

## Further Reading

There are so many great resources online to learn ML methods. @athey_machine_2019 provides a nice overview on the classes of methods. Text analysis is another class of machne learning that is useful. You can use the libraries `tidytext` or `tm` for that.
