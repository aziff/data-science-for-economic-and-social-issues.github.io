[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Economic and Social Issues",
    "section": "",
    "text": "Preface\nThis website contains the notes for ECON 470: Data Science for Economic and Social Issues taught at Texas A&M University. This course includes instruction in:\n\nusing R, and by extension computer programming tenants;\nthe responsible use and management of data including data cleaning and description;\nmethods in causal analysis that economists, policymakers, and decision-makers rely on to draw conclusions, not only about the world, but also about how to make changes the world.\n\nThese notes are designed to be used alongside other instructional material available on Canvas and Charles Wheelan’s book, Naked Statistics: Stripping the dread from the data.\nIn preparing these materials, I referred to several online resources. This includes both for content and for practice exercise ideas. At the end of each chapter, I list the resources I used. I thank Xinqi Xu for creating this website and ensuring it meets accessibility requirements. All errors are my own. Please email me (aziff “at” tamu.edu) so that I can rectify them.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_Installation.html",
    "href": "01_Installation.html",
    "title": "1  Quick Start: Installation Instructions",
    "section": "",
    "text": "1.1 R\nConfirm that installation was successful by opening the R program.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Start: Installation Instructions</span>"
    ]
  },
  {
    "objectID": "01_Installation.html#r",
    "href": "01_Installation.html#r",
    "title": "1  Quick Start: Installation Instructions",
    "section": "",
    "text": "R is available for free from the Comprehensive R Archive Network (CRAN). Go to this website.\nClick the download link that corresponds to your operating system.\n\nWindows Click “Base” and download the installer. Follow the installation instructions.\nMac Select the version that corresponds with the version of your operating system and chip type (silicon vs. Intel). Follow the installation instructions.\nLinux Click on your distribution. Follow the installation instructions.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Start: Installation Instructions</span>"
    ]
  },
  {
    "objectID": "01_Installation.html#rstudio",
    "href": "01_Installation.html#rstudio",
    "title": "1  Quick Start: Installation Instructions",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nWhile the R application is perfectly functional on its own, it is not as convenient as other applications. Specifically, we are interested in a more user-friendly Graphical User Interface (GUI). The GUI comprises the menus available to users for point-and-click operations. RStudio is a free software with an accessible GUI.\n\nNavigate to the RStudio IDE webpage.\nChoose the free version of RStudio Desktop.\nSelect your operating system and follow the installation instructions.\n\nConfirm that the installation was successful by opening RStudio.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Start: Installation Instructions</span>"
    ]
  },
  {
    "objectID": "01_Installation.html#aggie-virtual-desktop",
    "href": "01_Installation.html#aggie-virtual-desktop",
    "title": "1  Quick Start: Installation Instructions",
    "section": "1.3 Aggie Virtual Desktop",
    "text": "1.3 Aggie Virtual Desktop\nAggie Virtual Desktop, also known as the Virtual Open Access Lab (VOAL) provides access to software, including R. This is an option if you cannot download R or RStudio onto your computer.\n\nNavigate to the Aggie Virtual Desktop login page.\nYou can either download the VMware Horizon Client or access VMware Horizon via an internet browser.\nClick on VOAL. Make sure you allow for pop-up windows.\nYou can access R and RStudio through the virtual machine (VM).",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Start: Installation Instructions</span>"
    ]
  },
  {
    "objectID": "02_Setup.html",
    "href": "02_Setup.html",
    "title": "2  Getting Setup",
    "section": "",
    "text": "2.1 About R\nR is a programming language and work environment for data analysis. R is especially well-suited for data work and graphics, although it provides the base for many types of analyses. It is widely used in economics across sectors. The benefits of R are as follows.\nDespite these benefits, there are a few downsides that any user must consider.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Setup</span>"
    ]
  },
  {
    "objectID": "02_Setup.html#about-r",
    "href": "02_Setup.html#about-r",
    "title": "2  Getting Setup",
    "section": "",
    "text": "Free. Self-explanatory.\nOpen source. Any user can develop functions and packages. These packages are freely available, resulting in the constant expansion and improvement of R and its capabilities.\nFlexible. R incorporates many of the strengths of the alternatives. It can be used to clean, analyze, and visualize data. With the aid of packages, it can also be used for optimization, web scrapping, network analysis, text analysis, machine learning, and many other tasks.\nFast. R is designed for efficiency, especially for certain data structures. Even a novice user can write very efficient code.\nRelatively simple. Even though R has a steeper learning curve than STATA, it is much more straightforward than C.\n\n\n\nDifficulty. Is the user committed to learning how to use R to maximize its benefits? Often, code needs to be read by those other than the author. Are these other individuals equipped to understand R syntax and semantics? 1\n\n\nErrors. The user-written packages are heterogeneous in quality. Some may have poor documentation or even mistakes. Sometimes, authors do not update their code or respond to error reports, resulting in commands that do not work for more recent versions of R.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Setup</span>"
    ]
  },
  {
    "objectID": "02_Setup.html#installation",
    "href": "02_Setup.html#installation",
    "title": "2  Getting Setup",
    "section": "2.2 Installation",
    "text": "2.2 Installation\n\n2.2.1 R\n\nR is available for free from the Comprehensive R Archive Network (CRAN). Go to this website.\nClick the download link that corresponds to your operating system.\n\nWindows: Click “Base” and download the installer. Follow the installation instructions.\nMac: Select the version that corresponds with the version of your operating system and chip type (silicon vs. Intel). Follow the installation instructions.\nLinux: Click on your distribution. Follow the installation instructions.\n\n\nConfirm that installation was successful by opening the R program. When you open R, you will see a console that looks as figure below, or at least very similar. Every time you open R, a new session begins. At the start of each session, the console prints some information, like the version of R and some basic help commands. The line with &gt; is the command line. This is where you can type commands. Try typing print(\"Hello world!\") and press Enter or Return.\n\n\n\n\n\nR Console\n\n\n\n\n\n\n2.2.2 RStudio\nWhile the R application is perfectly functional on its own, it is not as convenient as other applications. Specifically, we are interested in a more user-friendly Graphical User Interface (GUI). The GUI comprises the menus available to users for point-and-click operations. RStudio is a free software with an accessible GUI.\n\nNavigate to the RStudio IDE webpage.\nChoose the free version of RStudio Desktop.\nSelect your operating system and follow the installation instructions.\n\nConfirm that the installation was successful by opening RStudio. The workspace should look very similar to Figure 2.\n\n\n2.2.3 Aggie Virtual Desktop\nAggie Virtual Desktop, also known as the Virtual Open Access Lab (VOAL) provides access to software, including R. This is an option if you cannot download R or RStudio onto your computer.\n\nNavigate to the Aggie Virtual Desktop login page.\nYou can either download the VMware Horizon Client or access VMware Horizon via an internet browser.\nClick on VOAL. Make sure you allow for pop-up windows.\nYou can access R and RStudio through the virtual machine (VM).",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Setup</span>"
    ]
  },
  {
    "objectID": "02_Setup.html#using-rstudio",
    "href": "02_Setup.html#using-rstudio",
    "title": "2  Getting Setup",
    "section": "2.3 Using RStudio",
    "text": "2.3 Using RStudio\nThere are three segments of the initial RStudio workspace: the console (left segment), which is the same as the basic R console, the workspace environment (upper right segment), and miscellaneous displays (lower right segment).\n\n\n\n\n\nRStudio Opening Console\n\n\n\n\nThere is one more segment that is crucial: the script editor. Although you can type commands directly into the console, it is much easier and better practice to execute code from scripts. To open the script editor with an empty file, click File &gt; New File &gt; R Script. The file extension is .R. With all four segments of the RStudio workspace open (Figure below), we can now go through each one in detail.\n\n\n\n\n\nRStudio Full Workspace\n\n\n\n\n\n2.3.1 Console\nJust like in the R application, you can type commands here directly at the angle bracket, &gt;. Try typing print(\"Hello world!\"). The output is printed in the same window. If you quit RStudio and reopen it, the commands you typed in the console will not be saved. For this reason, the console should be used to print output and test commands, not to write your entire analysis.\n\n\n2.3.2 Script Editor\nThe script editor is where you can write, edit, and save your code. The code can be executed with the output printed to the console. In your blank R script, type the following lines.\n\n# Title: R_00_Practice.R\n\ngreeting &lt;- \"Hello world!\"\nprint(greeting)\n\nExecute the code by highlighting all lines. Then click Run, circled in Figure 4.\n\n\n\n\n\nRStudio Run Button\n\n\n\n\n\n\n2.3.3 Workspace\nOnce you run your code, you will see a change in the workspace environment. Any object, structure, or function that is created in your code will be listed there. To remove an object, structure, or function, use the command rm(). It is good practice to clear the workspace before any code is executed. Add rm(list = ls()) to the top of your code to clear everything from the workspace. Note that the command ls() lists all of the contents of the workspace.\n\n# Title: R_00_Practice.R\n\nrm(list = ls())\n\ngreeting &lt;- \"Hello world!\"\nprint(greeting)\n\nThe History tab keeps track of recently used commands. The Connections tab is useful for SQL integration (outside the scope of this class). The Tutorial tab has instructions to access some tutorials.\n\n\n2.3.4 Miscellaneous Displays\nThe lower right segment contains several miscellaneous displays, each of which are useful for different steps of the R workflow.\n\n2.3.4.1 Files\nThe first display, Files, lists the folders of your computer. Clicking on the folders allows you to explore the file structure of your computer. Navigate to the folder in which you will save your code. At the top of the segment, you can see the file path to get there. This is useful when you need to change your working directory (the file system of your computer). In the console, experiment with the commands getwd() to print the current directory and setwd() to navigate the console to the directory in which you want to save your materials for this class. Add this to your R script. You will have a different filepath than the generic one below.\n\n# Title: R_00_Practice.R\n\nrm(list = ls())\nsetwd(\"path/to/file\")\n\ngreeting &lt;- \"Hello world!\"\nprint(greeting)\n\nSave the file with the name R_00_Practice.R to your current directory. This is done by clicking File &gt; Save.\n\n\n2.3.4.2 Plots\nThe Plots window is useful when you are creating graphs, as they show up directly. The graphics demo, typed directly into the console, showcases some examples of R’s graphing capabilities.\n\ndemo(graphics)\n\n\n\n2.3.4.3 Packages\nThe Packages tab displays all the external libraries on your computer. R is open source, meaning that anyone can publish functions to expand upon what is available for R users. The shareable code and documentation is called a package or library. The advantage of packages is that there may already be an implementation of a problem you aim to solve. For example, if you want to implement a method from an econometrics paper, a package for that method may already exist. If the package is well done, it may be more efficient and accurate than your own implementation.\nThe Comprehensive R Archive Network (CRAN) is the official repository of R packages. Most of the packages you use will be published on CRAN. The function to install a package is install.packages(). Here is an example of how to install a package.\n\ninstall.packages(\"ggplot2\")\n\nOnce you install a package, the code and documentation are stored on your computer. You do not need to re-install the package every R session. However, the package does need to be loaded every R session.\n\nlibrary(ggplot2)\n\nIf you want to specify which package the function comes from, you can use two colons, ::. Using this format allows you to skip loading the package, although the package does need to be installed. You may see published code or answers on Stack Exchange that use this format. Here is an example. Sometimes, packages are only available on GitHub or you will want to use a more recent version that the package authors have not yet published to CRAN. To install these packages from GitHub, we need to access the install_github() function from the package devtools.\n\ndevtools::install_github(\"https://github.com/tidyverse/ggplot2\")\n\nIt is common that different packages define functions that have the same name. If you call one of these functions, R will default to the package that was loaded later (type search() in the command line to see the order in which packages were loaded). To notify you of this issue, R throws a warning message upon loading the package. The message lists the objects that are masked and the packages with the conflict.\n\ninstall.packages(\"dplyr\")\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nThe double colon, :: can be used to solve this issue. If you want to use one of these functions, you can specify which package R should reference. This is not always necessary, but it can help avoid errors if R is referencing a different package than what you expect.\n\nbase::union(1:10, 7:12)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nGenerally, it is good practice to load the packages you will use at the top of your R script. Edit your R script to load ggplot2.\n\n# Title: R_00_Practice.R\n\nrm(list = ls())\nsetwd(\"path/to/file\")\n\nlibrary(ggplot2)\n\ngreeting &lt;- \"Hello world!\"\nprint(greeting)\n\n\n\n2.3.4.4 Help\nThe Help tab displays the documentation for functions. In the console, type the following.\n\nhelp(print) \nhelp(\"print\")\n\n?print\n\nYou will see an explanation of the function, a description of the usage, a list of the arguments, other details, references, and examples. As you are learning, it is great practice to get accustomed to accessing the documentation of commands; and RStudio makes it easy! While you will need to search online for specific help and troubleshooting, the ? command and the integrated Help window make the process of learning new commands as simple as possible.\n\n\n2.3.4.5 Viewer\nThe Viewer tab is helpful when creating websites and applications that use R input and output (Shiny). This is outside the scope of the class.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Setup</span>"
    ]
  },
  {
    "objectID": "02_Setup.html#practice-exercises",
    "href": "02_Setup.html#practice-exercises",
    "title": "2  Getting Setup",
    "section": "2.4 Practice Exercises",
    "text": "2.4 Practice Exercises\nCheck your understanding on the above materials with these exercises.\n\nThe top of the script has a commented line: # Title: R_00_Practice.R. The # signals to R not to execute the code. Add other lines to the header of your script for the author and date.\nAccess the documentation for union() from the console. Familiarize yourself with the structure of the help documentation. Take note of the following sections: Description, Usage, Arguments, Value, Examples.\nSeveral packages are combined in the collection tidyverse. These are all useful for data science, and we will be using functions from several of them in this class. Install this package. It takes a while!\nThe collection tidyverse contains ggplot2. Delete library(ggplot2) in R_00_Practice.R and load tidyverse instead.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Setup</span>"
    ]
  },
  {
    "objectID": "02_Setup.html#further-reading",
    "href": "02_Setup.html#further-reading",
    "title": "2  Getting Setup",
    "section": "2.5 Further Reading",
    "text": "2.5 Further Reading\nThis website goes through the most important features of the script editor. If you have never used RStudio before, skim through these features. Test out the keyboard shortcut to execute code.\nThe information above primarily comes from chapters 2 and 3.2-3.4 of Boehmke (2016).\n\n2.5.1 References\n\n\n\n\n\nBoehmke, Bradley C. 2016. Data Wrangling with R. Use R! Springer. https://link.springer.com/book/10.1007/978-3-319-45599-0.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Setup</span>"
    ]
  },
  {
    "objectID": "02_Setup.html#footnotes",
    "href": "02_Setup.html#footnotes",
    "title": "2  Getting Setup",
    "section": "",
    "text": "The syntax of programming languages comprises the symbols, words, and structure of the code. The semantics of programming languages comprise the meaning behind the symbols, words, and structure.↩︎",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Setup</span>"
    ]
  },
  {
    "objectID": "03_Command_Line_and_Git.html",
    "href": "03_Command_Line_and_Git.html",
    "title": "3  Command Line and Git",
    "section": "",
    "text": "3.1 Command Line\nThe command line is a direct way to type instructions for your computer to immediately execute. Comfort with some basics can help you more easily use R, Git, and any other language or software.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command Line and Git</span>"
    ]
  },
  {
    "objectID": "03_Command_Line_and_Git.html#command-line",
    "href": "03_Command_Line_and_Git.html#command-line",
    "title": "3  Command Line and Git",
    "section": "",
    "text": "If you are a Windows user, open GitBash. There are other options, but practicing in GitBash (or your Git application of choice) will help you learn Git later on.\nIf you are a Mac user, open Terminal.\nIf you are Linux user, open Terminal or your system’s shell application.\n\n\n3.1.1 Navigating Directories\nPrint the working directory with pwd.\n\npwd\n\nList files in the current directory with ls. Sometimes there are hidden files (e.g., .gitignore). Add the -a option to see those as well.\n\nls\nls -a\n\nChange directory with cd. To change to the root directory, specify /.\n\ncd /\n\nTo change to the home directory, specify ~.\n\ncd \ncd ~\n\nYou can change the directory to a specific path.\n\ncd path/to/file\n\nUse .. to indicate one level back.\n\ncd ..\ncd ../../relative/path/to/file\n\nNote that nothing is printed after cd unless you encounter an error. You can always check where you are in your file system with pwd. If you are using a Windows machine, you may need to use the backslash instead for the file paths.\nYou can create a folder in the current directory using mkdir. The flag -p will create dir/dir1 if those directories do not already exist.\n\nmkdir dir\nmkdir dir1 dir2 dir3\nmkdir -p dir/dir1/dir1a\n\nYou can move a file to another directory using mv.\n\nmv file.txt path/to/folder/text.txt\n\nThe command mv can also be used to change the name of a file.\n\nmv file.txt newname.txt\n\nYou can copy a file using cp.\n\ncp file.txt file_copy.txt\n\nWith caution, you can delete files using rm.\n\nrm file.txt\n\nBe very careful with this command. It is easy to accidentally erase all the files in your computer with the command (DO NOT TYPE THIS!!!!) rm -r * (DO NOT TYPE THIS!!!!). It is good practice to check which directory you are in (pwd) and slowly remove files and folders one at a time rather than deleting them in batches.\nSee Canonical Ltd. (2021) for more commands and details. The ones listed above are more than sufficient for this class.\n\n\n3.1.2 Vim\nVim is a text editor accessible through the command line. It is often the default text editor for Window’s GitBash, Apple’s Terminal, and Linux’s shell. There are other options (Nano, Emacs, Notepad++), and you are free to choose the one that works best for you. I will highlight the basics of Vim as that is the more universal option.\nOpen a read-only version of a file in Vim. If the file does not exist, Vim will create an empty file.\n\nvim file.txt\n\nTo edit the file, type i and begin editing. To exit without saving, esc + :q. To exit with saving (write and quit), esc + :wq.\n\n\n3.1.3 Naming Conventions\nOnly use letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-) in your file and folder names. This will make navigating your file system from the command line much smoother. It is also good practice to have a consistent naming system.\n\n\n\nName\nExample\n\n\n\n\nDash case\nmy-file.txt\n\n\nCamelCase\nmyFile.txt, MyFile.txt\n\n\nSnake case\nmy_file.txt\n\n\nFlat case\nmyfile.txt\n\n\nUPPERCASE\nMYFILE.txt\n\n\n\n\n\n3.1.4 Practice Exercises\n\nOpen the command line on your computer. Print your working directory and list the files there.\nNavigate to where you want to have a folder with the work for this class (e.g., Documents).\nCreate a folder called temp. Navigate inside the temp folder.\nWith one line, create two folders inside the temp folder called temp1 and temp2.\nCreate a file called test.txt and save it to temp/temp1. Challenge yourself by creating this file from the command line using Vim or another editor. Type \"Hello world!\" and save the file.\nMove test.txt from temp1 to temp2.\nMake a copy of test.txt called test-copy.txt. Save it in temp1.\nDelete the file test.txt.\nConfirm you are in the path from exercise 2. Now create the folder in which you will save all your class material. Call it whatever you would like.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command Line and Git</span>"
    ]
  },
  {
    "objectID": "03_Command_Line_and_Git.html#git",
    "href": "03_Command_Line_and_Git.html#git",
    "title": "3  Command Line and Git",
    "section": "3.2 Git",
    "text": "3.2 Git\nGit is a distributed version control system. It documents the complete history of a project, including an archive of changes and previous versions of files. Economists commonly use Git across sectors.\nGit has several advantages over other formal and casual methods of version control. It is a free tool that centrally stores project files with a record of all changes, who made the changes, and why those changes were made. Each change of a file is documented, and once entered in the database, cannot be changed. The disadvantage is that it takes some time and practice to learn how to use Git. This handout describes Git and introduces the essential Git commands.\n\n3.2.1 Setting Up Git\n\n3.2.1.1 Installation\nGit must be installed on your local machine, i.e., your personal computer. This section explains the most flexible way to use Git, which is through the command line (a direct place for you to type instructions to your computer). There are many GUIs (graphical user interfaces) available as well. Once you understand how to use Git on the command line, you should be able to learn any GUIs or other Git-related tools easily.\nMac computers should already have Git installed. You will access Git through the Terminal application (this application is how Mac users access the command line). Open Terminal and make sure you have Git installed by typing the below. It is fine if your version is different than mine.\n\ngit --version\n\ngit version 2.39.5 (Apple Git-154)\n\n\nIf it is not installed, you will get instructions on how to install it.\nMachines with Windows operating systems require more setup. Git Bash allows you to use Git in the command line the same way a Mac or Linux user would. The below steps are copied from Matoso (2019).\n\nDownload the Git installer.\nExecute the file with the default options in the “Select Components” page.\nThe option “Windows Explorer integration &gt; Context menu entries” makes it possible to right click on a folder and open Git Bash to that folder’s location.\nIn “Adjusting your path environment” select “Use Git Bash only.”\nSelect “Checkout as-is, commit Unix-style line endings.” This helps prevent compatibility issues due to different line endings with files from other operating systems.\nClick “Next” and then “Finish.”\n\nYou can also use another program like PowerShell or Command Prompt. Git Bash seems to be the preferred application.\nSee page 17 of Chacon and Straub (2020) for installation instructions for RPM-based and Debian-based distributions (Linux).\n\n\n3.2.1.2 Configuration\nOnce Git is installed, there are several options to configure options available through the config command. You will need to set your user name and email.\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email youremail\n\nThe global option means that this configuration is the same regardless of the project. You can always check your values.\n\ngit config user.name\ngit config user.email\n\nGitHub recently changed its security so that you require a Personal Access Token (PAT) to use Git from the command line.\n\nNavigate to the settings of your GitHub account.\nOn the left-hand side menu, scroll all the way to the bottom and click on “Developer Settings.”\nClick on “Personal access tokens.”\nClick on “Generate new token.” You may have to sign in with your GitHub credentials.\nGive the token a name, set the expiration for the appropriate amount of time, and select all scopes. Many of these are applicable to repositories that publish packages or software, but it is easier just to select all of them.\nClick on “Generate token.”\nYour token will appear on the screen. Copy it into a location where you can easily access it. This can be some note-taking software or a text file on your computer. Once you exit this screen, you will not be able to see this token again.\n\n\n\n\n3.2.2 Characteristics of Git\n\nInstead of storing changes to files across versions, Git stores snapshots of the project over time. The below figures from Chacon and Straub (2020) provide a visualization of what saving changes would look like and how Git stores data. Effectively, a picture of the whole project is taken each time you “save” your project.\n\n\n\n\n\n\nStoring Changes\n\n\n\n\n\n\n\n\n\nStoring Snapshots\n\n\n\n\n\nAll the project files and the project’s history are stored locally on your computer. This minimizes reliance on connecting to an external server, aiding speed and stability. It also allows one to keep track of changes even without an internet connection.\nOnce “saved,” a project’s history cannot be changed. This gives Git integrity; You cannot accidentally corrupt or lose information without a warning or error message. Everything in a Git database is checksummed and given an SHA-1 hash, basically a long string of numbers and letters. Here is an example SHA-1 hash: 6c658d1e96acb313eed5e9d13d723275b6479d04.\n\n\n\n3.2.3 Repositories\nA repository is a folder whose contents are under Git’s version control. You can turn any folder on your computer into a git repository. See page 245 of Chacon and Straub (2020) for instructions on this. In this class, we will talk about how to set up a Git repository on an external server (in our case, GitHub) and clone it on you local machine. Using a website like GitHub as a server for your Git-controlled repository has its advantages. The ability to control who can access your repository helps streamline collaborations. There are many interactive features on the website to help keep track of issues and changes for the project. Even without collaborators, using GitHub provides a natural backup for your own work.\nOnce the repository is setup on GitHub, you need to clone it to your local machine. Navigate to the directory where you want to store the repository and clone it there.1\n\ngit clone https://github.com/aziff/example.git\n\nThis essentially copies the entire repository, including all tracked changes, to your local machine. There is a hidden directory, .git, that stores all the version control information. The presence of this directory is what makes a folder a repository tracked by Git. To clone a repostiory, you will usually need to enter your PAT.\n\n\n3.2.4 Save and Track Changes\nEach file in the repository can be tracked or untracked. If a file is tracked, then it is backed up in the server’s repository and changes can be recorded. If a file is untracked, then Git does not “know” about it and any changes made to it are not stored.\n\nTracked Files. A tracked file can be in three states: unmodified, modified, or staged. If a file is unmodified, then the copy of the file on your local machine is identical to the last snapshot of the file in the repository. Once you make changes to the file, it is modified. The file becomes staged once the modifications are recorded to the repository. Figure 3 is from Chacon and Straub (2020) and visualizes these three states.\n\n\n\n\n\n\nStoring Snapshots\n\n\n\n\n\nUntracked Files. There are certain files that should remain untracked. Because Git takes a snapshot of the whole repository, keeping track of very large or complex files is burdensome. Generally speaking, do not track datasets, PDFs, images, auxiliary files, or Microsoft Office files (more details on this below).\n\n\n\n3.2.5 Basic Git Workflow\nChecking the status of your local repository is an important tool to help you navigate the Git workflow.\n\ngit status\n\nIf you have untracked files, they will be listed as such after typing git status. To track both new files and update the tracking on modified files, you first need to add them.\n\ngit add file.txt\n\nIf you run git status again, you will now see that there is a new file with “changes to be committed.’’ This means that the file is staged. You can also add all the contents of a folder.\n\ngit add dir1\n\nHere are some shortcuts. The period (.) stages every downstream change. The asterisk (*) is a placeholder.\n\ngit add .\ngit add *.txt\n\nAgain, you can check the status of the repository to ensure you staged what you wanted. You can use the -short (or -s) flag if you want to view a more condensed output. To view more details on what specifically was changed in the unstaged file.\n\ngit status\ngit status -short\ngit status -s\ngit diff\n\nYou can modify a file further even once it is staged. You will just need to add the file again to keep track of the additional changes.\nOnce you have staged everything you want, the next step is to commit your changes. Any file that is staged will be included in the commit.\n\ngit commit\n\nThis will open a text editor with information about the commit, including what files were newly staged or modified. You can add a message with additional information before quitting the text editor. Exiting the text editor induces the commit to be created with the commit message. It is often convenient to write the commit message inline.\n\ngit commit -m \"Initial commit.\"\n\nAlready, this commit provides a record of the current version of the repository. This is useful even without saving on GitHub. However, GitHub is useful for collaboration and to back-up your local machine. To “save” to GitHub, you will need to push your commits.\n\ngit push\n\n# Specifying the remote (origin) and the branch (master)\ngit push origin master\n\nThis push will be rejected if anyone else on the project has pushed work that you have not yet integrated into your files. It is thus good practice to update your files before changing anything.\n\ngit pull\n\nThis is good practice even if you are working independently across multiple machines (e.g., you work on your laptop and in the Econ cluster). To review, the workflow should be as follows.\n\nUpdate your local repository: git pull\nMake your changes\nStage your changes: git add\nCommit your changes: git commit -m \"Commit message.\"\nPush your changes: git push\n\nThe command git log allows you to view the commit history of your repository. See Chacon and Straub (2020) for details on how to format the output of this. Another option is to view the repository on GitHub’s website.\n\ngit log\n\n\n\n3.2.6 Deleting and Renaming Tracked Files\nTo delete tracked files, use git rm. This removes the file from the repository and from the staging area. Once you commit this change, the file will no longer be tracked.\n\ngit rm results.txt\n\nIf the file has been modified or it is already staged, you need to add the force flag, -f.\n\ngit rm -f results.txt\n\nTo remove files from the staging area without deleting the files entirely, use the cached flag. This is an issue that often arises, especially if you forgot to update your .gitignore file (more on this below).\n\ngit rm --cached results.txt\n\nIf you change the location of a file or change its name, git will view this as deleting the original file and creating a new file with the new name. You can use git mv to specify this directly. It is not strictly necessary, but is a convenience function that may be helpful when you want a specific commit message to go with changing a file’s location or name.\n\ngit mv original-name.txt new-name.txt\n\n\n\n3.2.7 Should all files be tracked?\nIn short: no. Keeping track of every change for certain types of files is burdensome and can greatly impede on your ability to use git efficiently within a repository. There may also be auxiliary files that you never reference anyway, such as log or output files that are automatically generated. Here is a list of some file types that are generally a good idea to avoid tracking.2\n\nOperating system files: Thumbs.db, .DS_Store\nApplication files: .Rhistory, .Rapp.history, .RData\nData files: .xlsx, .csv, .dta\nBinary files: .pdf, .docx, .pptx, image files\n\nYou can create a file inside your repository to instruct git what kind of files should never be tracked. This file is called .gitignore. The period at the beginning means that it is a hidden file (it will not show up in your file viewer unless you have set your options to view hidden files). To create a .gitignore file, navigate to your repository and open a new file with Vim.\n\nvim .gitignore\n\nIn the .gitignore file, blank lines and lines starting with # are ignored. The asterisk (*) is a place holder.\n\n# Ignore all Excel files\n*.xlsx \n\n# Ignore all files in the directory named Data\nData/ \n\n# Track Codebook.xslx even though Excel files are ignored\n!Codebook.xlsx \n\nThere are always exceptions to the above. Perhaps for your project, you want to have a codebook in Excel available to anyone who views the code. If the codebook will not change too much, then it is fine to track it. Another example is that you may want to track the images for the figures needed for your paper. If you use .jpg or .png, then you can view these images on GitHub itself. Even with these exceptions, it is good practice to maintain a .gitignore file, inserting the exceptions or using the force flag (-f) as needed.\nWhen starting a project and setting up a repository, you can reference this list to populate your .gitignore. Checking git status frequently will help you keep the .gitignore updated and useful.\n\n\n3.2.8 Practice Exercises\n\nOn GitHub, create your own (private and empty) repository for your assignments. Call it “Assignments-First-Last” with your first and last names.\nClone this repository to the folder you created for this class.\nFrom the command line, create a .gitignore file. Make it so that your repository will ignore all .csv and .xlsx files.\nFrom the command line, create a file called README.md. Write whatever you would like to describe the repository. If in doubt, write: This repository contains my assingments for the Summer 2022 R and workflow minicourse.\nMake it so these changes show up on GitHub (hint: three steps). Use git status for guidance.\nCheck that these changes show up on GitHub.\nNavigate to the repository’s settings on GitHub and add me as a collaborator (Settings \\(&gt;\\) Collaborators \\(&gt;\\) Add people). My GitHub username is aziff.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command Line and Git</span>"
    ]
  },
  {
    "objectID": "03_Command_Line_and_Git.html#troubleshooting-git",
    "href": "03_Command_Line_and_Git.html#troubleshooting-git",
    "title": "3  Command Line and Git",
    "section": "3.3 Troubleshooting Git",
    "text": "3.3 Troubleshooting Git\nGenerally, there are few things in Git that cannot be undone. But, when you make a mistake or run into trouble, be mindful that the fixes may actually be irreversible. When in doubt, make a defensive copy of your repository before troubleshooting.\n\n3.3.1 “I committed before making all my changes, but I haven’t pushed yet.”\nYou can always make a new commit by staging (git add) and committing (git commit) the other changes you wanted to make. It is also possible to change the original commit. Make your changes and git add. Then, add the amend option. This will result in one commit, with the amended commit completely replacing the original commit. This is possible as long as the original commit was not pushed.\n\ngit commit -m \"Initial commit.\"\ngit add forgotten-file\n\n# Amend without changing the commit message\ngit commit --amend --no-edit\n\n# Amend with an updated commit message\ngit commit --amend -m \"Intial commit proofread for typos.\"\n\n\n\n3.3.2 “I staged a file by accident, but I haven’t commited yet.”\nSuppose you typed git add * and accidentally staged a file you did not want to stage. The command git reset HEAD will unstage the specified file. The changes made to this file will be saved. Be careful with this command! I suggest against using the hard option.\n\ngit reset HEAD results.txt\ngit status\n\nAn alternative approach is to use the below to unstage the specified file. The below does the same using the more current restore command.\n\ngit restore --staged results.txt\ngit status\n\n\n\n3.3.3 “I want to discard my changes to a file since the last commit.”\nSuppose you change some files the repository and you do not want to keep those changes. In other words, you want to revert some files back to the version in the most recent commit. The below command accomplishes this. This is another potentially dangerous command as it deletes work done locally. Do not use this command unless you are 100% sure you want to delete your changes. If the change was not committed, it will not be saved at all.\n\ngit checkout -- results.txt\n\nAn alternative approach is to use git restore to revert the file back to the version of the last commit. As for git checkout, proceed with caution as local changes will be overwritten.\n\ngit restore results.txt",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command Line and Git</span>"
    ]
  },
  {
    "objectID": "03_Command_Line_and_Git.html#chapter1",
    "href": "03_Command_Line_and_Git.html#chapter1",
    "title": "3  Command Line and Git",
    "section": "3.4 Futher Reading",
    "text": "3.4 Futher Reading\nAs for any software, there are plentiful resources online, including StackExchange or other forums, as well as AI assistants like ChatGPT for specific trouble shooting. The text book Chacon and Straub (2020) is excellent if you want to learn more about the details of Git commands and the underlying system. If you want to see the official help documentation for a command, you can access the manual from the command line by typing git help &lt;verb&gt;. For example, this is how you would get the manual on add.\n\ngit help add\n\nSometimes, you just want to check the available options.\n\ngit add -h\n\nThere are so many capabilities of Git. The above will help you in this course, but they are just the basics. The textbook Chacon and Straub (2020) goes through all what is possible in Git in an accessible format.\n\n3.4.1 References\n\n\n\n\n\nCanonical Ltd. 2021. “The Linux Comand Line for Beginners.” https://ubuntu.com/tutorials/command-line-for-beginners.\n\n\nChacon, Scott, and Ben Straub. 2020. Pro Git: Everything You Need to Know About Git. 2nd ed. Vol. Version 2.1.277. Apress. https://git-scm.com/book/en/v2.\n\n\nMatoso, Douglas. 2019. “Using Git (and GitHub) on Windows.” https://www.pluralsight.com/resources/blog/guides/using-git-and-github-on-windows.\n\n\nZell. 2015. “What to Add to Your Gitignore File.” https://zellwk.com/blog/gitignore/.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command Line and Git</span>"
    ]
  },
  {
    "objectID": "03_Command_Line_and_Git.html#footnotes",
    "href": "03_Command_Line_and_Git.html#footnotes",
    "title": "3  Command Line and Git",
    "section": "",
    "text": "This method of cloning is through an HTTP protocol. If you are cloning a repository from GitHub to your local machine, this will be fine. If you want to clone a repository to a server or network share, you may need to use an SSH protocol instead. Follow the directions here for that.↩︎\nSee Zell (2015) for a helpful explanation.↩︎",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Command Line and Git</span>"
    ]
  },
  {
    "objectID": "04_R_Basics.html",
    "href": "04_R_Basics.html",
    "title": "4  R Basics",
    "section": "",
    "text": "4.1 Basic Operations\nBasic operations allow R to be used as a calculator and are building blocks for more complex calculations. Recall that any text after # is a comment and R does not evaluate it.\n6 / (1 + 1) * 2.5 - 15\n\n[1] -7.5\n\n10 %/% 3 # Integer division\n\n[1] 3\n\n10 %% 3 # Remainder\n\n[1] 1\n\n2^10 # Alternatively, 2 ** 10\n\n[1] 1024\n\n27^(1 / 3)\n\n[1] 3\nR has two values for undefined operations: NaN (not a number) and Inf (infinity).\n0 / 0\n\n[1] NaN\n\npi / 0\n\n[1] Inf\n\n-pi / 0\n\n[1] -Inf\n\nInf - Inf\n\n[1] NaN",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "04_R_Basics.html#basic-operations",
    "href": "04_R_Basics.html#basic-operations",
    "title": "4  R Basics",
    "section": "",
    "text": "4.1.1 Practice Exercises\n\nThe formula to convert fahrenheit (\\(f\\)) to celsius (\\(c\\)) is \\((f - 32)\\frac{5}{9} = c\\). Convert 60 degrees fahrenheit to celsius.\nIs 7,865,695,837 divisible by 3?",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "04_R_Basics.html#basic-functions-and-assignment",
    "href": "04_R_Basics.html#basic-functions-and-assignment",
    "title": "4  R Basics",
    "section": "4.2 Basic Functions and Assignment",
    "text": "4.2 Basic Functions and Assignment\nFunctions in R are comprised of a name and arguments. They generally, but do not have to, output a value.\n\nabs(-3)\n\n[1] 3\n\nsqrt(4)\n\n[1] 2\n\nfactorial(10)\n\n[1] 3628800\n\nexp(10)\n\n[1] 22026.47\n\ncos(pi)\n\n[1] -1\n\nlog(1)\n\n[1] 0\n\nround(2.7)\n\n[1] 3\n\nceiling(2.7)\n\n[1] 3\n\nfloor(2.7)\n\n[1] 2\n\n\nFunctions can have more than one argument. The value of the argument is specified with =. Required arguments do not need to follow the argument name = argument value format, although they can.\n\nlog(3, base = 10)\n\n[1] 0.4771213\n\nround(0.183104, digits = 3)\n\n[1] 0.183\n\n\nRecall from chapter 3 that the documentation for functions is readily available from the command line. Online searches can help bolster this information with troubleshooting and examples.\n\n?date # Alternatively help(date)\ndate()\n\n[1] \"Sun Aug  3 11:07:53 2025\"\n\n\nTyping the function without () will show the function’s code. This is useful if you want to see exactly what a function does. The actual substance of the function factorial() is gamma(x + 1). The &lt;bytecode:... &gt; is where the function is stored and &lt;environment:...&gt; is the package that defines that function.\n\nfactorial\n\nfunction (x) \ngamma(x + 1)\n&lt;bytecode: 0x132554540&gt;\n&lt;environment: namespace:base&gt;\n\n\nThe symbol &lt;- is called the assignment arrow. This is used to define the values of objects. The first line assigns the variable x to have value 1 (assignment). The second line displays the value of x (evaluation).\n\nx &lt;- 1 \nx\n\n[1] 1\n\n\nIt used to be the case that = could not be used for assignment. This changed in 2001 and = can be used for assignment in most cases. However, it is most general and best practice to only use &lt;- for assignment and = to specify values for arguments of functions.\nAnother detail of the assignment arrow is that it can clarify the direction of assignment. That is, &lt;- or -&gt;. Usually, it will make sense to only use the &lt;- assignment arrow.\n\n2 -&gt; x\nx\n\n[1] 2\n\n\nFinally, you may rarely see the use of &lt;&lt;- and -&gt;&gt;. These uncommon assignment arrows are used to assign values to global variables. That is, those objects that retain their value regardless of what is happening in the code (loops, functions, etc.). You should practice using &lt;- for assignment and = when specifying the values of arguments.\nThere are some basic requirements for variable names.\n\nVariable names are case-sensitive.\nVariable names may contain letters (a-z, A-Z), numbers (0-9), periods (.), and underscores (_).\nVariable names may not contain spaces, hyphens, start with a number, or be the same as functions, operations, and instructions unless the name is in quotation marks (not recommended).\n\n\n4.2.1 Practice Exercises\n\nDo all of these lines give the same result? Why or why not?\n\n\nlog(3, base = 10) \n\nlog(x = 3, base = 10) \n\nlog(base = 10, x = 3)\n  \nlog(3, 10) \n\nlog(10, 3)\n\n\nFunctions can be nested, i.e., the outputs of functions can be the inputs of other functions. Calculate \\(\\ln(3!)\\).\nDefine a variable x to take the value 10. Then, try the following code exactly as it is written. Why do you get an error? Fix the code.\n\n\nX^2",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "04_R_Basics.html#types",
    "href": "04_R_Basics.html#types",
    "title": "4  R Basics",
    "section": "4.3 Types",
    "text": "4.3 Types\nVariables can take non-numeric values. You may have noticed above that we did not specify that we were defining numeric variables above. Unlike other programming languages, R recognizes the type automatically. The core types are numeric (integer, double), complex, logical, character string, and binary.\n\n4.3.1 Numeric\nThere are two numeric types, integers and doubles (double precision floating point numbers). Integers can only contain integers, as the name suggests. The integer type requires less memory than the double type.\nR will automatically cast numeric objects as doubles.\n\nv &lt;- 10 \ntypeof(v)\n\n[1] \"double\"\n\nw &lt;- 2.5\ntypeof(w)\n\n[1] \"double\"\n\n\nYou can change doubles to integers using the as.integer() function.\n\nx &lt;- as.integer(v)\ntypeof(x)\n\n[1] \"integer\"\n\n\nYou can also add L to the end of integers to signal that R should cast these objects as integers.\n\ny &lt;- 2L\ny\n\n[1] 2\n\ntypeof(y)\n\n[1] \"integer\"\n\n\nJust as the function as.integer() converts a double to an integer, as.double() converts an integer to a double.\n\nz &lt;- as.double(y)\nz\n\n[1] 2\n\ntypeof(z)\n\n[1] \"double\"\n\n\n\n\n4.3.2 Complex\nComplex numbers have real and imaginary components.\n\nx &lt;- 1 + 2i\ntypeof(x)\n\n[1] \"complex\"\n\nRe(x) # Returns real component of x\n\n[1] 1\n\nIm(x) # Returns imaginary component of x\n\n[1] 2\n\n\n\n\n4.3.3 Logical\nThe logical (or boolean) type takes two values: TRUE or FALSE. The abbreviations T and F can also be used, but it is best practice to use TRUE and FALSE. The logical type will result from a logical operation. Logical operators include: &gt;, &gt;=, &lt;, &lt;=, ==, !=.\n\na &lt;- 10.5\nb &lt;- 3\nb &gt; a\n\n[1] FALSE\n\na == b\n\n[1] FALSE\n\nis.numeric(a)\n\n[1] TRUE\n\nis.integer(b)\n\n[1] FALSE\n\nis.double(b)\n\n[1] TRUE\n\nis.complex(x)\n\n[1] TRUE\n\nf &lt;- TRUE\nis.logical(f)\n\n[1] TRUE\n\n\nThe value TRUE corresponds to the numeric value 1 and the value FALSE corresponds to the numeric value 0. This makes it easy to check for the number of elements in a structure with more than one element (a vector of length 5 in this example) that are TRUE. More information on vectors is below.\n\na &lt;- c(1, 4, 0, 12, 21)\nb &lt;- 1:5\nsum(a &gt; b)\n\n[1] 3\n\na &gt; b\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\n\n\n4.3.3.1 Missing Values\nA missing value is denoted NA (not available). Technically, it is a logical value rather than a separate data type.\n\nx &lt;- NA\nis.na(x)\n\n[1] TRUE\n\nx + 3\n\n[1] NA\n\n\nThe values Inf and NaN often gets confused with NA. Recall that Inf is infinity and NaN is “not a number” (the output of an undefined function).\n\ny &lt;- 0 / 0\nis.nan(y)\n\n[1] TRUE\n\nis.finite(y)\n\n[1] FALSE\n\nis.infinite(y)\n\n[1] FALSE\n\nis.na(y)\n\n[1] TRUE\n\n\n\n\n\n4.3.4 Character Strings\nCharacter strings are information between quotation marks. If the character string contains only numbers, it can be converted to a numeric type. Single and double quotations are interchangeable, but there is an official stylistic preference for double quotations (see ?Quotes). Whichever you decide to use, make sure you are consistent!\n\nhw &lt;-\"Hello World!\"\ntypeof(hw)\n\n[1] \"character\"\n\nis.character(hw)\n\n[1] TRUE\n\nx &lt;- \"10.3\"\nas.numeric(x)\n\n[1] 10.3\n\n\nThe function paste() creates and concatenates (combines) character strings in a multitude of situations.\n\npaste(hw, 7)\n\n[1] \"Hello World! 7\"\n\npaste(\"Number\", 3.5)\n\n[1] \"Number 3.5\"\n\n\nThe argument sep specifies if there should be a character separating the inputs. Note the difference between these two lines.\n\npaste(\"I\", \"live\", \"in\", \"Wonderland\")\n\n[1] \"I live in Wonderland\"\n\npaste(\"I\", \"live\", \"in\", \"Wonderland\", sep = \"! \")\n\n[1] \"I! live! in! Wonderland\"\n\n\nThe function paste() can also be applied to data structures with more than one element.\n\n1:5\n\n[1] 1 2 3 4 5\n\npaste(\"Class\", 1:5, sep = \" #\")\n\n[1] \"Class #1\" \"Class #2\" \"Class #3\" \"Class #4\" \"Class #5\"\n\n\nThe function paste0() does the same as paste() but without spaces.\n\npaste0(\"All\", \"one\", \"word\")\n\n[1] \"Alloneword\"\n\n\nThe function as.character() converts objects to character strings. The function toString() produces a single character string from several objects.\n\nas.character(c(\"June\", 29, 2021))\n\n[1] \"June\" \"29\"   \"2021\"\n\ntoString(c(\"June\", 29, 2021))\n\n[1] \"June, 29, 2021\"\n\n\nData are frequently in character string format. Character strings can also be useful to print directly to the console or a file.\n\nprint(hw)\n\n[1] \"Hello World!\"\n\nnoquote(hw) # Alternatively, print(hw, quote = FALSE)\n\n[1] Hello World!\n\n\nThe function sprintf() comes from C and allows for additional formatting. Strings are substituted wherever there is %s.\n\na &lt;- \"elephants\"\nsprintf(\"My favorite animals are %s.\", a)\n\n[1] \"My favorite animals are elephants.\"\n\nb &lt;- \"turtles\"\nsprintf(\"I like %s and %s.\", a, b)\n\n[1] \"I like elephants and turtles.\"\n\n\nThe power of sprintf() is more evident when printing numbers. Integers are substituted with %d.\n\nint &lt;- 1\nsprintf(\"This is class number %d.\", int)\n\n[1] \"This is class number 1.\"\n\nsprintf(\"This is class: %2d.\", int) # Add leading spaces\n\n[1] \"This is class:  1.\"\n\nsprintf(\"Class number %02d.\", int) # Add leading zeroes (2 digits total)\n\n[1] \"Class number 01.\"\n\n\nDoubles are substituted with %f.\n\nsprintf(\"%f\", pi)\n\n[1] \"3.141593\"\n\nsprintf(\"%.3f\", pi) # 3 digits past the decimal\n\n[1] \"3.142\"\n\nsprintf(\"%1.0f\", pi) # 1 integer and no decimal digits\n\n[1] \"3\"\n\nsprintf(\"%1.7f\", pi) # 1 integer and 7 decimal digits\n\n[1] \"3.1415927\"\n\nsprintf(\"%+f\", pi) # Add a sign\n\n[1] \"+3.141593\"\n\nsprintf(\"%-f\", -pi)\n\n[1] \"-3.141593\"\n\nsprintf(\"% f\", pi) # Leading space\n\n[1] \" 3.141593\"\n\n\n\n\n4.3.5 Binary\nBinary (or raw) data is in hexadecimal format and is a more basic type of data. This data type does not generally arise for empirical work.\n\nb &lt;- as.raw(9)\nb\n\n[1] 09\n\ntypeof(b)\n\n[1] \"raw\"\n\n\n\n\n4.3.6 Practice Exercises\n\nDefine a variable with the value 82. Using this variable, define another variable that is \\(82^2 - 7\\).\nWhat do you think the following will output? Test it out.\n\n\npaste(\"Me\", \"Myself\", \"&\", \"I\")\n\n\nEdit the code in question 3 to output all one word without spaces.\nEdit the code in question 3 to output commas between each of the elements.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "04_R_Basics.html#structures",
    "href": "04_R_Basics.html#structures",
    "title": "4  R Basics",
    "section": "4.4 Structures",
    "text": "4.4 Structures\nThe data types can be organized into data structures. The basic structures are vectors, matrices, arrays, lists, data frames, and factors.\n\n4.4.1 Vectors\nVectors are sequences of data points of the same type. Even if you try to make a vector with different data types, the resulting vector will be coerced so that every component has the same type. R defaults to the more general type.\n\nc(1000, 1, 2)\n\n[1] 1000    1    2\n\nc(1000, TRUE, 2)\n\n[1] 1000    1    2\n\nc(1000, TRUE, \"2\")\n\n[1] \"1000\" \"TRUE\" \"2\"   \n\nc(\"a\", \"b\", \"c\")\n\n[1] \"a\" \"b\" \"c\"\n\n1:3 # Same as c(1, 2, 3) but stored as integer and less memory-intensive\n\n[1] 1 2 3\n\n3:1\n\n[1] 3 2 1\n\nseq(from = 1, to = 10, by = 0.5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\nseq(from = 100, to = 90, by = -2)\n\n[1] 100  98  96  94  92  90\n\nseq(from = 1, to = 10, length = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nrep(3, times = 10)\n\n [1] 3 3 3 3 3 3 3 3 3 3\n\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3\n\n\nSometimes it is useful to add names to the elements of the vector, rather than just the numerical index.\n\ns &lt;- c(1, 3, 3, 2, 1, 4, 2, 4, 1, 8, 5, 1, 3)\nnames(s) &lt;- letters[1:13] # First 13 letters of the alphabet\ns\n\na b c d e f g h i j k l m \n1 3 3 2 1 4 2 4 1 8 5 1 3 \n\nlength(s)\n\n[1] 13\n\nclass(s)\n\n[1] \"numeric\"\n\nattributes(s) # We added an attribute\n\n$names\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\"\n\n\nThe names can also be added when making the vector initially.\n\ns &lt;- c(a = 1, b = 3, c = 3, d = 2)\ns\n\na b c d \n1 3 3 2 \n\n\nAnother piece of metadata (attribute) available for vectors is comment.\n\ncomment(s) &lt;- \"Letter scores in Scrabble\"\nattributes(s)\n\n$names\n[1] \"a\" \"b\" \"c\" \"d\"\n\n$comment\n[1] \"Letter scores in Scrabble\"\n\n\n\n4.4.1.1 Vectorization\nMany operations and functions in R are designed to be particularly efficient with vectors. Instead of going through each element of the vector, you can apply operations and functions to the full vector.\n\nx &lt;- c(0, 1, 4)\ny &lt;- c(5, 9, 2)\nx + y\n\n[1]  5 10  6\n\nx * y\n\n[1] 0 9 8\n\nfactorial(x)\n\n[1]  1  1 24\n\nlog(y)\n\n[1] 1.6094379 2.1972246 0.6931472\n\n\n\n\n4.4.1.2 Recycling\nIf you perform an operation on vectors of unequal lengths, R will recycle elements of the shorter vector. In this example, x has 3 elements and y has 6 elements. The first 3 elements of y are added to the elements of x, and the same for the second 3 elements y\n\nx &lt;- c(0, 1, 4)\ny &lt;- c(5, 9, 2, 2, 1, 0)\nx + y\n\n[1]  5 10  6  2  2  4\n\n\nRecycling often occurs silently, i.e., no warning message is displayed. An exception is when the longer vector is not a multiple of the shorter object.\n\nx &lt;- c(0, 1, 4)\ny &lt;- c(5, 9, 2, 2, 1)\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1]  5 10  6  2  2\n\n\nWhile recycling may seem counter-intuitive, it is very useful within the framework of R. Notice that there are no scalars, only vectors with one element. Without recycling, dealing with very simple operations like the one below would be burdensome.\n\ny &lt;- 2\ny\n\n[1] 2\n\nlength(y)\n\n[1] 1\n\nx\n\n[1] 0 1 4\n\nx * y\n\n[1] 0 2 8\n\n\n\n\n4.4.1.3 Manipulation\nThe function c() is also used to add on to existing vectors.\n\nx &lt;- 1:10\nc(x, 90:100)\n\n [1]   1   2   3   4   5   6   7   8   9  10  90  91  92  93  94  95  96  97  98\n[20]  99 100\n\n\nSubsetting vectors involves accessing certain elements. It can be done with positive numbers, negative numbers, logical values, or names.\n\nx &lt;- 1:26\nnames(x) &lt;- LETTERS\n\nSubsetting can be done with positive numbers. This returns the elements at the specified positions.\n\nx[2]\n\nB \n2 \n\nx[15:17]\n\n O  P  Q \n15 16 17 \n\nx[c(24, 2, 11)]\n\n X  B  K \n24  2 11 \n\nx[c(2, 2, 7)]\n\nB B G \n2 2 7 \n\n\nSubsetting can also be done with negative numbers. This returns elements except at the specified positions.\n\nx[-1]\n\n B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z \n 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n\nx[-(1:13)]\n\n N  O  P  Q  R  S  T  U  V  W  X  Y  Z \n14 15 16 17 18 19 20 21 22 23 24 25 26 \n\nx[-c(2, 4, 6, 8, 13:26)]\n\n A  C  E  G  I  J  K  L \n 1  3  5  7  9 10 11 12 \n\n\nSubsetting with logical values returns elements corresponding to TRUE.\n\nx[c(TRUE, rep(FALSE, 25))]\n\nA \n1 \n\nx[x &lt; 13]\n\n A  B  C  D  E  F  G  H  I  J  K  L \n 1  2  3  4  5  6  7  8  9 10 11 12 \n\nx[x &gt; 22 | x &lt; 10]\n\n A  B  C  D  E  F  G  H  I  W  X  Y  Z \n 1  2  3  4  5  6  7  8  9 23 24 25 26 \n\nx[c(TRUE, FALSE)] # Remember recycling!\n\n A  C  E  G  I  K  M  O  Q  S  U  W  Y \n 1  3  5  7  9 11 13 15 17 19 21 23 25 \n\n\nFinally, when there are names, vectors can be subset by names. This returns elements corresponding to the specified names.\n\nx[\"A\"]\n\nA \n1 \n\nx[c(\"A\", \"Q\", \"M\")]\n\n A  Q  M \n 1 17 13 \n\n\n\n\n\n4.4.2 Practice Exercises\n\nR has built-in functions to create vectors of random numbers from different distributions. Here is an example that creates a vector with 1,000 random draws from the uniform \\([0,1]\\) distribution. Add this code to your script to test it.\n\n\nuniform &lt;- runif(1000, min = 0, max = 1) \nhist(uniform)\n\n\nCreate two vectors, x and y, each comprised of 10 random draws from the uniform \\([0,1]\\) distribution.\nSubset x to only keep the first 3 realizations. Define a vector called z with this subset.\nWhat will y + z output? Check your answer.\nSubset y to only keep elements greater than 0.75.\n\n\n\n4.4.3 Matrices and Arrays\nMatrices are like vectors but with two dimensions. Just like for vectors, all elements of matrices and arrays should have the same data type. The columns need to have the same number of elements.\n\nN &lt;- matrix(1:12, nrow = 4, ncol = 3) # Automatically fills by column\nN \n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nstr(N)\n\n int [1:4, 1:3] 1 2 3 4 5 6 7 8 9 10 ...\n\nattributes(N)\n\n$dim\n[1] 4 3\n\ndim(N)\n\n[1] 4 3\n\n\nNote that the matrix fills in by column. Specifying the byrow option will result in the matrix filling in by row.\n\nO &lt;- matrix(1:12, nrow = 4, ncol = 3, byrow = TRUE)\nO\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\n\nMatrices can contain other data types, as long as they are uniform. Here are examples with character strings, doubles, logical values, and missing values.\n\nP &lt;- matrix(letters, nrow = 2, ncol = 13)\nP\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n[1,] \"a\"  \"c\"  \"e\"  \"g\"  \"i\"  \"k\"  \"m\"  \"o\"  \"q\"  \"s\"   \"u\"   \"w\"   \"y\"  \n[2,] \"b\"  \"d\"  \"f\"  \"h\"  \"j\"  \"l\"  \"n\"  \"p\"  \"r\"  \"t\"   \"v\"   \"x\"   \"z\"  \n\nQ &lt;- matrix(runif(9, min = 0, max = 1), nrow = 3, ncol = 3)\nQ\n\n          [,1]      [,2]      [,3]\n[1,] 0.6689061 0.9793808 0.9015987\n[2,] 0.1989608 0.5397124 0.4742321\n[3,] 0.5351815 0.9112498 0.9732041\n\nR &lt;- matrix(c(TRUE, TRUE, FALSE, TRUE), nrow = 2, ncol = 2)\nR\n\n     [,1]  [,2]\n[1,] TRUE FALSE\n[2,] TRUE  TRUE\n\nS &lt;- matrix(NA, nrow = 3, ncol = 4) # Take note of the recycling here!\nS\n\n     [,1] [,2] [,3] [,4]\n[1,]   NA   NA   NA   NA\n[2,]   NA   NA   NA   NA\n[3,]   NA   NA   NA   NA\n\n\nMatrices always have the dim attribute. It is possible to add other attributes, including row names, column names, and comments.\n\nrownames(O) &lt;- c(\"row1\", \"row2\", \"row3\", \"row4\") \ncolnames(O) &lt;- c(\"col1\", \"col2\", \"col3\") \ndimnames(O)\n\n[[1]]\n[1] \"row1\" \"row2\" \"row3\" \"row4\"\n\n[[2]]\n[1] \"col1\" \"col2\" \"col3\"\n\ndimnames(O)[[1]] &lt;- c(\"new1\", \"new2\", \"new3\", \"new4\")\nO\n\n     col1 col2 col3\nnew1    1    2    3\nnew2    4    5    6\nnew3    7    8    9\nnew4   10   11   12\n\ncomment(O) &lt;- \"Comment for matrix O.\"\nattributes(O)\n\n$dim\n[1] 4 3\n\n$dimnames\n$dimnames[[1]]\n[1] \"new1\" \"new2\" \"new3\" \"new4\"\n\n$dimnames[[2]]\n[1] \"col1\" \"col2\" \"col3\"\n\n\n$comment\n[1] \"Comment for matrix O.\"\n\n\nWhen the number of elements of the data is smaller than the number of elements in the matrix, the data are recycled.\n\nmatrix(1:10, nrow = 2, ncol = 10, byrow = TRUE)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    3    4    5    6    7    8    9    10\n[2,]    1    2    3    4    5    6    7    8    9    10\n\nmatrix(1:10, nrow = 2, ncol = 6, byrow = TRUE)\n\nWarning in matrix(1:10, nrow = 2, ncol = 6, byrow = TRUE): data length [10] is\nnot a sub-multiple or multiple of the number of columns [6]\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    2    3    4    5    6\n[2,]    7    8    9   10    1    2\n\n\nThe functions cbind() (column bind) and rbind() (row bind) can be used to create matrices from vectors or to add on to matricies.\n\na &lt;- c(2, 4, 6)\nb &lt;- c(1, 3, 5)\nc &lt;- c(0, 0, 1)\ncbind(a, b)\n\n     a b\n[1,] 2 1\n[2,] 4 3\n[3,] 6 5\n\nrbind(a, b)\n\n  [,1] [,2] [,3]\na    2    4    6\nb    1    3    5\n\nA &lt;- cbind(a, b, c)\nA\n\n     a b c\n[1,] 2 1 0\n[2,] 4 3 0\n[3,] 6 5 1\n\nA &lt;- cbind(A, c(1, 1, 1))\nA\n\n     a b c  \n[1,] 2 1 0 1\n[2,] 4 3 0 1\n[3,] 6 5 1 1\n\nrbind(A, c(8, 7, 1, 1))\n\n     a b c  \n[1,] 2 1 0 1\n[2,] 4 3 0 1\n[3,] 6 5 1 1\n[4,] 8 7 1 1\n\n\nBrackets, with the general format matrix[rows, columns], are used to subset matrices.\n\nO\n\n     col1 col2 col3\nnew1    1    2    3\nnew2    4    5    6\nnew3    7    8    9\nnew4   10   11   12\n\nO[1, 2]\n\n[1] 2\n\n\nYou can select multiple items by passing vectors of indices.\n\nO[1:2, c(1, 3)]\n\n     col1 col3\nnew1    1    3\nnew2    4    6\n\n\nLeaving one argument empty selects all the rows or columns.\n\nO[, c(1, 3)]\n\n     col1 col3\nnew1    1    3\nnew2    4    6\nnew3    7    9\nnew4   10   12\n\nO[1:2, ]\n\n     col1 col2 col3\nnew1    1    2    3\nnew2    4    5    6\n\n\nIf you want to preserve the attributes of the matrix, specify drop = FALSE.\n\nO[, 2]\n\nnew1 new2 new3 new4 \n   2    5    8   11 \n\nO[, 2, drop = FALSE]\n\n     col2\nnew1    2\nnew2    5\nnew3    8\nnew4   11\n\n\nArrays are just like matrices but with more than two dimensions.\n\nP &lt;- array(1:12, dim = c(2, 2, 3))\nP\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12\n\ndim(P)\n\n[1] 2 2 3\n\nclass(P)\n\n[1] \"array\"\n\nQ &lt;- array(1:12, dim = c(2, 2 ,2 ,2))\nQ\n\n, , 1, 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2, 1\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 1, 2\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12\n\n, , 2, 2\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nSubsetting arrays works much the same way as subsetting matrices, except there are more than two dimensions.\n\nP[, , 1]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nP[1:2, 1, ]\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n\n\n\n4.4.3.1 Matrix Algebra\nWe define the matrices A, B, and C to demonstrate matrix operations and functions.\n\nA &lt;- matrix(1:4, nrow = 2)\nA\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nB &lt;- matrix(5:8, nrow = 2)\nB\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nC &lt;- diag(2)\nC\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nScalar addition, subtraction, multiplication, and division are straightforward.\n\nA + 1\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    3    5\n\nB - 0.5\n\n     [,1] [,2]\n[1,]  4.5  6.5\n[2,]  5.5  7.5\n\nC * 3\n\n     [,1] [,2]\n[1,]    3    0\n[2,]    0    3\n\nA / 4\n\n     [,1] [,2]\n[1,] 0.25 0.75\n[2,] 0.50 1.00\n\n\nElement-wise addition, subtraction, multiplication, and division are the same but with two matrices rather than scalars.\n\nA + B\n\n     [,1] [,2]\n[1,]    6   10\n[2,]    8   12\n\nA - B\n\n     [,1] [,2]\n[1,]   -4   -4\n[2,]   -4   -4\n\nA * B\n\n     [,1] [,2]\n[1,]    5   21\n[2,]   12   32\n\nA / B\n\n          [,1]      [,2]\n[1,] 0.2000000 0.4285714\n[2,] 0.3333333 0.5000000\n\n\nBelow are the core functions to manipulate, characterize, and multiply matrices. There are many others, both in the basic R functions and in external packages.\n\nt(B) # Transpose\n\n     [,1] [,2]\n[1,]    5    6\n[2,]    7    8\n\nA %*% B # Matrix multiplication\n\n     [,1] [,2]\n[1,]   23   31\n[2,]   34   46\n\nsolve(B) # Inverse\n\n     [,1] [,2]\n[1,]   -4  3.5\n[2,]    3 -2.5\n\ndet(A) # Determinant\n\n[1] -2\n\nsum(diag(A)) # Trace\n\n[1] 5\n\nkronecker(A, B) # Kronecker product\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    7   15   21\n[2,]    6    8   18   24\n[3,]   10   14   20   28\n[4,]   12   16   24   32\n\neigen(B) # Spectral decomposition\n\neigen() decomposition\n$values\n[1] 13.1520673 -0.1520673\n\n$vectors\n           [,1]       [,2]\n[1,] -0.6514625 -0.8053759\n[2,] -0.7586809  0.5927644\n\n\n\n\n\n4.4.4 Practice Exercises\n\nCreate the following matrices.\n\n\\[\\begin{align*}\nX = \\begin{bmatrix} 1 & 0 & 9 \\\\ 1 & 0 & 14 \\\\ 1 & 0 & 12 \\\\ 1 & 1 & 12 \\\\ 1 & 1 & 14 \\\\ 1 & 1 & 10  \\end{bmatrix}\\quad Y = \\begin{bmatrix} 415 \\\\ 740 \\\\ 582 \\\\ 493 \\\\ 623 \\\\ 530 \\end{bmatrix}  \n\\end{align*}\\]\n\nCalculate the OLS estimator: \\((X'X)^{-1}(X'Y)\\).\n\n\n\n4.4.5 Lists\nLists are general as they can combine different data types. The elements of lists can even be other data structures (including other lists!), and do not need to be uniform length.\n\nr &lt;- list(1, \"Hi\", FALSE, 2.3, 19:23, matrix(1:4, nrow = 2))\nr\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"Hi\"\n\n[[3]]\n[1] FALSE\n\n[[4]]\n[1] 2.3\n\n[[5]]\n[1] 19 20 21 22 23\n\n[[6]]\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nr[[3]] # Access the third elements\n\n[1] FALSE\n\nlength(r)\n\n[1] 6\n\nclass(r)\n\n[1] \"list\"\n\nstr(r)\n\nList of 6\n $ : num 1\n $ : chr \"Hi\"\n $ : logi FALSE\n $ : num 2.3\n $ : int [1:5] 19 20 21 22 23\n $ : int [1:2, 1:2] 1 2 3 4\n\n\nIf the order of the elements is not conceptually relevant, then it may make more sense to name the elements of lists.\n\ns &lt;- list(myinteger = 1, \n          mycharacter = \"Hi\", \n          mylogical = FALSE, \n          mydouble = 2.3, \n          myvector = 19:23, \n          mymatrix = matrix(1:4, nrow = 2))\nstr(s)\n\nList of 6\n $ myinteger  : num 1\n $ mycharacter: chr \"Hi\"\n $ mylogical  : logi FALSE\n $ mydouble   : num 2.3\n $ myvector   : int [1:5] 19 20 21 22 23\n $ mymatrix   : int [1:2, 1:2] 1 2 3 4\n\n\nThe dollar sign, $ can be used to refer to a named element of a list.\n\ns$myvector\n\n[1] 19 20 21 22 23\n\n\nNames can also be added once a list is created.\n\nt &lt;- list(1:10, c(\"USA\", \"Mexico\", \"Canada\"))\nt\n\n[[1]]\n [1]  1  2  3  4  5  6  7  8  9 10\n\n[[2]]\n[1] \"USA\"    \"Mexico\" \"Canada\"\n\nattributes(t)\n\nNULL\n\nnames(t) &lt;- c(\"numbers\", \"countries\")\nattributes(t)\n\n$names\n[1] \"numbers\"   \"countries\"\n\n\nComments can also be added, for the whole list or for certain elements\n\ncomment(t) &lt;- \"Example list\"\nattr(t, \"countries\") &lt;- \"North American Countries\"\nstr(t)\n\nList of 2\n $ numbers  : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ countries: chr [1:3] \"USA\" \"Mexico\" \"Canada\"\n - attr(*, \"comment\")= chr \"Example list\"\n - attr(*, \"countries\")= chr \"North American Countries\"\n\nattributes(t)\n\n$names\n[1] \"numbers\"   \"countries\"\n\n$comment\n[1] \"Example list\"\n\n$countries\n[1] \"North American Countries\"\n\n\n\n4.4.5.1 Manipulation\nThe function list() can be nested to append elements to lists. In this example, a new element is added to the first element (a list with 4 elements).\n\nl1 &lt;- list(\"R\", 1:9, c(TRUE, FALSE), 1.5)\nl1\n\n[[1]]\n[1] \"R\"\n\n[[2]]\n[1] 1 2 3 4 5 6 7 8 9\n\n[[3]]\n[1]  TRUE FALSE\n\n[[4]]\n[1] 1.5\n\nl2 &lt;- list(l1, letters[1:5])\nstr(l2)\n\nList of 2\n $ :List of 4\n  ..$ : chr \"R\"\n  ..$ : int [1:9] 1 2 3 4 5 6 7 8 9\n  ..$ : logi [1:2] TRUE FALSE\n  ..$ : num 1.5\n $ : chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n\n\nTo simply append a fifth element to the original list, use the function append().\n\nl3 &lt;- append(l1, list(letters[1:5]))\nstr(l3)\n\nList of 5\n $ : chr \"R\"\n $ : int [1:9] 1 2 3 4 5 6 7 8 9\n $ : logi [1:2] TRUE FALSE\n $ : num 1.5\n $ : chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n\n\nNotice the difference without list() in the second argument.\n\nl4 &lt;- append(l1, letters[1:5])\nstr(l4)\n\nList of 9\n $ : chr \"R\"\n $ : int [1:9] 1 2 3 4 5 6 7 8 9\n $ : logi [1:2] TRUE FALSE\n $ : num 1.5\n $ : chr \"a\"\n $ : chr \"b\"\n $ : chr \"c\"\n $ : chr \"d\"\n $ : chr \"e\"\n\n\nAnother way to add a new element is with the $ sign. This requires a name for the new element.\n\nl3$greeting &lt;- \"Hello world!\"\nstr(l3)\n\nList of 6\n $         : chr \"R\"\n $         : int [1:9] 1 2 3 4 5 6 7 8 9\n $         : logi [1:2] TRUE FALSE\n $         : num 1.5\n $         : chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n $ greeting: chr \"Hello world!\"\n\n\nSubsetting of lists is done with [], [[]],and $. The exact approach depends on whether you want to preserve the output or simplify the output.\nPreserving the output means keeping the list format and attributes.\n\nt\n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$countries\n[1] \"USA\"    \"Mexico\" \"Canada\"\n\nattr(,\"countries\")\n[1] \"North American Countries\"\n\nt[1]\n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\nt[\"numbers\"]\n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\nt[1:2]\n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$countries\n[1] \"USA\"    \"Mexico\" \"Canada\"\n\nt[c(\"numbers\", \"countries\")]\n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$countries\n[1] \"USA\"    \"Mexico\" \"Canada\"\n\n\nSimplifying the output extracts what is inside each element. Imagine that each element of a list is a box. The simplified output returns what is inside each box.\n\nt[[1]]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nt[[\"numbers\"]]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nt$numbers\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWith a simplified output, you can also extract elements of the objects.\n\nt[[1]][8]\n\n[1] 8\n\nt[[\"countries\"]][1:2]\n\n[1] \"USA\"    \"Mexico\"\n\nt$countries[3]\n\n[1] \"Canada\"\n\n\nSubsetting with a nested list follows the same idea as above. Continue to be mindful of persevering vs. simplifying output!\n\nstr(l2)\n\nList of 2\n $ :List of 4\n  ..$ : chr \"R\"\n  ..$ : int [1:9] 1 2 3 4 5 6 7 8 9\n  ..$ : logi [1:2] TRUE FALSE\n  ..$ : num 1.5\n $ : chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n\nl2[[2]][3] \n\n[1] \"c\"\n\nl2[[1]][1:5]\n\n[[1]]\n[1] \"R\"\n\n[[2]]\n[1] 1 2 3 4 5 6 7 8 9\n\n[[3]]\n[1]  TRUE FALSE\n\n[[4]]\n[1] 1.5\n\n[[5]]\nNULL\n\nnames(l2) &lt;- c(\"item1\", \"item2\")\nnames(l2[[1]]) &lt;- paste0(\"subitem\", 1:4)\nstr(l2)\n\nList of 2\n $ item1:List of 4\n  ..$ subitem1: chr \"R\"\n  ..$ subitem2: int [1:9] 1 2 3 4 5 6 7 8 9\n  ..$ subitem3: logi [1:2] TRUE FALSE\n  ..$ subitem4: num 1.5\n $ item2: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n\nl2[[\"item1\"]][[\"subitem4\"]]\n\n[1] 1.5\n\nl2$item1$subitem2\n\n[1] 1 2 3 4 5 6 7 8 9\n\nl2$item1$subitem2[6:9]\n\n[1] 6 7 8 9\n\n\n\n\n\n4.4.6 Practice Exercises\n\nCreate a list with the following elements. First, a character string with your name. Second, an integer with your age. Third, a logical vector whose elements are TRUE if you know how to ride/drive a motorcycle, bicycle, scooter, and car, and FALSE otherwise.\nAccess the second index of the third element of your list. That is, do you know how to ride a bicycle?\n\n\n\n4.4.7 Factors\nFactors organize character strings by extracting the possible values. This structure can be useful in datasets with character string variables. It has some efficiency advantages as well because R stores the vector as integers rather than character strings.\n\nu &lt;- factor(c(\"Macro\", \"Metrics\", \"Micro\", \"Micro\", \"Macro\"))\nu\n\n[1] Macro   Metrics Micro   Micro   Macro  \nLevels: Macro Metrics Micro\n\nlength(u)\n\n[1] 5\n\nclass(u)\n\n[1] \"factor\"\n\ntypeof(u)\n\n[1] \"integer\"\n\nlevels(u)\n\n[1] \"Macro\"   \"Metrics\" \"Micro\"  \n\nsummary(u)\n\n  Macro Metrics   Micro \n      2       1       2 \n\n\nThe function as.factor() converts vectors of character strings or integers to factors.\n\na &lt;- c(\"Red\", \"Blue\", \"Blue\", \"Red\", \"Red\", \"Blue\")\nas.factor(a)\n\n[1] Red  Blue Blue Red  Red  Blue\nLevels: Blue Red\n\nb &lt;- c(1:4, 4:1)\nb\n\n[1] 1 2 3 4 4 3 2 1\n\nas.factor(b)\n\n[1] 1 2 3 4 4 3 2 1\nLevels: 1 2 3 4\n\n\n\n\n4.4.8 Data Frames\nWe will discuss other data structures to handle the datasets we are interested in as empiricists. The most basic structure to represent individual \\(\\times\\) variable tables is the data frame. Each row corresponds to a unit and each column corresponds to a variable. The columns must have the same type across units, although different columns can have different types. The columns have names, corresponding to the variable names.\n\nt &lt;- data.frame(Salary = c(623, 515, 611, 729, 843),\n                Dpt = c(\"IT\", \"Operations\", \"IT\", \"HR\", \"Finance\"))\nt\n\n  Salary        Dpt\n1    623         IT\n2    515 Operations\n3    611         IT\n4    729         HR\n5    843    Finance\n\ndim(t)\n\n[1] 5 2\n\nclass(t)\n\n[1] \"data.frame\"\n\nstr(t)\n\n'data.frame':   5 obs. of  2 variables:\n $ Salary: num  623 515 611 729 843\n $ Dpt   : chr  \"IT\" \"Operations\" \"IT\" \"HR\" ...\n\n\nVectors can be combined into a data frame.\n\na &lt;- 1:3\nb &lt;- letters[1:3]\nc &lt;- LETTERS[1:3]\nabc &lt;- data.frame(var1 = a, var2 = b, var3 = c)\nabc\n\n  var1 var2 var3\n1    1    a    A\n2    2    b    B\n3    3    c    C\n\n\nThe possible attributes of data frames are row names, column names, and comments.\n\nattributes(abc)\n\n$names\n[1] \"var1\" \"var2\" \"var3\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\nrownames(abc) &lt;- c(\"first\", \"second\", \"third\")\nnames(abc) &lt;- c(\"numbers\", \"lower\", \"upper\") # Alternatively, colnames()\ncomment(abc) &lt;- \"This is a very small dataframe.\"\nattributes(abc)\n\n$names\n[1] \"numbers\" \"lower\"   \"upper\"  \n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] \"first\"  \"second\" \"third\" \n\n$comment\n[1] \"This is a very small dataframe.\"\n\n\nThe function as.data.frame() converts lists and matrices to data frames.\n\nO\n\n     col1 col2 col3\nnew1    1    2    3\nnew2    4    5    6\nnew3    7    8    9\nnew4   10   11   12\n\nOdf &lt;- as.data.frame(O)\nstr(Odf)\n\n'data.frame':   4 obs. of  3 variables:\n $ col1: int  1 4 7 10\n $ col2: int  2 5 8 11\n $ col3: int  3 6 9 12\n\n\nColumns can be added to data frames using cbind() as long as the number of elements of the vector(s) is the same as the number of rows of the data frame. Rows can be added with rbind(), but this is not advised unless you are confident that the data types are the same in the original data frame and the new row(s). If not, the columns will silently change type.\n\nt\n\n  Salary        Dpt\n1    623         IT\n2    515 Operations\n3    611         IT\n4    729         HR\n5    843    Finance\n\nnew &lt;- c(1, 1, 1, 0, 0)\nt &lt;- cbind(t, new)\nt\n\n  Salary        Dpt new\n1    623         IT   1\n2    515 Operations   1\n3    611         IT   1\n4    729         HR   0\n5    843    Finance   0\n\nt &lt;- rbind(t, c(313, \"Marketing\", 1))\nt\n\n  Salary        Dpt new\n1    623         IT   1\n2    515 Operations   1\n3    611         IT   1\n4    729         HR   0\n5    843    Finance   0\n6    313  Marketing   1\n\n\nThe more robust way to add rows is by first formatting the new rows as a data frame and then combining the two data frames.\n\nnewdf &lt;- data.frame(Salary = 701, Dpt = \"Finance\", new = 0)\nrbind(t, newdf)\n\n  Salary        Dpt new\n1    623         IT   1\n2    515 Operations   1\n3    611         IT   1\n4    729         HR   0\n5    843    Finance   0\n6    313  Marketing   1\n7    701    Finance   0\n\n\nSubsetting data frames is very similar to subsetting matrices and arrays. Importantly, you should consider if you need preserving or simplifying output. Subsetting can be done by row numbers, row names, column numbers, and column names.\n\nt[1:2, ]\n\n  Salary        Dpt new\n1    623         IT   1\n2    515 Operations   1\n\nt[c(\"2\", \"3\"), ]\n\n  Salary        Dpt new\n2    515 Operations   1\n3    611         IT   1\n\nt[c(\"Salary\", \"new\")] # List-type subsetting\n\n  Salary new\n1    623   1\n2    515   1\n3    611   1\n4    729   0\n5    843   0\n6    313   1\n\nt[ , c(\"Salary\", \"new\")] # Matrix-type subsetting\n\n  Salary new\n1    623   1\n2    515   1\n3    611   1\n4    729   0\n5    843   0\n6    313   1\n\nt[3:5, 1]\n\n[1] \"611\" \"729\" \"843\"\n\nt[ , 2] # Simplifying\n\n[1] \"IT\"         \"Operations\" \"IT\"         \"HR\"         \"Finance\"   \n[6] \"Marketing\" \n\nt[ , 2, drop = FALSE] # Preserving\n\n         Dpt\n1         IT\n2 Operations\n3         IT\n4         HR\n5    Finance\n6  Marketing\n\n\n\n\n4.4.9 Practice Exercises\n\nCreate a data frame from the following list.\n\n\nscores &lt;- list(scrabble = c(1, 3, 3, 2, 1, 4, 2, 4, 1, 8, 5, 1, 3, \n\n                             1, 1, 3, 10, 1, 1, 1, 1, 4, 4, 8, 4, 10), \n\n                sequential = 1:26, \n\n                name = letters) \n\n\nSubset the letters with Scrabble scores larger than 3. Do this once with simplifying and once with preserving output.\nAdd a variable called reverse that assigns a value of 26 to A and 1 to Z.\n\n\n\n4.4.10 Other Structures\nDates and time series are two other basic data structures. See chapters 3.2.2.6 and 3.2.2.7 of Lafaye de Micheaux, Drouilhet, and Liquet (2013) and chapter 8 of Boehmke (2016) for more information.\nThere are many other types of structures. We will learn more about structures that are particularly helpful for data analysis. We will learn how to use the basic types and structures in conditional statements, loops, and functions.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "04_R_Basics.html#further-reading",
    "href": "04_R_Basics.html#further-reading",
    "title": "4  R Basics",
    "section": "4.5 Further Reading",
    "text": "4.5 Further Reading\nThe above information comes from chapters 3.5-3.6, 4, 5.1, 7.1, 9-13 of Boehmke (2016) and chapters 3.1-3.2, 10.2 of Lafaye de Micheaux, Drouilhet, and Liquet (2013).\n\n4.5.1 References\n\n\n\n\n\nBoehmke, Bradley C. 2016. Data Wrangling with R. Use R! Springer. https://link.springer.com/book/10.1007/978-3-319-45599-0.\n\n\nLafaye de Micheaux, Pierre, Rémy Drouilhet, and Benoit Liquet. 2013. The R Software : Fundamentals of Programming and Statistical Analysis. Statistics and Computing, 1431-8784 ; 40. New York, NY: Springer.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "05_R_Markdown.html",
    "href": "05_R_Markdown.html",
    "title": "5  R Markdown",
    "section": "",
    "text": "5.1 Overview\nR Markdown is a document format that allows for R code to be embedded in documents. Because R is based on Markdown, a simple, plain text formatting syntax, it is easy to learn and use.\nThe downside of R Markdown is that you may be more limited in document formatting options than if you used something like Word or LaTeX. For this reason, R Markdown may not be appropriate for writing in many cases. Regardless, it is a great tool whenever you want to formally communicate with integrated R code. For demonstration, the class materials are written in R Markdown.\nR Markdown can produce a multitude of outputs including document (PDF, HTML, Word), presentations (Beamer, PowerPoint), interactive applications (Shiny), and websites. These notes will just focus on what is needed to produce a document. Xie, Allaire, and Grolemund (2020) cover the other types of outputs.\nEven when just considering document outputs, R Markdown is great for",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "05_R_Markdown.html#overview",
    "href": "05_R_Markdown.html#overview",
    "title": "5  R Markdown",
    "section": "",
    "text": "code documentation or demonstration (including these chapters);\nnotes and memos;\nhomework assignments. You must turn in your homeworks for this class with R Markdown.\n\n\n5.1.1 Prerequisites\nMake sure you have the rmarkdown package installed.\n\nrequire(\"rmarkdown\")\n\nIt is most convenient to use RStudio to write and compile documents in R Markdown. If you do not want to use RStudio, install the pandoc package.\nTo output to PDFs, you will also need to have LaTeX installed on your computer. An alternative is to install the tinytex package. This is not necessary for document output, as you can output to HTML or Word. If you want PDFs that have similar formatting as LaTeX documents, then either LaTeX or tinytex will be necessary.\n\nrequire(\"tinytex\") \n\n\n\n5.1.2 First Document\nAn R Markdown document is a plain text file with the .Rmd file extension. Create a new R Markdown document in RStudio with File &gt; New File &gt; R Markdown. Make sure HTML is selected. Click OK in the bottom right hand corner. A document will appear that includes some example text. You can delete everything for now, but later it can be a useful reference.\nThe metadata is written at the top of the document between a pair of three hyphens. The syntax of metadata is YAML (YAML Ain’t Markup Language). Write this simple header in your empty document. Indentation matters in YAML, so make sure to copy the below exactly. Save your script with whatever name you would like.\n---\ntitle: \"Learning R Markdown\"\nauthor: \"My Name\"\ndate: \"January 1, 2025\"\noutput: html_document\n---\nType ?html\\document in your console. You will see a list of options that control the formatting and elements of the document. You can input these options into YAML using the following format. Take note of the indentation.\noutput: \n  html_document:\n    toc: true \n    toc_depth: 2\n    dev: \"jpg\"\nThe body for the document comes after the metadata and is a mix of text and code. Text is written in Markdown. Computer code is inserted in code chunks or inline expressions. Add this example below to your script. Note the use of backticks.\nThis is a paragraph in an R Markdown document.\n\nThis is an example of a code chunk:\n\n```{r}\navgmpg &lt;- mean(mtcars$mpg)\nhist(mtcars$mpg)\n```\nThis is an example of inline R code. The average MPG is `r avgmpg`.\nTo compile, click the Knit button. Even if you have defined objects in your environment, the knitting occurs in a new R session. This ensures that the R Markdown document has everything it needs to compile, regardless of the current environment.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "05_R_Markdown.html#markdown",
    "href": "05_R_Markdown.html#markdown",
    "title": "5  R Markdown",
    "section": "5.2 Markdown",
    "text": "5.2 Markdown\nMarkdown is a syntax that allows for simple text formatting. It is widely used, not just in R Markdown, and a passing familiarity with it can be useful. If you are already familiar with Markdown, the specific syntax that R Markdown uses is based on that of Pandoc.\n\n5.2.1 Text\nBasic text formatting, including italic, bold, subscript, and superscript are done as follows. Practice inserting these into your document and knit.\n_italic text_\nitalic text\n*also italic text*\nalso italic text\n**bold text**\nbold text\ntext~subscript~\ntextsubscript\ntext^superscript^\ntextsuperscript\nLinking to external websites or footnotes is done as follows.\n[clickable link](https://bookdown.org/yihui/rmarkdown/)\nclickable link\nexample^[footnote]\nexample1\nHeaders are denoted with pound signs. Try adding these to your document and knit. If you set the toc option to true, you will see a table of contents at the top of the document.\n# Header 1\n\n## Header 2\n\n### Header 3\nUse *, -, or + to create unordered lists. Indentation allows for nested lists.\n* red\n- blue\n  + navy\n  + indigo\n\nred\nblue\n\nnavy\nindigo\n\n\nOrdered lists have numbers and can also be nested.\n1. red\n2. blue\n\nred\nblue\n\nThe &gt; symbol allows for blockquotes. These are block of texts set apart from the narrative text.\n&gt; I never have considered myself a perfectionist, \n&gt; but I do think of myself as a \"professionalist\"...\n&gt; I always strive simply to be my very best.\n&gt; --- Dolly Parton\n\nI never have considered myself a perfectionist, but I do think of myself as a “professionalist”… I always strive simply to be my very best. — Dolly Parton\n\nVerbatim text is not formatted.\n```\nplain text **this is not bold**\n```\nplain text **this is not bold**\n\n\n5.2.2 Math\nMath expressions are inserted using LaTeX syntax. Inline math is inserted with a pair of single dollar signs $.\nHere is an inline math expression: $\\sqrt{4} = \\pm 2$\nHere is an inline math expression: \\(\\sqrt{4} = \\pm 2\\)\nEquations are inserted with a pair of double dollar signs $$.\n$$\\frac{1}{\\sigma \\sqrt{2\\pi}} \n\\exp\\left\\{ \\frac{-1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right)^2\\right\\}$$\n\\[\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left\\{ \\frac{-1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right)^2\\right\\}\\]\n\n\n5.2.3 Code\nR Markdown integrates R code not just to show the commands but to also print the results and output of the code. Options are available to control how the code is evaluated and how the output is displayed. Code chunks format the code and output to be separate from the narrative text. Inline code is integrated into the narrative text.\n\n5.2.3.1 Code Chunks\nIt is the default for R Markdown to evaluate the code chunk. To suppress evaluation, set eval = FALSE. If rnorm(10) were evaluated, then the output would be 10 values randomly drawn from the standard normal distribution. It is useful to suppress evaluation when you simply want to show a command or step in your code but the evaluation of it is not necessary in the remaining code chunks. Evaluation is necessary if you define an object or load a package that is used in later code chunks.\n```{r, eval = FALSE}\nx &lt;- rnorm(10)\nx\n```\nIf you need the code chunk to be evaluated but do not want to display the output, set results = \"hide\". Even though the results are not displayed, the workspace includes the object x and the document will include the code. The default is for the output to be displayed in plain code blocks (i.e., the output is not formatted using Markdown).\n```{r, results = \"hide\"}\nx &lt;- rnorm(10)\nx\n```\nYou might aesthetically prefer to have the output formatted in the same area as the code. Setting collapse = TRUE achieves this. The default is collapse = FALSE.\n```{r, collapse = TRUE}\nx &lt;- rnorm(10)\nx\n```\nThis is what the collapsed output looks like.\n\nx &lt;- rnorm(10)\nx\n##  [1] -0.56998375 -0.83724747  0.95592806  0.46090601 -0.65593631 -1.61407544\n##  [7] -1.61947514  2.45311399 -0.00346038 -0.11415747\n\nR prints warnings, messages, and errors for some commands. You can control which of these messages are printed from your code chunk. The default is for warnings and messages to be displayed (warning = TRUE and message = TRUE) and errors to be supressed (error = FALSE). If error = FALSE, then the document will not be compiled if there is an error. Compare the two code chunks below to see the difference.\n```{r, message = FALSE}\nmatrix(1:4, nrow = 5, ncol = 1) # No warning printed\n```\n\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    1\n\n\n```{r}\nmatrix(1:4, nrow = 5, ncol = 1) # Warning printed by default\n```\n\n\nWarning in matrix(1:4, nrow = 5, ncol = 1): data length [4] is not a\nsub-multiple or multiple of the number of rows [5]\n\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    1\n\n\nIf you want to only display the result but hide the code itself, set echo = FALSE. Insert this code chunk in your code and knit to see the difference from the default of displaying the code.\n```{r, echo = FALSE}\nrnorm(10)\n```\nSuppose you do not want to display the code (echo = FALSE), the results (results = \"hide\"), nor warnings and messages (warning = FALSE and message = FALSE), but you still want to evaluate the code (eval = TRUE). It is possible to combine these options in a single chunk, with each option separated with a comma.\n```{r, echo = FALSE, results = \"hide\", warning = FALSE, message = FALSE, eval = TRUE}\nmatrix(1:4, nrow = 5, ncol = 1) \n```\nA more concise alternative is to set include = FALSE. This will still evaluate the code chunk but will not display the code, results, warnings, or messages.\n```{r, include = FALSE}\nmatrix(1:4, nrow = 5, ncol = 1) \n```\nYou can name your chunks with alphanumeric characters (a-z, A-Z, 0-9) and dashes (-). This is not necessary but may be helpful if you want to find a particular code chunk quickly.\n```{r make-matrix, include = FALSE}\nmatrix(1:4, nrow = 5, ncol = 1) \n```\nYou can set the options for the entire document at the top of the document. This is useful if you want the majority of your code chunks to differ from the default output.\n```{r, setup, include = FALSE}\nknitr::opts_chunk$set(collapse = TRUE)\n```\n\n\n5.2.3.2 Inline Code\nInline code is useful to reference objects or results. Add the below example to your script and knit to test the output.\n```{r}\nx &lt;- rnorm(100)\n```\nThe mean of x is x60r mean(x)x60.\n\n\n5.2.3.3 Figures\nFigures are inserted right after the code chunk creating them.\n```{r}\nhist(x)\n```\n\n\n\n\n\n\n\n\n\nThe options fig.width, fig.height, fig.dim, fig.align, dev, and fig.cap are useful for formatting figures produced in the code chunks.\n```{r, fig.cap = \"Histogram of X\", fig.dim = c(6, 4)}\nhist(x)\n```\nIt is also possible to insert images not created by code chunks using the function include\\graphics() from the knitr package.\n```{r, out.width = \"25%\", fig.align = \"center\", fig.cap = \"Caption\"}\nknitr::include_graphics(\"image.png\")\n```\nThe option fig.alt can be used to provide alternative text for figures, which improves accessibility.\n```{r, fig.alt = \"Histogram of variable x. The x-axis shows the range of x values, and the y-axis shows frequency counts. The histogram displays the distribution of x with several bars of varying heights.\"}\nhist(x)\n```\n\n\n5.2.3.4 Tables\nOutput can be automatically formatted into a table using the kable() function from the knitr package.\n```{r}\nknitr::kable(mtcars[1:5, ], caption = \"Some Cars\")\n```\n\n\n\nSome Cars\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "05_R_Markdown.html#further-reading",
    "href": "05_R_Markdown.html#further-reading",
    "title": "5  R Markdown",
    "section": "5.3 Further Reading",
    "text": "5.3 Further Reading\nThe above information comes from chapters 1, 2.2, and 2.4-2.6 of Xie, Allaire, and Grolemund (2020). Further options to format documents may be especially relevant. These options are discussed in chapter 3.\nThere are many other ways R Markdown can be used. One useful capability is to write code chunks in other languages, including STATA, C, bash, Python, and many others. These are described in chapter 2.7 of Xie, Allaire, and Grolemund (2020). The remaining chapters of Xie, Allaire, and Grolemund (2020) cover using R Markdown to create interactive documents, presentations, applications, websites, books, and tutorials.\n\n5.3.1 References\n\n\n\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2020. R Markdown: The Definitive Guide. Chapman & Hall/CRC. https://web.archive.org/web/20200419010736/https://bookdown.org/yihui/rmarkdown/.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "05_R_Markdown.html#footnotes",
    "href": "05_R_Markdown.html#footnotes",
    "title": "5  R Markdown",
    "section": "",
    "text": "footnote↩︎",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "06_Structure.html",
    "href": "06_Structure.html",
    "title": "6  Code Structure",
    "section": "",
    "text": "6.1 Logical Operations\nRecall from chapter 4 that a logical object can take two values: TRUE or FALSE. Logical operators, those that have logical objects as inputs or outputs, are commonly used and crucial to understand before we discuss conditional statements and loops. We will define \\(a\\), \\(b\\), \\(c\\), and \\(d\\) to help demonstrate logical operators.\na &lt;- 3\nb &lt;- 8\nc &lt;- c(1, 9, 3)\nd &lt;- c(7, 10, 3)\nThe inequalities, \\(&lt;, &gt;, \\leq, \\geq\\), are denoted &lt;, &gt;, &lt;=, &gt;=, respectively.\na &gt; b\n\n[1] FALSE\n\na &lt;= b\n\n[1] TRUE\nNote that we can also apply these operators to vectors with more than one element.\nc\n\n[1] 1 9 3\n\nd\n\n[1]  7 10  3\n\nc &lt; d\n\n[1]  TRUE  TRUE FALSE\nThe operator == allows us to test if two objects are equal. The operator != allows us to test if two objects are not equal.\na == b\n\n[1] FALSE\n\nc != d\n\n[1]  TRUE  TRUE FALSE\nGenerally, ! indicates negation.\nis.character(b)\n\n[1] FALSE\n\n!is.character(b)\n\n[1] TRUE\nWe can combine logical operators using & (conjunction, “and”) or | (disjunction, “or”).\n(a &gt; b) & (b != 7)\n\n[1] FALSE\n\n(a &gt; b) | (b != 7)\n\n[1] TRUE\nIf there are many logical comparisons, it is useful to have functions that output TRUE if any or all of the comparisons are true.\nc &lt; d\n\n[1]  TRUE  TRUE FALSE\n\nany(c &lt; d)\n\n[1] TRUE\n\nall(c &lt; d)\n\n[1] FALSE\nIf you are comparing objects with more than one element, you may want TRUE if the two objects are identical and FALSE otherwise. That is, you do not want a logical vector with more than one element.\nc == d\n\n[1] FALSE FALSE  TRUE\n\nidentical(c, d)\n\n[1] FALSE\nThe %in% operator is like the subset operator in mathematics \\(in\\).\n5 %in% c\n\n[1] FALSE\n\nc(1, 12, 0, 7, 9) %in% c\n\n[1]  TRUE FALSE FALSE FALSE  TRUE",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Code Structure</span>"
    ]
  },
  {
    "objectID": "06_Structure.html#logical-operations",
    "href": "06_Structure.html#logical-operations",
    "title": "6  Code Structure",
    "section": "",
    "text": "6.1.1 Practice Exercises\n\nDefine a vector x of 10 random draws from the uniform \\([0,1]\\) distribution. Test if each element is greater than or equal to 0.9. Use the command runif().\nDefine a variable that is TRUE if at least 1 element of x is greater than or equal to 0.9.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Code Structure</span>"
    ]
  },
  {
    "objectID": "06_Structure.html#conditional-statements",
    "href": "06_Structure.html#conditional-statements",
    "title": "6  Code Structure",
    "section": "6.2 Conditional Statements",
    "text": "6.2 Conditional Statements\nConditional statements allow you to dictate different actions depending on the outcome of a condition. The general syntax is as follows.\n\nif (&lt;condition&gt;) {\n  &lt;command if condition is true&gt;\n}\n\nIf the condition, denoted in a general way as &lt;condition&gt;, is true, then the commands inside the brackets are executed. Otherwise, if the condition is false, R will continue executing the next line of code. Here is a concrete example.\n\nf &lt;- runif(1, min = 0, max = 1)  \nf\n\n[1] 0.7862228\n\nif (f &lt; 0.5) {\n  print(\"The random number is less than one half.\")\n}\n\nIf you want R to execute a different command if the condition is false, then the general code is as follows.\n\nif (&lt;condition&gt;) {\n  &lt;command if condition is true&gt;\n} else {\n  &lt;command if condition is false&gt;\n}\n\nHere is a concrete example.\n\nf\n\n[1] 0.7862228\n\nif (f &lt; 0.5) {\n  print(\"The random number is less than one half.\")\n} else {\n  print(\"The random number is greater than or equal to one half.\")\n}\n\n[1] \"The random number is greater than or equal to one half.\"\n\n\nIt is possible that there are more than two logical possibilities.\n\nf\n\n[1] 0.7862228\n\nif (f &lt; 0.25) {\n  print(\"The random number is less than one quarter.\")\n} else if (f &gt;= 0.25 & f &lt; 0.5) {\n  print(\"The random number is between one quarter and one half.\")\n} else if (f &gt;= 0.5 & f &lt; 0.75) {\n  print(\"The random number is between one half and three quarters.\")\n} else {\n  print(\"The random number is between three quarters and one.\")\n}\n\n[1] \"The random number is between three quarters and one.\"\n\n\nConditional statements can be nested. It is always best practice to have consistent use of indentations and brackets, but these conventions are especially important when nesting conditional statements. The below chunk produces the same results as the above chunk, but using nested conditional statements rather than else if.\n\nf\n\n[1] 0.7862228\n\nif (f &lt; 0.5) {\n  if (f &lt; 0.25) {\n    print(\"The random number is less than one quarter.\")\n  }\n  else {\n    print(\"The random number is between one quarter and one half.\")\n  }\n} else {\n  if (f &lt; 0.75) {\n    print(\"The random number is between one half and three quarters.\")\n  } else {\n    print(\"The random number is between three quarters and one.\")\n  }\n}\n\n[1] \"The random number is between three quarters and one.\"\n\n\nIt is important to be careful with the conditions you use. A common error is to have a condition that has more than one element (i.e., a logical vector with length greater than 1). In this case, R will use the first element and throw a warning.\n\ng &lt;- runif(2, min = 0, max = 1) \ng\nif (g &lt; 0.5) {\n  print(\"The random number is less than one half.\")\n}\ng &lt; 0.5\n\nThe function ifelse() is sometimes useful when applying functions to vectors or assigning values to a variable. It follows the same logic as conditional statements, but allows for a more concise implementation.\n\nx &lt;- 2:-1\ny &lt;- ifelse(x &gt;= 0, sqrt(x), NA)\n\nWarning in sqrt(x): NaNs produced\n\nifelse(is.na(y), 0, 1)\n\n[1] 1 1 1 0\n\n\n\n6.2.1 Practice Exercises\n\nRun the following (incorrect) code. How can you change the condition to avoid the warning and ensure that the print out is correct?\n\n\ng &lt;- runif(2, min = 0, max = 1) \ng\nif (g &lt; 0.5) {\n  print(\"All the random numbers are less than one half.\")\n}",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Code Structure</span>"
    ]
  },
  {
    "objectID": "06_Structure.html#loops",
    "href": "06_Structure.html#loops",
    "title": "6  Code Structure",
    "section": "6.3 Loops",
    "text": "6.3 Loops\nLoops indicate portions of the code to be executed more than once. The loop ends when the number of iterations has been reached or there is an exit condition.\n\n6.3.1 for\nThe for instruction involves running the code a pre-specified number of iterations. The general syntax is as follows.\n\nfor (i in &lt;vector&gt;) {\n  &lt;commands&gt;\n}\n\nR iterates through each value of &lt;vector&gt; and executes the commands inside the brackets. Once the last element of the vector is reached, R executes the next line of code. Here is an example.\n\nfor (i in 1:3) {\n  print(factorial(i))\n}\n\n[1] 1\n[1] 2\n[1] 6\n\n\n\n\n6.3.2 while\nThe while instruction involves running the code until an exit condition is satisfied. The general syntax is as follows.\n\nwhile (&lt;condition&gt;) {\n  &lt;command&gt;\n}\n\nHere is an example.\n\nj &lt;- 1\nwhile (j &lt; 4) {\n  print(factorial(j))\n  j &lt;- j + 1\n}\n\n[1] 1\n[1] 2\n[1] 6\n\n\n\n\n6.3.3 Other Loop Instructions\nIn some contexts, it is useful to further control a loop. The instruction break tells R to exit the loop.\n\nl &lt;- c(2, 4, 7)\nfor (i in l) {\n  if (i == 4) {\n    out &lt;- i\n    break\n  }\n}\nout\n\n[1] 4\n\n\nThe instruction next tells R to move to the next iteration.\n\nfor (i in l) {\n  if (i == 4) {\n    next\n  }\n  print(i)\n}\n\n[1] 2\n[1] 7\n\n\n\n\n6.3.4 Efficiency\nA common maxim that loops are slow in R and it is best to avoid them. It is true that there are faster alternatives. The use of vectorized operations is preferable when possible as these operations are incredibly fast. The function system.time() allows you to test the speed of your code. Evidently, the vectorized operation factorial(1:100000) is faster than the loop. The divergence in speeds between the two methods will be larger for more complex functions and more iterations.\n\nsystem.time(for (i in 1:100000) {\n  factorial(i)\n}) \n\n   user  system elapsed \n  0.009   0.000   0.009 \n\nsystem.time(factorial(1:100000))\n\n   user  system elapsed \n      0       0       0 \n\n\nRegardless, it is still crucial to be comfortable with loops. For smaller computations, like factorial, the difference is very minor. Sometimes, the code is clearer and makes more sense with a loop than with a vectorized operation.\n\n\n6.3.5 Practice Exercises\n\nDefine an object out with the value 0. In a for loop iterating 1, 2, , 20, add the reciprocal to out if the number is even. The sum should be \\(\\frac{1}{2} + \\frac{1}{4} + \\ldots + \\frac{1}{20}\\).\n\n\n\n6.3.6 Apply Functions\nA family of functions allows you to take advantage of R’s efficiency with vectorized operations, rather than relying too heavily on loops. There are several different functions within this family that differ by what object the function is applied and the desired output.\n\n6.3.6.1 apply()\nThe function apply() applies a function to the rows or columns (these are called margins) of matrices or data frames. While it is not faster than loops, it allows for more compact code. Following chapter 19 in Boehmke (2016), we will use the built-in data frame mtcars. To see the first 6 rows of mtcars, use the head() function.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nIf we want the mean of each column, we can use the apply() function. Note that 2 corresponds to the second margin of the data frame, i.e., the columns. Notice that the output is a named vector.\n\nx &lt;- apply(mtcars, 2, mean)\nx\n\n       mpg        cyl       disp         hp       drat         wt       qsec \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 \n        vs         am       gear       carb \n  0.437500   0.406250   3.687500   2.812500 \n\nstr(x)\n\n Named num [1:11] 20.09 6.19 230.72 146.69 3.6 ...\n - attr(*, \"names\")= chr [1:11] \"mpg\" \"cyl\" \"disp\" \"hp\" ...\n\n\nThe first margin of the data frame is row.\n\napply(mtcars, 1, max)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n              160.0               160.0               108.0               258.0 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n              360.0               225.0               360.0               146.7 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n              140.8               167.6               167.6               275.8 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n              275.8               275.8               472.0               460.0 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n              440.0                78.7                75.7                71.1 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n              120.1               318.0               304.0               350.0 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n              400.0                79.0               120.3               113.0 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n              351.0               175.0               335.0               121.0 \n\n\nIf the function to be applied has other arguments, these can be specified separated by commas. Here is an example where we trim 10% of observations from each end.\n\nout &lt;- apply(mtcars, 2, mean, trim = 0.1)\nout\n\n        mpg         cyl        disp          hp        drat          wt \n 19.6961538   6.2307692 222.5230769 141.1923077   3.5792308   3.1526923 \n       qsec          vs          am        gear        carb \n 17.8276923   0.4230769   0.3846154   3.6153846   2.6538462 \n\n\nThere are some function that are faster than the analogous implementation in apply(). These include summary(), colSums(), rowSums(), colMeans(), and rowMeans().\n\ncolMeans(mtcars)\n\n       mpg        cyl       disp         hp       drat         wt       qsec \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 \n        vs         am       gear       carb \n  0.437500   0.406250   3.687500   2.812500 \n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\n\n\n6.3.6.2 lapply()\nThe lapply() function is designed to apply functions to lists and return a list. It efficiently loops through a list and applies the specified function to each element.\n\ncarlist &lt;- as.list(mtcars[1:5, ]) # Convert the first 5 rows into a list\nstr(carlist)\n\nList of 11\n $ mpg : num [1:5] 21 21 22.8 21.4 18.7\n $ cyl : num [1:5] 6 6 4 6 8\n $ disp: num [1:5] 160 160 108 258 360\n $ hp  : num [1:5] 110 110 93 110 175\n $ drat: num [1:5] 3.9 3.9 3.85 3.08 3.15\n $ wt  : num [1:5] 2.62 2.88 2.32 3.21 3.44\n $ qsec: num [1:5] 16.5 17 18.6 19.4 17\n $ vs  : num [1:5] 0 0 1 1 0\n $ am  : num [1:5] 1 1 1 0 0\n $ gear: num [1:5] 4 4 4 3 3\n $ carb: num [1:5] 4 4 1 1 2\n\nlapply(carlist, mean)\n\n$mpg\n[1] 20.98\n\n$cyl\n[1] 6\n\n$disp\n[1] 209.2\n\n$hp\n[1] 119.6\n\n$drat\n[1] 3.576\n\n$wt\n[1] 2.894\n\n$qsec\n[1] 17.71\n\n$vs\n[1] 0.4\n\n$am\n[1] 0.6\n\n$gear\n[1] 3.6\n\n$carb\n[1] 2.4\n\n\nThe elements of the list carlist are all vectors. A list may contain matrices or data frames as well. In that case, we may want to iterate through each element of the list and apply the function to each item of each element. This is efficiently done through nesting apply functions. To demonstrate, we create a list in which each element is a data frame.\n\ncarlist &lt;- list(mazda = mtcars[1:2, ], hornet = mtcars[4:5, ], merc = mtcars[8:14, ])\ncarlist\n\n$mazda\n              mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4\n\n$hornet\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n$merc\n             mpg cyl  disp  hp drat   wt qsec vs am gear carb\nMerc 240D   24.4   4 146.7  62 3.69 3.19 20.0  1  0    4    2\nMerc 230    22.8   4 140.8  95 3.92 3.15 22.9  1  0    4    2\nMerc 280    19.2   6 167.6 123 3.92 3.44 18.3  1  0    4    4\nMerc 280C   17.8   6 167.6 123 3.92 3.44 18.9  1  0    4    4\nMerc 450SE  16.4   8 275.8 180 3.07 4.07 17.4  0  0    3    3\nMerc 450SL  17.3   8 275.8 180 3.07 3.73 17.6  0  0    3    3\nMerc 450SLC 15.2   8 275.8 180 3.07 3.78 18.0  0  0    3    3\n\n\nThe x is a stand-in value.\n\nlapply(carlist, function(x) apply(x, 2, mean))\n\n$mazda\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 21.0000   6.0000 160.0000 110.0000   3.9000   2.7475  16.7400   0.0000 \n      am     gear     carb \n  1.0000   4.0000   4.0000 \n\n$hornet\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 20.0500   7.0000 309.0000 142.5000   3.1150   3.3275  18.2300   0.5000 \n      am     gear     carb \n  0.0000   3.0000   1.5000 \n\n$merc\n        mpg         cyl        disp          hp        drat          wt \n 19.0142857   6.2857143 207.1571429 134.7142857   3.5228571   3.5428571 \n       qsec          vs          am        gear        carb \n 19.0142857   0.5714286   0.0000000   3.5714286   3.0000000 \n\n\n\n\n6.3.6.3 sapply()\nThe function sapply() is very similar to lapply() except it outputs a simplified result whenever possible. If the output is a list with elements of length 1 (more than 1), sapply() returns a vector (matrix). Otherwise, sapply() returns a list.\n\nsapply(carlist, function(x) apply(x, 2, mean))\n\n        mazda   hornet        merc\nmpg   21.0000  20.0500  19.0142857\ncyl    6.0000   7.0000   6.2857143\ndisp 160.0000 309.0000 207.1571429\nhp   110.0000 142.5000 134.7142857\ndrat   3.9000   3.1150   3.5228571\nwt     2.7475   3.3275   3.5428571\nqsec  16.7400  18.2300  19.0142857\nvs     0.0000   0.5000   0.5714286\nam     1.0000   0.0000   0.0000000\ngear   4.0000   3.0000   3.5714286\ncarb   4.0000   1.5000   3.0000000\n\n\n\n\n6.3.6.4 tapply()\nThe function tapply() efficiently applies functions over subsets of a vector. It is useful when you want to apply functions within groups. The below code calculates the average miles per gallon (mpg) grouped by the number of cylinders (cyl).\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n\nHere is an example of the same situation but for each column in the data frame.\n\napply(mtcars, 2, function(x) tapply(x, mtcars$cyl, mean))\n\n       mpg cyl     disp        hp     drat       wt     qsec        vs\n4 26.66364   4 105.1364  82.63636 4.070909 2.285727 19.13727 0.9090909\n6 19.74286   6 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286\n8 15.10000   8 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000\n         am     gear     carb\n4 0.7272727 4.090909 1.545455\n6 0.4285714 3.857143 3.428571\n8 0.1428571 3.285714 3.500000\n\n\n\n\n\n6.3.7 Practice Exercises\n\nCreate a list with 4 elements, each containing a vector with 30 numbers: the first element is 30 draws from the uniform \\([0,1]\\) distribution, the second element is 30 draws from the uniform \\([1, 2]\\) distribution, the third element is 30 draws from the uniform \\([2, 3]\\) distribution, and the fourth element is 30 draws from the uniform \\([3, 4]\\) distribution. How does lapply and sapply differ here? –&gt;\nHere is an example of implementing the function quantile. Calculate the 25th, 50th, and 75th percentile for the columns of mtcars using apply(). –&gt;\n\n\nquantile(1:100, probs = c(0.10, 0.90))\n\n 10%  90% \n10.9 90.1",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Code Structure</span>"
    ]
  },
  {
    "objectID": "06_Structure.html#functions",
    "href": "06_Structure.html#functions",
    "title": "6  Code Structure",
    "section": "6.4 Functions",
    "text": "6.4 Functions\nBy now, you have been exposed to functions, both those built into the base packages of R, and those that are accessible from external packages. Now, you will write your own functions. User-defined functions are crucial in the workflow. They allow for tasks to be more general and automatic. Often, they make the script easier to read and understand. They also make it easier to test and debug, resulting in more correct output. Even if it feels burdensome to write functions, it is usually worthwhile.\nThe example function we will be using is very simple from Boehmke (2016). Suppose we want a function to calculate the present value of a future value given interest rate and number of periods. The output should be rounded to three digits.\n\ncalc_pv &lt;- function(fv, r, n) { \n  pv &lt;- fv / ((1 + r)^n)\n  return(round(pv, 3))\n}\n\nThere are three components of the function. The body is the meat of the function. It contains all the calculations, that is, everything between {\\}.\n\nbody(calc_pv)\n\n{\n    pv &lt;- fv/((1 + r)^n)\n    return(round(pv, 3))\n}\n\n\nThe formals are the inputs the function requires. These are also called arguments.\n\nformals(calc_pv)\n\n$fv\n\n\n$r\n\n\n$n\n\n\nFinally, the environment includes all of the named objects accessible to the function.\n\nenvironment(calc_pv)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nThe function is called the same way as built-in functions.\n\ncalc_pv(fv = 1000, r = 0.08, n = 10)\n\n[1] 463.193\n\ncalc_pv(1000, 0.08, 10) # Positional matching\n\n[1] 463.193\n\n\nIt might be convenient to set default values for some arguments. You can see examples of default values in many built-in functions. In the documentation for mean(), the arguments trim and na.rm are listed in the description with trim = 0 and na.rm = FALSE. Setting default values for your own code can help prevent errors and make the function easier to use.\n\ncalc_pv &lt;- function(fv, r = 0.08, n) { \n  pv &lt;- fv / ((1 + r)^n)\n  return(round(pv, 3))\n}\n\nNow, you can use the function even without specifying r.\n\ncalc_pv(fv = 1000, n = 20)\n\n[1] 214.548\n\n\nIf you specify a function with an argument that is not used, it will not throw an error or warning. The technical name for this is lazy evaluation.\n\ncalc_pv &lt;- function(fv, r, n, x) {\n  pv &lt;- fv / ((1 + r)^n)\n  return(round(pv, 3))\n}\ncalc_pv(fv = 1000, r = 0.08, n = 10) # No need to pass a value to x\n\n[1] 463.193\n\n\nIn the present value example, the output is one number. Functions can also return more than one object. Above, we used the function return() to be very clear about what the function is returning. While this is good practice, the function will default to returning the last object of the body.\n\narith &lt;- function(x, y) {\n  x + y \n  x - y\n  x * y\n  x / y\n}\narith(1, 2)\n\n[1] 0.5\n\n\nReturning a vector allows the function to return more than one result.\n\narith &lt;- function(x, y) {\n  addition &lt;- x + y \n  subtraction &lt;- x - y\n  multiplication &lt;- x * y\n  division &lt;- x / y\n  c(addition, subtraction, multiplication, division)\n}\narith(1, 2)\n\n[1]  3.0 -1.0  2.0  0.5\n\n\nReturning a list allows the function to return more than one result and allows for an easier understanding of the output.\n\narith &lt;- function(x, y) {\n  addition &lt;- x + y \n  subtraction &lt;- x - y\n  multiplication &lt;- x * y\n  division &lt;- x / y\n  c(list(Addition = addition, Subtraction = subtraction, \n         Multiplication = multiplication, Division = division))\n}\narith(1, 2)\n\n$Addition\n[1] 3\n\n$Subtraction\n[1] -1\n\n$Multiplication\n[1] 2\n\n$Division\n[1] 0.5\n\n\n\n6.4.1 Scoping\nWhen writing functions, it is useful to have a sense of the scoping rules. These are the rules that R follows to decide on the value of the objects in a function. R will first search within the function. If all the objects are defined within the function, R does not search anymore.\n\ncalc_pv &lt;- function() {\n  fv &lt;- 1000\n  r &lt;- 0.08\n  n &lt;- 10\n  fv / ((1 + r)^n)\n}\ncalc_pv()\n\n[1] 463.1935\n\n\nIf a value is not present within the function, R will expand the search up one level. The levels are demarcated with {}.\n\nfv &lt;- 1000\ncalc_pv &lt;- function() {\n  r &lt;- 0.08\n  n &lt;- 10\n  fv / ((1 + r)^n)\n}\ncalc_pv()\n\n[1] 463.1935\n\n\nThese rules are general, including when the function takes arguments.\n\ncalc_pv &lt;- function(fv, r) {\n  n &lt;- 10\n  fv / ((1 + r)^n)\n}\ncalc_pv(1000, 0.08)\n\n[1] 463.1935\n\n\n\n\n6.4.2 Niceties\nOur example functions are simple, but often actual functions can be complex. Your script may define and use many functions that have many inputs and outputs. Even if you are the only person who will ever read your code, it is useful to include some checks to help ensure you are properly using your function. Even though these checks take time, they can save trouble down the line. The function stop() stops the execution of the function and throws an error message.\n\ncalc_pv &lt;- function(fv, r, n) {\n  \n  # Input validation\n  if (!is.numeric(fv) | !is.numeric(r) | !is.numeric(n)) {\n    stop(\"All inputs must be numeric.\\n\",\n         \"The inputs are of the following classes:\\n\",\n         \"fv: \", class(fv), \"\\n\",\n         \"r: \", class(r), \"\\n\",\n         \"n: \", class(n))\n  }\n  \n  pv &lt;- fv / ((1 + r)^n)\n  return(round(pv, 3))\n  \n}\ncalc_pv(\"1000\", 0.08, 10)\n\nError in calc_pv(\"1000\", 0.08, 10): All inputs must be numeric.\nThe inputs are of the following classes:\nfv: character\nr: numeric\nn: numeric\n\n\nNotice that our function can take in vectors. As discussed above, vectorized operations like this are very efficient in R.\n\ncalc_pv(fv = 1:10, r = 0.08, n = 10)\n\n [1] 0.463 0.926 1.390 1.853 2.316 2.779 3.242 3.706 4.169 4.632\n\n\nBut what if one of the elements of the input vector is a missing value?\n\ncalc_pv(fv = c(100, NA, 1000), r = 0.08, n = 10)\n\n[1]  46.319      NA 463.193\n\n\nThis is a frequent issue that arises when using functions within a larger script. Adding the argument na.rm allows you to specify how you want the function to handle NA values.\n\ncalc_pv &lt;- function(fv, r, n, na.rm = FALSE) {\n  \n  # Input validation\n  if (!is.numeric(fv) | !is.numeric(r) | !is.numeric(n)) {\n    stop(\"All inputs must be numeric.\\n\",\n         \"The inputs are of the following classes:\\n\",\n         \"fv: \", class(fv), \"\\n\",\n         \"r: \", class(r), \"\\n\",\n         \"n: \", class(n))\n  }\n  \n  # na.rm argument\n  if (na.rm == TRUE) {\n    fv &lt;- fv[!is.na(fv)] # Only keep non-missing values in fv\n  }\n  \n  pv &lt;- fv / ((1 + r)^n)\n  return(round(pv, 3))\n  \n}\ncalc_pv(fv = c(100, NA, 1000), r = 0.08, n = 10)\n\n[1]  46.319      NA 463.193\n\ncalc_pv(fv = c(100, NA, 1000), r = 0.08, n = 10, na.rm = TRUE)\n\n[1]  46.319 463.193\n\n\nWith input validation and conditional statements, the code to write functions can be very long. You can write your function in a separate script. The script should contain the entirety of the code for the function.\n\n# Title: calc_pv.R\n\ncalc_pv &lt;- function(fv, r, n, na.rm = FALSE) {\n  \n  # Input validation\n  if (!is.numeric(fv) | !is.numeric(r) | !is.numeric(n)) {\n    stop(\"All inputs must be numeric.\\n\",\n         \"The inputs are of the following classes:\\n\",\n         \"fv: \", class(fv), \"\\n\",\n         \"r: \", class(r), \"\\n\",\n         \"n: \", class(n))\n  }\n  \n  # na.rm argument\n  if (na.rm == TRUE) {\n    fv &lt;- fv[!is.na(fv)] # Only keep non-missing values in fv\n  }\n  \n  pv &lt;- fv / ((1 + r)^n)\n  return(round(pv, 3))\n  \n}\n\nThe function source() allows you to add the function to your global environment.\n\n# Title: main.R \n\nsource(\"calc_pv.R\")\ncalc_pv(fv = c(100, NA, 1000), r = 0.08, n = 10, na.rm = TRUE)\n\n\n\n6.4.3 Practice Exercises\n\nWrite a function to convert fahrenheit to celsius. Define a 10 \\(\\times\\) 10 matrix with 100 draws from the uniform \\([-10, 100]\\) distribution. Test your function on this matrix.\nThe Fibonacci sequence is recursive: \\(x_n = x_{n-1} + x_{n-2}\\). Write a function that takes \\(n\\) as an argument and computes \\(x_n\\). Recall that \\(x_0 = 0\\) and \\(x_1 = 1\\). Use a for loop to print the first 15 elements of the Fibonacci sequence. Bonus: try using sapply to print the first 15 elements of the Fibonacci sequence.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Code Structure</span>"
    ]
  },
  {
    "objectID": "06_Structure.html#further-reading",
    "href": "06_Structure.html#further-reading",
    "title": "6  Code Structure",
    "section": "6.5 Further Reading",
    "text": "6.5 Further Reading\nThe information from this chapter comes from chapters 18, 19 of Boehmke (2016) and chapters 5.2, 5.7-5.8 of Zamora Saiz et al. (2020).\n\n6.5.1 References\n\n\n\n\n\nBoehmke, Bradley C. 2016. Data Wrangling with R. Use R! Springer. https://link.springer.com/book/10.1007/978-3-319-45599-0.\n\n\nZamora Saiz, Alfonso, Carlos Quesada González, Lluís Hurtado Gil, and Diego Mondéjar Ruiz. 2020. An Introduction to Data Analysis in R: Hands-on Coding, Data Mining, Visualization and Statistics from Scratch. https://link.springer.com/book/10.1007/978-3-030-48997-7.",
    "crumbs": [
      "R and R Markdown Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Code Structure</span>"
    ]
  },
  {
    "objectID": "07_Data_Loading_Saving.html",
    "href": "07_Data_Loading_Saving.html",
    "title": "7  Loading and Saving Data",
    "section": "",
    "text": "7.1 Files, Folders, and Filepaths\nIn this chapter, we will go through the R functions needed to load and save data. Most of the functions are built-in, but some require an additional package. Before going through this chapter, make sure you have the following package installed and loaded.\nWe will practice importing real data. Go to Canvas &gt; Modules &gt; Module 1 &gt; Data. Download and decompress the file Gapminder.zip to a convenient folder on your computer.\nBefore learning about loading and saving data in R, it is important to understand the file system of your computer and how to navigate it. All operating systems have a file system that can be manually browsed through the GUI (Graphical User Interface) of the operating system.\nThere you can see files and folders that you commonly use. For example, you may save all of your schoolwork to a folder called Documents. You should have your own file organization system that allows you quickly locate files. This is important for coding in R because you save your data, scripts, and output to the file system of your computer. Staying organized can help you prevent making errors or duplicating work.\nLocate the folder that you use for this class. If you have not already made it, make it. Where are you saving it? You will need to know the filepath (also called path), or the location within your computer’s file system structure.\nBetween each slash is a folder. The filepath above leads to the Documents folder. Filepaths can also lead to files. For example, /Users/username/Documents/myscript.R.\nIn Mac and Linux systems, you can use ~ to indicate the home directory for the user. The above filepath would then be ~/Documents.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading and Saving Data</span>"
    ]
  },
  {
    "objectID": "07_Data_Loading_Saving.html#files-folders-and-filepaths",
    "href": "07_Data_Loading_Saving.html#files-folders-and-filepaths",
    "title": "7  Loading and Saving Data",
    "section": "",
    "text": "Mac. Click on the Finder icon or navigate to Go &gt; Computer.\nWindows. Click Start &gt; File Explorer.\n\n\n\n\nMac. You can find the filepath of any file by right-clicking on the icon and selecting Get Info. You will se an item called Where which lists the filepath. It might take something that looks like:\n\n/Users/username/Documents\n\n\n\nWindows. You can find the filepath of any file by right-clicking on the icon and selecting Copy as Path. Then, paste the path to a document to see the path. You can also see the path in Properties of a file. The format of the filepath will look very similar to that for Mac computers. The main difference is that the slashes go the other way. Another difference is how the volume (e.g., the C drive) is indicated. For example, the analogous path for windows may look like:\n\nC:\\Documents\n\n7.1.1 Getting the Filepath in R\nYou can get the filepath from the console in R. Below is the filepath where I have the data for this chapter saved on my computer. Your filepath will look completely different.\n\ngetwd()\n\n[1] \"/Users/aziff/Desktop/1_PROJECTS_W/projects/data-science-for-economic-and-social-issues.github.io\"\n\n\nThe function getwd() stands for “get working directory.” Directory is another name for a folder and working means current.\n\n\n7.1.2 Changing the Filepath in R\nYou will need to change the filepath in R to be able to load data you have saved on your computer. Otherwise, you will ask R to load a file that it cannot find. That is, R does not look through your whole computer for a file when you ask R to load it. Rather, it searches for the exact file name in the working directory. You can change, or set, the working directory with the command setwd(). The argument should be the filepath you want R to set as the working directory.\n\nsetwd(\"/Users/annaziff/Documents\")\ngetwd()\n\nUsing your own computer’s file system, try to set the directory to the folder you made for this class.\n\nsetwd(\"\")\ngetwd()\n\n\n\n7.1.3 Absolute and Relative Paths\nThe above examples are all absolute paths. That means that they have the home or volume at the root (the very beginning of the path). Relative paths allow you to navigate forward'' andbackwards’’ in the file system relative to your working directory. For example, if I am in the above folder and I want to navigate to Desktop, I can use two periods (..) to stand in for the prior directories. Each set of periods navigates me one directory backwards.\n\n# Absolute path\nsetwd(\"/Users/annaziff/Desktop/1_PROJECTS_W/ECON470_Spring2025/Plan_Econ470\")\ngetwd()\n\n# Relative path\nsetwd(\"../../..\")\ngetwd()\n\nHere is another example of a relative path that combines navigating backwards with the two periods and navigating forwards with folder names.\n\n# Absolute path\nsetwd(\"/Users/annaziff/Documents\")\ngetwd()\n\n# Relative path\nsetwd(\"../Desktop\")\ngetwd()\n\n\n\n7.1.4 Common Problems and Solutions\n\n7.1.4.1 Mis-specified Filepaths\nIt is easy to have a typo in the filepath. When this happens, R returns the following error.\nError in setwd(\"/Users/aziff/Desktop/Econ470\") : \n  cannot change working directory\nIn response to this error, assume that you are the one who is wrong, not R, and double check your spelling and the layout of your file system. You can use the function list.files() to check the contents of your working directory.\n\n\n7.1.4.2 Collaborating Across Operating Systems\nIf you are the only person using your code, it is fine to specify filepaths that work for only your computer. What if you are collaborating with someone else? If they use a different operating system, they also use a different slash (/ for Mac and \\ for Windows). Even if you both use the same operating system, you need the same file structure. Here are some solutions that can help make it easier to collaborate.\nUse file.path() to construct a filepath that uses the slashes appropriate for the computer’s operating system.\n\nsetwd(file.path(\"Users\", \"annaziff\", \"Desktop\"))\n\nThis still requires everyone who will run the code to have the same file structure. You can define an object at the beginning of the script with the proper working directory for each user.\n\nfp &lt;- file.path(\"Users\", \"asus\", \"Desktop\", \"OurProject\")\n# fp &lt;- file.path(\"C:\", \"Documents\", \"OurProject\")\n\nThe more advanced option is to set up environment variables. This is outside the scope of the class.\n\n\n7.1.4.3 Difficult File and Folder Names\nOnly use letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-) in your file and folder names. This will make navigating your file system much smoother. It is also good practice to have a consistent naming system.\n\n\n\nName\nExample\n\n\n\n\nDash case\nmy-file.txt\n\n\nCamelCase\nmyFile.txt, MyFile.txt\n\n\nSnake case\nmy_file.txt\n\n\nFlat case\nmyfile.txt\n\n\nUPPERCASE\nMYFILE.txt",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading and Saving Data</span>"
    ]
  },
  {
    "objectID": "07_Data_Loading_Saving.html#importing-and-exporting-data",
    "href": "07_Data_Loading_Saving.html#importing-and-exporting-data",
    "title": "7  Loading and Saving Data",
    "section": "7.2 Importing and Exporting Data",
    "text": "7.2 Importing and Exporting Data\nImporting text files, including those files with extensions .txt and .csv can be done with the function read.table(). This function reads a file and creates a data frame. The function read.csv() is a wrapper meaning it implements the same command but sets some defaults optimized for .csv files.\n\ndf1 &lt;- read.csv(\"Data/Gapminder/gapminder.csv\")\nstr(df1)\n\n'data.frame':   197 obs. of  4 variables:\n $ country: chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : chr  \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n\nhead(df1) # Display the first 5 rows\n\n              country   gdp gini              region\n1         Afghanistan   574 36.8      Asia & Pacific\n2             Albania  4520 29.0              Europe\n3             Algeria  4780 27.6         Arab States\n4             Andorra 42100 40.0              Europe\n5              Angola  3750 42.6              Africa\n6 Antigua and Barbuda 13300 40.0 South/Latin America\n\n\nThis command does the exact same thing.\n\ndf2 &lt;- read.table(\"Data/Gapminder/gapminder.csv\", header = TRUE, sep = \",\")\nstr(df2)\n\n'data.frame':   197 obs. of  4 variables:\n $ country: chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : chr  \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n\n\nHere is an example of reading a .txt file with read.delim(). Note that we need to specify the delimiter, in this case a space. You will need to inspect your file to determine the delimiter.\n\ndf3 &lt;- read.delim(\"Data/Gapminder/gapminder.txt\", sep = \" \")\nstr(df3)\n\n'data.frame':   197 obs. of  4 variables:\n $ country: chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : chr  \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n\n\nThese three functions have many arguments available to adjust how the data files are read. The argument stringsAsFactors is automatically set to FALSE. If it is set to TRUE, then variables with character strings are read in as factors.\n\ndf4 &lt;- read.csv(\"Data/Gapminder/gapminder.csv\", stringsAsFactors = TRUE)\nstr(df4)\n\n'data.frame':   197 obs. of  4 variables:\n $ country: Factor w/ 195 levels \"Afghanistan\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ gdp    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : Factor w/ 7 levels \"Africa\",\"Arab States\",..: 3 4 2 4 1 7 7 4 3 4 ...\n\n\nYou can specify the classes of all the columns using the argument colClasses. This is especially usefull if the dataset is larger as it means that R does not need to determine the classes itself.\n\ndf5 &lt;- read.csv(\"Data/Gapminder/gapminder.csv\", \n                colClasses = c(\"character\", \"integer\", \"double\", \"factor\"))\nstr(df5)\n\n'data.frame':   197 obs. of  4 variables:\n $ country: chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : Factor w/ 7 levels \"Africa\",\"Arab States\",..: 3 4 2 4 1 7 7 4 3 4 ...\n\n\nColumn names (or variable names) and row names can be set while reading the file as well.\n\ndf6 &lt;- read.csv(\"Data/Gapminder/gapminder.csv\",\n                col.names = c(\"Country\", \"GDP\", \"GiniIndex\", \"Region\")) \n# row.names for rows\nstr(df6) \n\n'data.frame':   197 obs. of  4 variables:\n $ Country  : chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ GDP      : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ GiniIndex: num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ Region   : chr  \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n\n\nIf you just want to get a sense of what types of variables a dataset contains, you can use the nrows argument to read in very few rows. This is especially helpful with larger datasets.\n\ncheckcols &lt;- read.csv(\"Data/Gapminder/gapminder.csv\",\n                      nrows = 3)\ncheckcols\n\n      country  gdp gini         region\n1 Afghanistan  574 36.8 Asia & Pacific\n2     Albania 4520 29.0         Europe\n3     Algeria 4780 27.6    Arab States\n\n\nThe built-in functions to export data are very similar to those to import data. Again, write.table() is the general function with write.csv() being a wrapper for different file types. Here is an example data frame that we will export.\n\ndf &lt;- data.frame(id = seq(1:50),\n                 v1 = rnorm(50, mean = 10, sd = 2),\n                 v2 = rbinom(50, size = 1, prob = 0.5),\n                 v3 = c(TRUE, FALSE),\n                 v4 = c(\"Group 1\", \"Group 2\", \"Group 3\", \"Group 4\", \"Group 5\"))\nhead(df)\n\n  id        v1 v2    v3      v4\n1  1 10.114429  1  TRUE Group 1\n2  2 11.811811  1 FALSE Group 2\n3  3  7.111778  0  TRUE Group 3\n4  4 12.775881  1 FALSE Group 4\n5  5 12.243354  0  TRUE Group 5\n6  6  8.903413  1 FALSE Group 1\n\n\nBefore exporting, make sure the correct directory is set. Remember you can use getwd() to check and setwd() to change the directory.\nThe function write.csv() exports a comma-delimited text file. You need to specify the object to be saved and the name of the file. The argument row.names determines whether the row names are exported as well. Unless you have custom row names, it is useful to set this argument to FALSE.\n\nwrite.csv(df, file = \"Data/Gapmidner/Output/df_csv.csv\", row.names = FALSE)\n\nFor greater generality, write.table() is available.\n\nwrite.table(df, file = \"Data/Gapminder/Output_Data/df_table.txt\", sep = \"t\")\n\nIf you want to read Excel files, you will need an external package. A good option is the package readxl to access the function read_excel(). This package relies on tibbles, which will be discussed in chapter 5.\n\ntib4 &lt;- read_excel(\"Data/Gapminder/gapminder.xlsx\")\nhead(tib4)\n\n# A tibble: 6 × 4\n  country             gdp    gini region             \n  &lt;chr&gt;               &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;              \n1 Afghanistan         574    36.8 Asia & Pacific     \n2 Albania             4520   29   Europe             \n3 Algeria             4780   27.6 Arab States        \n4 Andorra             42100  40   Europe             \n5 Angola              3750   42.6 Africa             \n6 Antigua and Barbuda 13300  40   South/Latin America\n\n\nOther packages that allow you to read and write Excel files include xlsx and r2excel.\nThere are other packages that allow you to import and export datasets in other formats. For example, the foreign package allows for data files from SPSS, SAS, and STATA.\n\n7.2.1 R Saved Objects\nThere are R-specific data formats to save the environment or components of it. To save the entire environment, use the .RData format.\n\nids &lt;- 1:100\nverbose_sqrt &lt;- function(num) {\n  if (num &gt;= 0) {\n    return(sqrt(num))\n  } else {\n    return(\"Negative number input.\")\n  }\n}\nsave(ids, verbose_sqrt, file = \"Data/Gapminder/Output_Data/workspace.RData\")\n\nThis file includes both the objects and the names of the objects. You can directly load .RData and the workspace is populated. If you only want to save one object, you can use .rds files instead. These do not save the object’s name. They are very memory-efficient (similar to saving a zipped file).\n\nhead(df)\n\n  id        v1 v2    v3      v4\n1  1 10.114429  1  TRUE Group 1\n2  2 11.811811  1 FALSE Group 2\n3  3  7.111778  0  TRUE Group 3\n4  4 12.775881  1 FALSE Group 4\n5  5 12.243354  0  TRUE Group 5\n6  6  8.903413  1 FALSE Group 1\n\nsaveRDS(df, \"Data/Gapminder/Output_Data/dataframe.rds\")\n\nImporting these objects is done as follows.\n\nload(\"Data/Gapminder/Output_Data/workspace.RData\") # Imports objects and names\nmydf &lt;- readRDS(\"Data/Gapminder/Output_Data/dataframe.rds\") # Imports one object assigned to mydf\n\n\n\n7.2.2 Select Variables\n\ndf &lt;- read.csv(\"Data/Gapminder/gapminder_large.csv\")\nstr(df)\n\n'data.frame':   195 obs. of  21 variables:\n $ country     : chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp_2015    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini_2015   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region      : chr  \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n $ co2_2015    : num  0.262 1.6 3.8 5.97 1.22 5.84 4.64 1.65 16.8 7.7 ...\n $ co2_2016    : num  0.245 1.57 3.64 6.07 1.18 5.9 4.6 1.76 17 7.7 ...\n $ co2_2017    : num  0.247 1.61 3.56 6.27 1.14 5.89 4.55 1.7 17 7.94 ...\n $ co2_2018    : num  0.254 1.59 3.69 6.12 1.12 5.88 4.41 1.89 16.9 7.75 ...\n $ cpi_2012    : int  8 33 34 NA 22 NA 35 34 85 69 ...\n $ cpi_2013    : int  8 31 36 NA 23 NA 34 36 81 69 ...\n $ cpi_2014    : int  12 33 36 NA 19 NA 34 37 80 72 ...\n $ cpi_2015    : int  11 36 36 NA 15 NA 32 35 79 76 ...\n $ cpi_2016    : int  15 39 34 NA 18 NA 36 33 79 75 ...\n $ cpi_2017    : int  15 38 33 NA 19 NA 39 35 77 75 ...\n $ lifeexp_2012: num  60.8 77.8 76.8 82.4 61.3 76.7 76 74.7 82.5 81 ...\n $ lifeexp_2013: num  61.3 77.9 76.9 82.5 61.9 76.8 76.1 75.2 82.6 81.2 ...\n $ lifeexp_2014: num  61.2 77.9 77 82.5 62.8 76.8 76.4 75.3 82.5 81.4 ...\n $ lifeexp_2015: num  61.2 78 77.1 82.6 63.3 76.9 76.5 75.3 82.5 81.5 ...\n $ lifeexp_2016: num  61.2 78.1 77.4 82.7 63.8 77 76.5 75.4 82.5 81.7 ...\n $ lifeexp_2017: num  63.4 78.2 77.7 82.7 64.2 77 76.7 75.6 82.4 81.8 ...\n $ lifeexp_2018: num  63.7 78.3 77.9 NA 64.6 77.2 76.8 75.8 82.5 81.9 ...\n\n\nThe built-in functions import data as data frames. Chapter 2 discusses how to select variables (columns). Here is a small review. To practice, anticipate what each line will do before running it.\n\ndf[, 1:3]\ndf[, c(2, 4)]\ndf[, \"cpi_2017\"]\ndf[, c(\"lifeexp_2012\", \"cpi_2016\")]\ndf[c(\"country\", \"region\")]\ndf[1:3]\ndf$gini_2015",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading and Saving Data</span>"
    ]
  },
  {
    "objectID": "07_Data_Loading_Saving.html#best-practices-for-data-handling",
    "href": "07_Data_Loading_Saving.html#best-practices-for-data-handling",
    "title": "7  Loading and Saving Data",
    "section": "7.3 Best Practices for Data Handling",
    "text": "7.3 Best Practices for Data Handling\nNow that you know how to load and save data, it is important to implement some rules to safeguard your data. The below are some simple best practices to keep in mind.\n\n7.3.1 Maintain the Itegrity of the Raw Data\nWhen it comes to data management, one advantage of R over a program like Excel is that it makes it easy to write a script that can be run again and again, exactly the same way. For this to work, the original raw dataset needs to remain as is, without any changes. Never overwrite the raw data. Instead, create a new ‘’cleaned’’ dataset (chapter 5 goes through how to clean data) that can be created by simply running your R script.\n\n\n7.3.2 Respect the Data Use Conditions\nSome data come with conditions to respect the privacy of the data subjects or the proprietary nature of the data. Some firms or agencies only share data after you sign a contract stipulating their requirements to use the data. Even downloading publicly available data can require an agreement. It is important to comply with these requirements. Not only do you threaten your own integrity by breaking them, but you make that entity less likely to share its data in the future.\n\n\n7.3.3 Respect the Data Subjects\nThis is especially important for data collected on vulnerable populations, but it should always hold that you respect the data subjects. Handling data in R can feel far removed from the collection process, but maintaining the trust of those who provide information is an essential part of data science and research.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading and Saving Data</span>"
    ]
  },
  {
    "objectID": "07_Data_Loading_Saving.html#further-reading",
    "href": "07_Data_Loading_Saving.html#further-reading",
    "title": "7  Loading and Saving Data",
    "section": "7.4 Further Reading",
    "text": "7.4 Further Reading\nThe above information comes from chapters 5.1-5.3, 6, and 21 of Boehmke (2016), chapters 2.2.5 and 3 of Zamora Saiz et al. (2020). See Zamora Saiz et al. (2020) chapter 3 for information on data.table.\n\n7.4.1 References\n\n\n\n\n\nBoehmke, Bradley C. 2016. Data Wrangling with R. Use R! Springer. https://link.springer.com/book/10.1007/978-3-319-45599-0.\n\n\nZamora Saiz, Alfonso, Carlos Quesada González, Lluís Hurtado Gil, and Diego Mondéjar Ruiz. 2020. An Introduction to Data Analysis in R: Hands-on Coding, Data Mining, Visualization and Statistics from Scratch. https://link.springer.com/book/10.1007/978-3-030-48997-7.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Loading and Saving Data</span>"
    ]
  },
  {
    "objectID": "08_Data_Management.html",
    "href": "08_Data_Management.html",
    "title": "8  Data Management",
    "section": "",
    "text": "8.1 Built-in Functions\nIn this chapter, we will go through the R functions needed for data management. The built-in R functions are useful tools and it is important to know their syntax. There are several packages that are widely used that are helpful to work with larger data, produce cleaner code, and be more efficient in data management. The suite of packages called tidyverse is especially common.\nHere are all the libraries you should install for this chapter. Most of these are packages in tidyverse.\nWe will practice with imported data. Go to Canvas &gt; Modules &gt; Module 1 &gt; Data for the example data. Download the entire folder Gapminder to a convenient file on your computer and save the file path for use in the below notes. Make sure that you have the folder unzipped and the correct path to the folder.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "08_Data_Management.html#built-in-functions",
    "href": "08_Data_Management.html#built-in-functions",
    "title": "8  Data Management",
    "section": "",
    "text": "8.1.1 Select Variables\n\ndf &lt;- read.csv(\"Data/Gapminder/gapminder_large.csv\")\nstr(df)\n\n'data.frame':   195 obs. of  21 variables:\n $ country     : chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp_2015    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini_2015   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region      : chr  \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n $ co2_2015    : num  0.262 1.6 3.8 5.97 1.22 5.84 4.64 1.65 16.8 7.7 ...\n $ co2_2016    : num  0.245 1.57 3.64 6.07 1.18 5.9 4.6 1.76 17 7.7 ...\n $ co2_2017    : num  0.247 1.61 3.56 6.27 1.14 5.89 4.55 1.7 17 7.94 ...\n $ co2_2018    : num  0.254 1.59 3.69 6.12 1.12 5.88 4.41 1.89 16.9 7.75 ...\n $ cpi_2012    : int  8 33 34 NA 22 NA 35 34 85 69 ...\n $ cpi_2013    : int  8 31 36 NA 23 NA 34 36 81 69 ...\n $ cpi_2014    : int  12 33 36 NA 19 NA 34 37 80 72 ...\n $ cpi_2015    : int  11 36 36 NA 15 NA 32 35 79 76 ...\n $ cpi_2016    : int  15 39 34 NA 18 NA 36 33 79 75 ...\n $ cpi_2017    : int  15 38 33 NA 19 NA 39 35 77 75 ...\n $ lifeexp_2012: num  60.8 77.8 76.8 82.4 61.3 76.7 76 74.7 82.5 81 ...\n $ lifeexp_2013: num  61.3 77.9 76.9 82.5 61.9 76.8 76.1 75.2 82.6 81.2 ...\n $ lifeexp_2014: num  61.2 77.9 77 82.5 62.8 76.8 76.4 75.3 82.5 81.4 ...\n $ lifeexp_2015: num  61.2 78 77.1 82.6 63.3 76.9 76.5 75.3 82.5 81.5 ...\n $ lifeexp_2016: num  61.2 78.1 77.4 82.7 63.8 77 76.5 75.4 82.5 81.7 ...\n $ lifeexp_2017: num  63.4 78.2 77.7 82.7 64.2 77 76.7 75.6 82.4 81.8 ...\n $ lifeexp_2018: num  63.7 78.3 77.9 NA 64.6 77.2 76.8 75.8 82.5 81.9 ...\n\n\nTake a moment to look at the data in the data viewer. It is useful to consider the following questions whenever loading data:\n\nHow many rows and columns are there?\nWhat type of observation (e.g., countries, people, firms, households) do the rows represent? Another way to say this is: what is the unit of observation?\nAre the names of the variables convenient?\n\nThe built-in functions import data as data frames. Chapter 2 discusses how to select variables (columns). Here is a small review.\n\ndf[, 1:3]\ndf[, c(2, 4)]\ndf[, \"cpi_2017\"]\ndf[, c(\"lifeexp_2012\", \"cpi_2016\")]\ndf[c(\"country\", \"region\")]\ndf[1:3]\ndf$gini_2015\n\n\n\n8.1.2 Rename and Create Variables\nThe names of a data frame can be access with names(). This is an attribute of the data frame and can be used to rename all the variables this way.\n\nnames(df)\n\n [1] \"country\"      \"gdp_2015\"     \"gini_2015\"    \"region\"       \"co2_2015\"    \n [6] \"co2_2016\"     \"co2_2017\"     \"co2_2018\"     \"cpi_2012\"     \"cpi_2013\"    \n[11] \"cpi_2014\"     \"cpi_2015\"     \"cpi_2016\"     \"cpi_2017\"     \"lifeexp_2012\"\n[16] \"lifeexp_2013\" \"lifeexp_2014\" \"lifeexp_2015\" \"lifeexp_2016\" \"lifeexp_2017\"\n[21] \"lifeexp_2018\"\n\nnames(df) &lt;- paste0(\"var\", 1:length(names(df)))\nnames(df)\n\n [1] \"var1\"  \"var2\"  \"var3\"  \"var4\"  \"var5\"  \"var6\"  \"var7\"  \"var8\"  \"var9\" \n[10] \"var10\" \"var11\" \"var12\" \"var13\" \"var14\" \"var15\" \"var16\" \"var17\" \"var18\"\n[19] \"var19\" \"var20\" \"var21\"\n\n\nAn alternative is to use the function setNames(). This function can also be used for other data structures besides data frames, such as vectors.\n\nvnames &lt;- c(\"country\", \"gdp_2015\", \"gini_2015\", \"region\",      \n            \"co2_2015\", \"co2_2016\", \"co2_2017\", \"co2_2018\",   \n            \"cpi_2012\", \"cpi_2013\", \"cpi_2014\", \"cpi_2015\",    \n            \"cpi_2016\", \"cpi_2017\", \"lifeexp_2012\", \"lifeexp_2013\",\n            \"lifeexp_2014\", \"lifeexp_2015\", \"lifeexp_2016\", \"lifeexp_2017\",\n            \"lifeexp_2018\")\ndf &lt;- setNames(df, vnames)\nnames(df)\n\n [1] \"country\"      \"gdp_2015\"     \"gini_2015\"    \"region\"       \"co2_2015\"    \n [6] \"co2_2016\"     \"co2_2017\"     \"co2_2018\"     \"cpi_2012\"     \"cpi_2013\"    \n[11] \"cpi_2014\"     \"cpi_2015\"     \"cpi_2016\"     \"cpi_2017\"     \"lifeexp_2012\"\n[16] \"lifeexp_2013\" \"lifeexp_2014\" \"lifeexp_2015\" \"lifeexp_2016\" \"lifeexp_2017\"\n[21] \"lifeexp_2018\"\n\n\nIt is also possible to rename a subset of the variables.\n\nnames(df)[1] &lt;- \"COUNTRY\"\nnames(df)\n\n [1] \"COUNTRY\"      \"gdp_2015\"     \"gini_2015\"    \"region\"       \"co2_2015\"    \n [6] \"co2_2016\"     \"co2_2017\"     \"co2_2018\"     \"cpi_2012\"     \"cpi_2013\"    \n[11] \"cpi_2014\"     \"cpi_2015\"     \"cpi_2016\"     \"cpi_2017\"     \"lifeexp_2012\"\n[16] \"lifeexp_2013\" \"lifeexp_2014\" \"lifeexp_2015\" \"lifeexp_2016\" \"lifeexp_2017\"\n[21] \"lifeexp_2018\"\n\nnames(df)[2:3] &lt;- c(\"GDP\", \"GINI\")\nnames(df)\n\n [1] \"COUNTRY\"      \"GDP\"          \"GINI\"         \"region\"       \"co2_2015\"    \n [6] \"co2_2016\"     \"co2_2017\"     \"co2_2018\"     \"cpi_2012\"     \"cpi_2013\"    \n[11] \"cpi_2014\"     \"cpi_2015\"     \"cpi_2016\"     \"cpi_2017\"     \"lifeexp_2012\"\n[16] \"lifeexp_2013\" \"lifeexp_2014\" \"lifeexp_2015\" \"lifeexp_2016\" \"lifeexp_2017\"\n[21] \"lifeexp_2018\"\n\n\nCreating new variables can be done with cbind() as discussed in chapter 2.\n\nrandom1 &lt;- rnorm(dim(df)[1])\nhead(random1)\n\n[1]  1.0554011 -1.4637454 -0.5282606  1.5977476 -0.9002189  0.2934467\n\ndf &lt;- cbind(df, random1)\ndf[1:5, c(\"COUNTRY\", \"random1\")]\n\n      COUNTRY    random1\n1 Afghanistan  1.0554011\n2     Albania -1.4637454\n3     Algeria -0.5282606\n4     Andorra  1.5977476\n5      Angola -0.9002189\n\n\nThis method has the advantage that it can be used to add more than one variable at a time.\n\nrandom2 &lt;- runif(dim(df)[1])\nrandom3 &lt;- rexp(dim(df)[1])\ndf &lt;- cbind(df, random2, random3)\ndf[1:5, c(\"COUNTRY\", \"random2\", \"random3\")]\n\n      COUNTRY    random2   random3\n1 Afghanistan 0.54912043 0.3842395\n2     Albania 0.80180192 2.0000175\n3     Algeria 0.66452378 0.6596464\n4     Andorra 0.07791196 0.9120150\n5      Angola 0.23502908 0.4231180\n\n\nThe following shortcut is helpful to create one variable at a time.\n\ndf$random4 &lt;- df$random3^2\ndf[1:5, c(\"COUNTRY\", \"random4\")]\n\n      COUNTRY   random4\n1 Afghanistan 0.1476400\n2     Albania 4.0000700\n3     Algeria 0.4351334\n4     Andorra 0.8317714\n5      Angola 0.1790288\n\n\n\n\n8.1.3 Filter Observations\nFiltering observations can be done by row name or number, as shown in chapter 2.\n\ndf[1:3, ]\ndf[c(3, 40), ]\ndf[c(\"4\", \"17\"), ]\ndf[!c(1:190), ]\ndf[-c(1:190), ]\n\nFiltering can also be done using logical statements.\n\ndf[df$random2 &gt;= 1, ]\n\n [1] COUNTRY      GDP          GINI         region       co2_2015    \n [6] co2_2016     co2_2017     co2_2018     cpi_2012     cpi_2013    \n[11] cpi_2014     cpi_2015     cpi_2016     cpi_2017     lifeexp_2012\n[16] lifeexp_2013 lifeexp_2014 lifeexp_2015 lifeexp_2016 lifeexp_2017\n[21] lifeexp_2018 random1      random2      random3      random4     \n&lt;0 rows&gt; (or 0-length row.names)\n\ndf[df$random2 &gt;= 1 & df$random3 &lt;= 0.5, ]\n\n [1] COUNTRY      GDP          GINI         region       co2_2015    \n [6] co2_2016     co2_2017     co2_2018     cpi_2012     cpi_2013    \n[11] cpi_2014     cpi_2015     cpi_2016     cpi_2017     lifeexp_2012\n[16] lifeexp_2013 lifeexp_2014 lifeexp_2015 lifeexp_2016 lifeexp_2017\n[21] lifeexp_2018 random1      random2      random3      random4     \n&lt;0 rows&gt; (or 0-length row.names)\n\nsubset(df, df$random3 &lt;= 0.05)[, c(\"COUNTRY\", \"random3\")]\n\n         COUNTRY     random3\n65       Germany 0.008742552\n72        Guyana 0.048876320\n73         Haiti 0.001249049\n74      Honduras 0.029414866\n147 Saudi Arabia 0.026581063\n179       Uganda 0.027493716\n\n\nThe which() function returns the row numbers that are being filtered.\n\nwhich(df$random3 &lt;= 0.05)\n\n[1]  65  72  73  74 147 179\n\n\n\n\n8.1.4 Organize\nSorting can be done by one or more columns. Note that even though the rows are re-ordered, the original row names remain.\n\ndforder1 &lt;- order(df$GINI)\nhead(df[dforder1, c(\"COUNTRY\", \"GINI\")])\n\n            COUNTRY GINI\n180         Ukraine 24.8\n154        Slovenia 25.6\n46   Czech Republic 26.0\n153 Slovak Republic 26.7\n16          Belarus 26.9\n87       Kazakhstan 26.9\n\ndforder2 &lt;- order(df$region, df$GINI)\nhead(df[dforder2, c(\"COUNTRY\", \"region\", \"GINI\")])\n\n                  COUNTRY region GINI\n146 Sao Tome and Principe Africa 30.8\n105                  Mali Africa 33.0\n96                Liberia Africa 33.3\n70                 Guinea Africa 33.7\n151          Sierra Leone Africa 34.0\n125                 Niger Africa 34.1",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "08_Data_Management.html#tidyverse-functions",
    "href": "08_Data_Management.html#tidyverse-functions",
    "title": "8  Data Management",
    "section": "8.2 tidyverse Functions",
    "text": "8.2 tidyverse Functions\nHadley Wickham developed the idea behind a suite of packages that streamline data work called tidyverse. There are many packages in this suite that relate to different types of datasets and parts of the data process. This chapter goes through dplyr, tidyr, and readr.\n\n8.2.1 Import and Export: readr\nThe functions in the readr package to read and write data are faster than the built-in functions. Apart from efficiency, they have another advantage in that they help ensure consistency in the imported data. For example, if there are spaces in the variable name, read.csv(), the built-in function, will automatically remove these. The readr function read_csv() will not remove them.\n\ntib1 &lt;- read_csv(\"Data/Gapminder/gapminder.csv\")\n\nRows: 197 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, region\ndbl (2): gdp, gini\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(tib1)\n\nspc_tbl_ [197 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country: chr [1:197] \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp    : num [1:197] 574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num [1:197] 36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : chr [1:197] \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   gdp = col_double(),\n  ..   gini = col_double(),\n  ..   region = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nImmediately, you can see that the data structure is different. The package readr, and all the packages in the tidyverse suite, rely on a data structure called tibbles instead of data frames. The two main differences between tibbles and data frames are the following. More information on the differences is available here.\n\nUnlike data frames, tibbles only show the first 10 rows and enough columns to fit on the screen. Each column is printed with its type.\nWhen subsetting, [] always returns another tibble and [[]] always returns a vector.\n\nJust like in read.csv(), you can specify the columns.\n\ntib2 &lt;- read_csv(\"Data/Gapminder/gapminder.csv\",\n                 col_types = list(col_character(),\n                                  col_integer(),\n                                  col_double(),\n                                  col_factor()))\nstr(tib2)\n\nspc_tbl_ [197 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country: chr [1:197] \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp    : int [1:197] 574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num [1:197] 36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : Factor w/ 7 levels \"Asia & Pacific\",..: 1 2 3 2 4 5 5 2 1 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   gdp = col_integer(),\n  ..   gini = col_double(),\n  ..   region = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE)\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nIf you want to completely rename the columns, you can do so with the option col_names. You will just need to tell R to skip reading in the first line of the file.\n\ntib3 &lt;- read_csv(\"Data/Gapminder/gapminder.csv\", skip = 1,\n                 col_names = c(\"V1\", \"V2\", \"V3\", \"V4\"))\n\nRows: 197 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): V1, V4\ndbl (2): V2, V3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(tib3)\n\nspc_tbl_ [197 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ V1: chr [1:197] \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ V2: num [1:197] 574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ V3: num [1:197] 36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ V4: chr [1:197] \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   V1 = col_character(),\n  ..   V2 = col_double(),\n  ..   V3 = col_double(),\n  ..   V4 = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThe argument n_max determines the maximum number of lines that are read.\n\nread_csv(\"Data/Gapminder/gapminder.csv\", n_max = 3)\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, region\ndbl (2): gdp, gini\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 3 × 4\n  country       gdp  gini region        \n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 Afghanistan   574  36.8 Asia & Pacific\n2 Albania      4520  29   Europe        \n3 Algeria      4780  27.6 Arab States   \n\n\nThe readr analogues to read.table() and read.delim() are read_table() and read_delim(). They have similar arguments as read_csv(). Reading in data files usually presents unexpected difficulties and complications, and the myriad of arguments available can help address any formatting issues automatically.\nThe write functions in readr are faster than the built-in functions and automatically omit row names.\n\nwrite_csv(df, file = \"Data/Gapminder/Output_Data/df_csv_readr.csv\")\n\n\n8.2.1.1 Practice Exercises\n\nAnother way to list the column types is string shortcuts. For example \"d\" for double, \"c\" for character, etc. Check the documentation for read_csv(), and call in \"gapminder.csv\" with a character column, an integer column, a double column, and a factor column.\nYou can also easily skip columns with this shorthand. Why do you think this be useful? Call in \"gapminder.csv\" again skipping the Region column.\n\n\n\n\n8.2.2 Transform: dplyr\nThe package dplyr includes functions that transform tibbles and data frames.\n\ndf &lt;- read_csv(\"Data/Gapminder/gapminder.csv\")\n\nRows: 197 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, region\ndbl (2): gdp, gini\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(df)\n\n# A tibble: 6 × 4\n  country               gdp  gini region             \n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n1 Afghanistan           574  36.8 Asia & Pacific     \n2 Albania              4520  29   Europe             \n3 Algeria              4780  27.6 Arab States        \n4 Andorra             42100  40   Europe             \n5 Angola               3750  42.6 Africa             \n6 Antigua and Barbuda 13300  40   South/Latin America\n\n\n\n8.2.2.1 Select Variables\nThe general form of functions in dplyr involves identifying the data frame first and then specifying the options. To demonstrate, the function select chooses which variables.\n\nselect(tib1, country)\n\n# A tibble: 197 × 1\n   country            \n   &lt;chr&gt;              \n 1 Afghanistan        \n 2 Albania            \n 3 Algeria            \n 4 Andorra            \n 5 Angola             \n 6 Antigua and Barbuda\n 7 Argentina          \n 8 Armenia            \n 9 Australia          \n10 Austria            \n# ℹ 187 more rows\n\n\nNote that the original data frame is not changed. You will have to assign an object if you want to save this selection in an object.\n\nhead(tib1)\n\n# A tibble: 6 × 4\n  country               gdp  gini region             \n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n1 Afghanistan           574  36.8 Asia & Pacific     \n2 Albania              4520  29   Europe             \n3 Algeria              4780  27.6 Arab States        \n4 Andorra             42100  40   Europe             \n5 Angola               3750  42.6 Africa             \n6 Antigua and Barbuda 13300  40   South/Latin America\n\n\nThere are several ways to select more than one variable. The last method used a helper, starts_with(). See the documentation for select for other helpers.\n\nselect(tib1, country, gdp)\nselect(tib1, gdp:gini)\nselect(tib1, -gdp)\nselect(tib1, -c(country, gini))\nselect(tib1, starts_with(\"g\"))\n\nSometimes it is desirable to rename variables when selecting them. This is very convenient in select!\n\nselect(tib1, country_name = country)\n\n# A tibble: 197 × 1\n   country_name       \n   &lt;chr&gt;              \n 1 Afghanistan        \n 2 Albania            \n 3 Algeria            \n 4 Andorra            \n 5 Angola             \n 6 Antigua and Barbuda\n 7 Argentina          \n 8 Armenia            \n 9 Australia          \n10 Austria            \n# ℹ 187 more rows\n\nselect(tib1, var = starts_with(\"g\"))\n\n# A tibble: 197 × 2\n    var1  var2\n   &lt;dbl&gt; &lt;dbl&gt;\n 1   574  36.8\n 2  4520  29  \n 3  4780  27.6\n 4 42100  40  \n 5  3750  42.6\n 6 13300  40  \n 7 10600  41.8\n 8  3920  31.9\n 9 55100  32.3\n10 47800  30.6\n# ℹ 187 more rows\n\n\n\n\n8.2.2.2 Rename and Create Variables\nIf you want to rename variables without dropping any, use the function rename.\n\nrename(tib1, country_name = country)\n\n# A tibble: 197 × 4\n   country_name          gdp  gini region             \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Afghanistan           574  36.8 Asia & Pacific     \n 2 Albania              4520  29   Europe             \n 3 Algeria              4780  27.6 Arab States        \n 4 Andorra             42100  40   Europe             \n 5 Angola               3750  42.6 Africa             \n 6 Antigua and Barbuda 13300  40   South/Latin America\n 7 Argentina           10600  41.8 South/Latin America\n 8 Armenia              3920  31.9 Europe             \n 9 Australia           55100  32.3 Asia & Pacific     \n10 Austria             47800  30.6 Europe             \n# ℹ 187 more rows\n\nrename(tib1, country_name = country, gdp_percapita = gdp)\n\n# A tibble: 197 × 4\n   country_name        gdp_percapita  gini region             \n   &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Afghanistan                   574  36.8 Asia & Pacific     \n 2 Albania                      4520  29   Europe             \n 3 Algeria                      4780  27.6 Arab States        \n 4 Andorra                     42100  40   Europe             \n 5 Angola                       3750  42.6 Africa             \n 6 Antigua and Barbuda         13300  40   South/Latin America\n 7 Argentina                   10600  41.8 South/Latin America\n 8 Armenia                      3920  31.9 Europe             \n 9 Australia                   55100  32.3 Asia & Pacific     \n10 Austria                     47800  30.6 Europe             \n# ℹ 187 more rows\n\n\nThe function mutate() allows for new variables to be added to the data frame or existing variables to be modified without changing the other variables.\n\nmutate(tib1, gdp_sq = gdp^2)\n\n# A tibble: 197 × 5\n   country               gdp  gini region                  gdp_sq\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n 1 Afghanistan           574  36.8 Asia & Pacific          329476\n 2 Albania              4520  29   Europe                20430400\n 3 Algeria              4780  27.6 Arab States           22848400\n 4 Andorra             42100  40   Europe              1772410000\n 5 Angola               3750  42.6 Africa                14062500\n 6 Antigua and Barbuda 13300  40   South/Latin America  176890000\n 7 Argentina           10600  41.8 South/Latin America  112360000\n 8 Armenia              3920  31.9 Europe                15366400\n 9 Australia           55100  32.3 Asia & Pacific      3036010000\n10 Austria             47800  30.6 Europe              2284840000\n# ℹ 187 more rows\n\nmutate(tib1, row_id = 1:length(country))\n\n# A tibble: 197 × 5\n   country               gdp  gini region              row_id\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                &lt;int&gt;\n 1 Afghanistan           574  36.8 Asia & Pacific           1\n 2 Albania              4520  29   Europe                   2\n 3 Algeria              4780  27.6 Arab States              3\n 4 Andorra             42100  40   Europe                   4\n 5 Angola               3750  42.6 Africa                   5\n 6 Antigua and Barbuda 13300  40   South/Latin America      6\n 7 Argentina           10600  41.8 South/Latin America      7\n 8 Armenia              3920  31.9 Europe                   8\n 9 Australia           55100  32.3 Asia & Pacific           9\n10 Austria             47800  30.6 Europe                  10\n# ℹ 187 more rows\n\nmutate(tib1, gdp_large = ifelse(gdp &gt;= 25000, TRUE, FALSE))\n\n# A tibble: 197 × 5\n   country               gdp  gini region              gdp_large\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;lgl&gt;    \n 1 Afghanistan           574  36.8 Asia & Pacific      FALSE    \n 2 Albania              4520  29   Europe              FALSE    \n 3 Algeria              4780  27.6 Arab States         FALSE    \n 4 Andorra             42100  40   Europe              TRUE     \n 5 Angola               3750  42.6 Africa              FALSE    \n 6 Antigua and Barbuda 13300  40   South/Latin America FALSE    \n 7 Argentina           10600  41.8 South/Latin America FALSE    \n 8 Armenia              3920  31.9 Europe              FALSE    \n 9 Australia           55100  32.3 Asia & Pacific      TRUE     \n10 Austria             47800  30.6 Europe              TRUE     \n# ℹ 187 more rows\n\n\nIf you want to create a new variable and drop the other variables, use the function transmute().\n\ntransmute(tib1, gini_small = ifelse(gini &lt;= 40, TRUE, FALSE))\n\n# A tibble: 197 × 1\n   gini_small\n   &lt;lgl&gt;     \n 1 TRUE      \n 2 TRUE      \n 3 TRUE      \n 4 TRUE      \n 5 FALSE     \n 6 TRUE      \n 7 FALSE     \n 8 TRUE      \n 9 TRUE      \n10 TRUE      \n# ℹ 187 more rows\n\n\n\n\n8.2.2.3 Filter Observations\nThe function select allows you to choose which variables (columns) are included in your data. The function filter allows you choose which observations (rows) are included in your data.\n\nfilter(tib1, region == \"North America\")\n\n# A tibble: 2 × 4\n  country         gdp  gini region       \n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n1 Canada        50300  31.7 North America\n2 United States 52100  41.3 North America\n\nfilter(tib1, is.na(gdp))\n\n# A tibble: 8 × 4\n  country         gdp  gini region             \n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n1 Djibouti         NA  44.1 Arab States        \n2 Eritrea          NA  40   Africa             \n3 Liechtenstein    NA  40   Europe             \n4 Venezuela        NA  46.9 South/Latin America\n5 Holy See         NA  40   Europe             \n6 North Korea      NA  37   Asia & Pacific     \n7 Somalia          NA  48   Arab States        \n8 Syria            NA  35.2 Asia & Pacific     \n\nfilter(tib1, gdp &gt; 25000 & region != \"Europe\")\n\n# A tibble: 12 × 4\n   country                gdp  gini region             \n   &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Australia            55100  32.3 Asia & Pacific     \n 2 Bahamas              27500  43.7 South/Latin America\n 3 Brunei               32900  40   Asia & Pacific     \n 4 Canada               50300  31.7 North America      \n 5 Japan                47100  32.1 Asia & Pacific     \n 6 Kuwait               36000  40   Middle east        \n 7 New Zealand          36800  34.5 Asia & Pacific     \n 8 Qatar                65100  40   Middle east        \n 9 Singapore            54000  40.9 Asia & Pacific     \n10 South Korea          26100  31.6 Asia & Pacific     \n11 United Arab Emirates 40200  40   Middle east        \n12 United States        52100  41.3 North America      \n\nfilter(tib1, region %in% c(\"North America\", \"Middle east\"))\n\n# A tibble: 14 × 4\n   country                gdp  gini region       \n   &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n 1 Canada               50300  31.7 North America\n 2 Egypt                 2700  31.2 Middle east  \n 3 Iran                  6070  38.5 Middle east  \n 4 Iraq                  5300  29.5 Middle east  \n 5 Jordan                3310  33.7 Middle east  \n 6 Kuwait               36000  40   Middle east  \n 7 Lebanon               6490  31.8 Middle east  \n 8 Libya                 5900  40   Middle east  \n 9 Oman                 16200  40   Middle east  \n10 Qatar                65100  40   Middle east  \n11 Saudi Arabia         21400  40   Middle east  \n12 United Arab Emirates 40200  40   Middle east  \n13 United States        52100  41.3 North America\n14 Yemen                  785  36.7 Middle east  \n\n\nTo select rows based on the number index, use slice.\n\nslice(tib1, 32:37)\n\n# A tibble: 6 × 4\n  country                    gdp  gini region             \n  &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n1 Cape Verde                3410  47.2 Africa             \n2 Central African Republic   347  56.2 Africa             \n3 Chad                       957  43.3 Africa             \n4 Chile                    14700  47.5 South/Latin America\n5 China                     6500  39.4 Asia & Pacific     \n6 Colombia                  7580  51.7 South/Latin America\n\n\nThe function distinct() filters out duplicated rows.\n\ndistinct(tib1)\n\n# A tibble: 195 × 4\n   country               gdp  gini region             \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Afghanistan           574  36.8 Asia & Pacific     \n 2 Albania              4520  29   Europe             \n 3 Algeria              4780  27.6 Arab States        \n 4 Andorra             42100  40   Europe             \n 5 Angola               3750  42.6 Africa             \n 6 Antigua and Barbuda 13300  40   South/Latin America\n 7 Argentina           10600  41.8 South/Latin America\n 8 Armenia              3920  31.9 Europe             \n 9 Australia           55100  32.3 Asia & Pacific     \n10 Austria             47800  30.6 Europe             \n# ℹ 185 more rows\n\nfilter(tib1, duplicated(tib1)) # Check which observations are duplicated\n\n# A tibble: 2 × 4\n  country    gdp  gini region             \n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n1 Norway   90000  27.1 Europe             \n2 Suriname  8460  61   South/Latin America\n\n\nThe function slice_sample() randomly selects rows.\n\nslice_sample(tib1, n = 4)\n\n# A tibble: 4 × 4\n  country                gdp  gini region             \n  &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n1 United Arab Emirates 40200  40   Middle east        \n2 Maldives              7500  38.4 Asia & Pacific     \n3 Poland               14600  31.7 Europe             \n4 Suriname              8460  61   South/Latin America\n\nslice_sample(tib1, prop = 0.03)\n\n# A tibble: 5 × 4\n  country       gdp  gini region\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n1 Luxembourg 108000  32.9 Europe\n2 Namibia      6270  59.5 Africa\n3 Albania      4520  29   Europe\n4 Russia      11400  38.8 Europe\n5 Romania      9710  32.1 Europe\n\n\n\n\n8.2.2.4 Organize\nThe functions so far produce data frames that explicitly differ from the inputted data frame. There are some silent functions that change the underlying structure without changing the outputted data frame. The function group_by() is an example of these silent functions. It groups the data based on the values of a set of variables. It makes most sense to group by categorical variables. The only difference is that now it says Groups: region [7].\n\ngroup_tib1 &lt;- group_by(tib1, region)\ngroup_tib1\n\n# A tibble: 197 × 4\n# Groups:   region [7]\n   country               gdp  gini region             \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Afghanistan           574  36.8 Asia & Pacific     \n 2 Albania              4520  29   Europe             \n 3 Algeria              4780  27.6 Arab States        \n 4 Andorra             42100  40   Europe             \n 5 Angola               3750  42.6 Africa             \n 6 Antigua and Barbuda 13300  40   South/Latin America\n 7 Argentina           10600  41.8 South/Latin America\n 8 Armenia              3920  31.9 Europe             \n 9 Australia           55100  32.3 Asia & Pacific     \n10 Austria             47800  30.6 Europe             \n# ℹ 187 more rows\n\n\nUngrouping the data is another silent function and it removes this underlying grouping.\n\nungroup(group_tib1)\n\n# A tibble: 197 × 4\n   country               gdp  gini region             \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Afghanistan           574  36.8 Asia & Pacific     \n 2 Albania              4520  29   Europe             \n 3 Algeria              4780  27.6 Arab States        \n 4 Andorra             42100  40   Europe             \n 5 Angola               3750  42.6 Africa             \n 6 Antigua and Barbuda 13300  40   South/Latin America\n 7 Argentina           10600  41.8 South/Latin America\n 8 Armenia              3920  31.9 Europe             \n 9 Australia           55100  32.3 Asia & Pacific     \n10 Austria             47800  30.6 Europe             \n# ℹ 187 more rows\n\n\nThe function arrange sorts the data based on the rank order of a set of variables. Adding desc() changes the rank-order to descending.\n\narrange(tib1, gini)\n\n# A tibble: 197 × 4\n   country           gdp  gini region        \n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n 1 Ukraine          2830  24.8 Europe        \n 2 Slovenia        23800  25.6 Europe        \n 3 Czech Republic  21400  26   Europe        \n 4 Slovak Republic 18900  26.7 Europe        \n 5 Belarus          6380  26.9 Europe        \n 6 Kazakhstan      10600  26.9 Asia & Pacific\n 7 Moldova          2950  27   Europe        \n 8 Finland         45600  27.1 Europe        \n 9 Norway          90000  27.1 Europe        \n10 Norway          90000  27.1 Europe        \n# ℹ 187 more rows\n\narrange(tib1, desc(region), gini)\n\n# A tibble: 197 × 4\n   country                          gdp  gini region             \n   &lt;chr&gt;                          &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Antigua and Barbuda            13300  40   South/Latin America\n 2 Dominica                        6890  40   South/Latin America\n 3 Grenada                         8190  40   South/Latin America\n 4 St. Kitts and Nevis            16700  40   South/Latin America\n 5 St. Vincent and the Grenadines  6580  40   South/Latin America\n 6 Uruguay                        13900  40.1 South/Latin America\n 7 El Salvador                     3310  41.1 South/Latin America\n 8 Trinidad and Tobago            16800  41.3 South/Latin America\n 9 Argentina                      10600  41.8 South/Latin America\n10 St. Lucia                       8490  42.6 South/Latin America\n# ℹ 187 more rows\n\n\n\n\n8.2.2.5 Practice Exercises\n\nBefore running this code, what do you think the output will be? Check to see if you were right!\n\n\nanti_join(popA, tib1, by = \"country\")",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "08_Data_Management.html#pipes",
    "href": "08_Data_Management.html#pipes",
    "title": "8  Data Management",
    "section": "8.3 Pipes",
    "text": "8.3 Pipes\nThe magrittr package contains the pipe operator, %&gt;%. The purpose of this operator is to make code clearer and more efficient. The idea is to minimize unnecessary saved objects. For example, if you are cleaning a dataset, it would be cumbersome to save a new data frame for each step in the cleaning process. Pipe operators, or pipes, help with this.\nThe idea is that the pipe forwards a value to the next function. The two lines result in the same output. The first argument of filter() is forwarded by the pipe operator.\n\nfilter(tib1, region == \"North America\")\n\n# A tibble: 2 × 4\n  country         gdp  gini region       \n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n1 Canada        50300  31.7 North America\n2 United States 52100  41.3 North America\n\ntib1 %&gt;% filter(region == \"North America\")\n\n# A tibble: 2 × 4\n  country         gdp  gini region       \n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n1 Canada        50300  31.7 North America\n2 United States 52100  41.3 North America\n\n\nPipe operators are especially useful when there are several operations being applied to the same object.\n\n\nRows: 195 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (1): population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (1): population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ntib1 %&gt;%\n  distinct() %&gt;%\n  full_join(pop, by = \"country\") %&gt;%\n  arrange(desc(region), desc(population)) %&gt;%\n  head()\n\n# A tibble: 6 × 5\n  country     gdp  gini region              population\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n1 Brazil    11400  51.6 South/Latin America  204000000\n2 Mexico    10000  46.5 South/Latin America  122000000\n3 Colombia   7580  51.7 South/Latin America   47500000\n4 Argentina 10600  41.8 South/Latin America   43100000\n5 Peru       6110  43.7 South/Latin America   30500000\n6 Venezuela    NA  46.9 South/Latin America   30100000\n\n\nTo highlight the utility of pipe operators, consider these alternatives. They produce the same results. The first approach results in two objects that are not necessary for the final analysis, tmp1 and tmp2. These objects are created with the sole purpose of being used in other functions. If the dataset is large, saving different versions of it can be burdensome. Additionally, the workspace becomes messy with so many temporary objects. While the second approach avoids temporary versions, it is difficult to read and understand.\n\ntmp1 &lt;- distinct(tib1)\ntmp2 &lt;- full_join(tmp1, pop, by = \"country\")\ndf &lt;- arrange(tmp2, desc(region), desc(population))\nhead(df)\n\n# A tibble: 6 × 5\n  country     gdp  gini region              population\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n1 Brazil    11400  51.6 South/Latin America  204000000\n2 Mexico    10000  46.5 South/Latin America  122000000\n3 Colombia   7580  51.7 South/Latin America   47500000\n4 Argentina 10600  41.8 South/Latin America   43100000\n5 Peru       6110  43.7 South/Latin America   30500000\n6 Venezuela    NA  46.9 South/Latin America   30100000\n\nhead(arrange(distinct(full_join(tib1, pop, by = \"country\")), desc(region), desc(population)))\n\n# A tibble: 6 × 5\n  country     gdp  gini region              population\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n1 Brazil    11400  51.6 South/Latin America  204000000\n2 Mexico    10000  46.5 South/Latin America  122000000\n3 Colombia   7580  51.7 South/Latin America   47500000\n4 Argentina 10600  41.8 South/Latin America   43100000\n5 Peru       6110  43.7 South/Latin America   30500000\n6 Venezuela    NA  46.9 South/Latin America   30100000\n\n\nNote that the pipe operator can be used with functions outside of the tidyverse functions.\n\nfull_join(df, pop, by = \"country\") %&gt;%\n  write.csv(\"Data/Gapminder/Output_Data/country_info.csv\")\n\nPipe operators can forward objects to other arguments besides the first one. A period (.) indicates this. Here is an example with plotting (see chapter 7).\n\n\nRows: 195 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): country, region\ndbl (19): gdp_2015, gini_2015, co2_2015, co2_2016, co2_2017, co2_2018, cpi_2...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 6 × 21\n  country gdp_2015 gini_2015 region co2_2015 co2_2016 co2_2017 co2_2018 cpi_2012\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Afghan…      574      36.8 Asia …    0.262    0.245    0.247    0.254        8\n2 Albania     4520      29   Europe    1.6      1.57     1.61     1.59        33\n3 Algeria     4780      27.6 Arab …    3.8      3.64     3.56     3.69        34\n4 Andorra    42100      40   Europe    5.97     6.07     6.27     6.12        NA\n5 Angola      3750      42.6 Africa    1.22     1.18     1.14     1.12        22\n6 Antigu…    13300      40   South…    5.84     5.9      5.89     5.88        NA\n# ℹ 12 more variables: cpi_2013 &lt;dbl&gt;, cpi_2014 &lt;dbl&gt;, cpi_2015 &lt;dbl&gt;,\n#   cpi_2016 &lt;dbl&gt;, cpi_2017 &lt;dbl&gt;, lifeexp_2012 &lt;dbl&gt;, lifeexp_2013 &lt;dbl&gt;,\n#   lifeexp_2014 &lt;dbl&gt;, lifeexp_2015 &lt;dbl&gt;, lifeexp_2016 &lt;dbl&gt;,\n#   lifeexp_2017 &lt;dbl&gt;, lifeexp_2018 &lt;dbl&gt;\n\n\n\ntib1 %&gt;%\n  distinct() %&gt;%\n  full_join(pop, by = \"country\") %&gt;%\n  ggplot(aes(x = gini_2015), data = .) + geom_histogram(stat = \"bin\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nGetting comfortable with %&gt;% can vastly smooth your workflow in R. Another useful pipe is %&lt;&gt;% which functions the same as %&gt;% but also assigns the object (it is called the assginment pipe). These two lines of code result in the same tibble.\n\ntib1 &lt;- tib1 %&gt;%\n  mutate(id = 1:n())\n\n\ntib1 %&lt;&gt;%\n  mutate(id = 1:n())\n\n\n8.3.1 Practice Exercises\n\nUse pipes to accomplish the following tasks on tib1: select country, region, co2_2015, and co2_2016, remove rows with missing values for either CO2 variables, create a variable that is TRUE when CO2 emissions in 2016 are smaller than those in 2015, and only keep the rows where this variable is TRUE.\nIf you are curious, look at the documentation for ?magrittr. There are four types of pipes. Take a moment to familiarize yourself with their differences.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "08_Data_Management.html#further-reading",
    "href": "08_Data_Management.html#further-reading",
    "title": "8  Data Management",
    "section": "8.4 Further Reading",
    "text": "8.4 Further Reading\nThere are many great resources online, including cheat sheets. Here is one for dplyr. Save this cheat sheet if you find it useful! More cheat sheets can be found here.\nThe above information comes from chapters 5.1-5.3, 6, and 21 of Boehmke (2016), chapters 2.2.5 and 3 of Zamora Saiz et al. (2020). See Zamora Saiz et al. (2020) chapter 3 for information on data.table.\n\n8.4.1 References\n\n\n\n\n\nBoehmke, Bradley C. 2016. Data Wrangling with R. Use R! Springer. https://link.springer.com/book/10.1007/978-3-319-45599-0.\n\n\nZamora Saiz, Alfonso, Carlos Quesada González, Lluís Hurtado Gil, and Diego Mondéjar Ruiz. 2020. An Introduction to Data Analysis in R: Hands-on Coding, Data Mining, Visualization and Statistics from Scratch. https://link.springer.com/book/10.1007/978-3-030-48997-7.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "09_Merging_and_Reshaping_Data.html",
    "href": "09_Merging_and_Reshaping_Data.html",
    "title": "9  Merging and Reshaping Data",
    "section": "",
    "text": "9.1 Merge: Built-in Functions\nHere are all the libraries you should install for this chapter.\nAs discussed in chapter 4, rbind() can be used to append additional observations. If using this approach, it is better to transform the new row(s) into a data frame. This will help avoid silently changing a variable type.\ndf &lt;- read.csv(\"Data/Gapminder/gapminder_large.csv\")\nstr(df)\n\n'data.frame':   195 obs. of  21 variables:\n $ country     : chr  \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp_2015    : int  574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini_2015   : num  36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region      : chr  \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n $ co2_2015    : num  0.262 1.6 3.8 5.97 1.22 5.84 4.64 1.65 16.8 7.7 ...\n $ co2_2016    : num  0.245 1.57 3.64 6.07 1.18 5.9 4.6 1.76 17 7.7 ...\n $ co2_2017    : num  0.247 1.61 3.56 6.27 1.14 5.89 4.55 1.7 17 7.94 ...\n $ co2_2018    : num  0.254 1.59 3.69 6.12 1.12 5.88 4.41 1.89 16.9 7.75 ...\n $ cpi_2012    : int  8 33 34 NA 22 NA 35 34 85 69 ...\n $ cpi_2013    : int  8 31 36 NA 23 NA 34 36 81 69 ...\n $ cpi_2014    : int  12 33 36 NA 19 NA 34 37 80 72 ...\n $ cpi_2015    : int  11 36 36 NA 15 NA 32 35 79 76 ...\n $ cpi_2016    : int  15 39 34 NA 18 NA 36 33 79 75 ...\n $ cpi_2017    : int  15 38 33 NA 19 NA 39 35 77 75 ...\n $ lifeexp_2012: num  60.8 77.8 76.8 82.4 61.3 76.7 76 74.7 82.5 81 ...\n $ lifeexp_2013: num  61.3 77.9 76.9 82.5 61.9 76.8 76.1 75.2 82.6 81.2 ...\n $ lifeexp_2014: num  61.2 77.9 77 82.5 62.8 76.8 76.4 75.3 82.5 81.4 ...\n $ lifeexp_2015: num  61.2 78 77.1 82.6 63.3 76.9 76.5 75.3 82.5 81.5 ...\n $ lifeexp_2016: num  61.2 78.1 77.4 82.7 63.8 77 76.5 75.4 82.5 81.7 ...\n $ lifeexp_2017: num  63.4 78.2 77.7 82.7 64.2 77 76.7 75.6 82.4 81.8 ...\n $ lifeexp_2018: num  63.7 78.3 77.9 NA 64.6 77.2 76.8 75.8 82.5 81.9 ...\n[1] \"COUNTRY\"      \"gdp_2015\"     \"gini_2015\"    \"region\"       \"co2_2015\"    \n [6] \"co2_2016\"     \"co2_2017\"     \"co2_2018\"     \"cpi_2012\"     \"cpi_2013\"    \n[11] \"cpi_2014\"     \"cpi_2015\"     \"cpi_2016\"     \"cpi_2017\"     \"lifeexp_2012\"\n[16] \"lifeexp_2013\" \"lifeexp_2014\" \"lifeexp_2015\" \"lifeexp_2016\" \"lifeexp_2017\"\n[21] \"lifeexp_2018\"\n\n\n [1] \"COUNTRY\"      \"GDP\"          \"GINI\"         \"region\"       \"co2_2015\"    \n [6] \"co2_2016\"     \"co2_2017\"     \"co2_2018\"     \"cpi_2012\"     \"cpi_2013\"    \n[11] \"cpi_2014\"     \"cpi_2015\"     \"cpi_2016\"     \"cpi_2017\"     \"lifeexp_2012\"\n[16] \"lifeexp_2013\" \"lifeexp_2014\" \"lifeexp_2015\" \"lifeexp_2016\" \"lifeexp_2017\"\n[21] \"lifeexp_2018\"\ndf1 &lt;- df[1:98, ]\ndf2 &lt;- df[99:195, ]\nrbind(df1, df2)\nAn even more robust approach is to use the merge() function. This allows for the two data frames to have different variables and similar observations. As long as there is at least one variable common to both data frames, they can be merged. Here is a very simple example.\ndf1 &lt;- df[1:5, c(\"COUNTRY\", \"region\")]\ndf2 &lt;- df[1:7, c(\"COUNTRY\", \"GDP\", \"GINI\")]\nmerge(df1, df2, by = \"COUNTRY\")\n\n      COUNTRY         region   GDP GINI\n1 Afghanistan Asia & Pacific   574 36.8\n2     Albania         Europe  4520 29.0\n3     Algeria    Arab States  4780 27.6\n4     Andorra         Europe 42100 40.0\n5      Angola         Africa  3750 42.6\nNote that df2 has 7 observations while df1 only has 5. Yet, the output of the merge has 5 observations. This is because the arguments all.x and all.y are set to FALSE by default. This means that only rows that appear in both are present in the output. If we set all.y = TRUE, all the rows of df2 are added with missing values for region.\nmerge(df1, df2, by = \"COUNTRY\", all.y = TRUE)\n\n              COUNTRY         region   GDP GINI\n1         Afghanistan Asia & Pacific   574 36.8\n2             Albania         Europe  4520 29.0\n3             Algeria    Arab States  4780 27.6\n4             Andorra         Europe 42100 40.0\n5              Angola         Africa  3750 42.6\n6 Antigua and Barbuda           &lt;NA&gt; 13300 40.0\n7           Argentina           &lt;NA&gt; 10600 41.8\nIf you want to keep all the rows in both data frames, the argument all = TRUE sets both all.x = TRUE and all.y = TRUE.\nmerge(df1, df2, by = \"COUNTRY\", all = TRUE)\n\n              COUNTRY         region   GDP GINI\n1         Afghanistan Asia & Pacific   574 36.8\n2             Albania         Europe  4520 29.0\n3             Algeria    Arab States  4780 27.6\n4             Andorra         Europe 42100 40.0\n5              Angola         Africa  3750 42.6\n6 Antigua and Barbuda           &lt;NA&gt; 13300 40.0\n7           Argentina           &lt;NA&gt; 10600 41.8\nSuppose the variable you are merging on has different names in the two data frames. The arguments by.x and by.y allow for you to specify both variables.\nnames(df1)[1] &lt;- \"country\"\nmerge(df1, df2, by.x = \"country\", by.y = \"COUNTRY\")\n\n      country         region   GDP GINI\n1 Afghanistan Asia & Pacific   574 36.8\n2     Albania         Europe  4520 29.0\n3     Algeria    Arab States  4780 27.6\n4     Andorra         Europe 42100 40.0\n5      Angola         Africa  3750 42.6\nIf the two data frames have different variables with the same name, the merge will not combine these columns. This even applies if the columns are different types.\ndf1 &lt;- df[c(\"COUNTRY\", \"region\", \"GDP\")]\ndf1$GDP &lt;- as.character(df1$GDP) # GDP is now character in df1\nmerge(df1, df2, by = \"COUNTRY\")\n\n              COUNTRY              region GDP.x GDP.y GINI\n1         Afghanistan      Asia & Pacific   574   574 36.8\n2             Albania              Europe  4520  4520 29.0\n3             Algeria         Arab States  4780  4780 27.6\n4             Andorra              Europe 42100 42100 40.0\n5              Angola              Africa  3750  3750 42.6\n6 Antigua and Barbuda South/Latin America 13300 13300 40.0\n7           Argentina South/Latin America 10600 10600 41.8",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "09_Merging_and_Reshaping_Data.html#merge-tidyverse-functions",
    "href": "09_Merging_and_Reshaping_Data.html#merge-tidyverse-functions",
    "title": "9  Merging and Reshaping Data",
    "section": "9.2 Merge: tidyverse functions",
    "text": "9.2 Merge: tidyverse functions\nMerging data frames is useful when there are several data frames with similar observations but different variables. To demonstrate the join functions in dplyr, we have two datasets. One is the population of all countries and the other is the population of all countries that begin with “A.” Neither of these datasets have duplicates.\n\npop &lt;- read_csv(\"Data/Gapminder/population.csv\")\n\nRows: 195 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (1): population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopA &lt;- read_csv(\"Data/Gapminder/population_A.csv\")\n\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (1): population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntib1 &lt;- read_csv(\"Data/Gapminder/gapminder.csv\")\n\nRows: 197 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, region\ndbl (2): gdp, gini\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr((tib1))\n\nspc_tbl_ [197 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country: chr [1:197] \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ gdp    : num [1:197] 574 4520 4780 42100 3750 13300 10600 3920 55100 47800 ...\n $ gini   : num [1:197] 36.8 29 27.6 40 42.6 40 41.8 31.9 32.3 30.6 ...\n $ region : chr [1:197] \"Asia & Pacific\" \"Europe\" \"Arab States\" \"Europe\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   gdp = col_double(),\n  ..   gini = col_double(),\n  ..   region = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThe different join functions relate to which observations are kept. In full_join(), all observations in the two data frames are kept, even if there are unmatched observations. The argument by indicates which variable on which to match.\n\nfull_join(tib1, pop, by = \"country\")\n\n# A tibble: 197 × 5\n   country               gdp  gini region              population\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n 1 Afghanistan           574  36.8 Asia & Pacific        34400000\n 2 Albania              4520  29   Europe                 2890000\n 3 Algeria              4780  27.6 Arab States           39700000\n 4 Andorra             42100  40   Europe                   78000\n 5 Angola               3750  42.6 Africa                27900000\n 6 Antigua and Barbuda 13300  40   South/Latin America      93600\n 7 Argentina           10600  41.8 South/Latin America   43100000\n 8 Armenia              3920  31.9 Europe                 2930000\n 9 Australia           55100  32.3 Asia & Pacific        23900000\n10 Austria             47800  30.6 Europe                 8680000\n# ℹ 187 more rows\n\n\nThe function inner_join() only keeps observations that are present in both data frames. In this case, that is only countries that begin with “A.”\n\ninner_join(tib1, popA, by = \"country\")\n\n# A tibble: 11 × 5\n   country               gdp  gini region              population\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n 1 Afghanistan           574  36.8 Asia & Pacific        34400000\n 2 Albania              4520  29   Europe                 2890000\n 3 Algeria              4780  27.6 Arab States           39700000\n 4 Andorra             42100  40   Europe                   78000\n 5 Angola               3750  42.6 Africa                27900000\n 6 Antigua and Barbuda 13300  40   South/Latin America      93600\n 7 Argentina           10600  41.8 South/Latin America   43100000\n 8 Armenia              3920  31.9 Europe                 2930000\n 9 Australia           55100  32.3 Asia & Pacific        23900000\n10 Austria             47800  30.6 Europe                 8680000\n11 Azerbaijan           6060  32.4 Asia & Pacific         9620000\n\n\nThe function left_join() only keeps from the data frame in the left argument (tib1 in this case).\n\nleft_join(tib1, popA, by = \"country\")\n\n# A tibble: 197 × 5\n   country               gdp  gini region              population\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n 1 Afghanistan           574  36.8 Asia & Pacific        34400000\n 2 Albania              4520  29   Europe                 2890000\n 3 Algeria              4780  27.6 Arab States           39700000\n 4 Andorra             42100  40   Europe                   78000\n 5 Angola               3750  42.6 Africa                27900000\n 6 Antigua and Barbuda 13300  40   South/Latin America      93600\n 7 Argentina           10600  41.8 South/Latin America   43100000\n 8 Armenia              3920  31.9 Europe                 2930000\n 9 Australia           55100  32.3 Asia & Pacific        23900000\n10 Austria             47800  30.6 Europe                 8680000\n# ℹ 187 more rows\n\n\nThe function right_join() is the same except it only keeps the observations from the data frame in the right argument.\n\nright_join(tib1, popA, by = \"country\")\n\n# A tibble: 11 × 5\n   country               gdp  gini region              population\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n 1 Afghanistan           574  36.8 Asia & Pacific        34400000\n 2 Albania              4520  29   Europe                 2890000\n 3 Algeria              4780  27.6 Arab States           39700000\n 4 Andorra             42100  40   Europe                   78000\n 5 Angola               3750  42.6 Africa                27900000\n 6 Antigua and Barbuda 13300  40   South/Latin America      93600\n 7 Argentina           10600  41.8 South/Latin America   43100000\n 8 Armenia              3920  31.9 Europe                 2930000\n 9 Australia           55100  32.3 Asia & Pacific        23900000\n10 Austria             47800  30.6 Europe                 8680000\n11 Azerbaijan           6060  32.4 Asia & Pacific         9620000\n\n\nThe function semi_join() keeps all rows in tib1 that have a match in popA.\n\nsemi_join(tib1, popA, by = \"country\")\n\n# A tibble: 11 × 4\n   country               gdp  gini region             \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Afghanistan           574  36.8 Asia & Pacific     \n 2 Albania              4520  29   Europe             \n 3 Algeria              4780  27.6 Arab States        \n 4 Andorra             42100  40   Europe             \n 5 Angola               3750  42.6 Africa             \n 6 Antigua and Barbuda 13300  40   South/Latin America\n 7 Argentina           10600  41.8 South/Latin America\n 8 Armenia              3920  31.9 Europe             \n 9 Australia           55100  32.3 Asia & Pacific     \n10 Austria             47800  30.6 Europe             \n11 Azerbaijan           6060  32.4 Asia & Pacific     \n\n\nThe function anti_join() keeps all rows in tib1 that do not have a match in popA.\n\nanti_join(tib1, popA, by = \"country\")\n\n# A tibble: 186 × 4\n   country      gdp  gini region             \n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 Bahamas    27500  43.7 South/Latin America\n 2 Bahrain    22400  40   Arab States        \n 3 Bangladesh  1000  32.3 Asia & Pacific     \n 4 Barbados   15800  43.8 South/Latin America\n 5 Belarus     6380  26.9 Europe             \n 6 Belgium    45500  27.8 Europe             \n 7 Belize      4300  53.3 South/Latin America\n 8 Benin       1130  46.9 Africa             \n 9 Bhutan      2780  38   Asia & Pacific     \n10 Bolivia     2360  46.3 South/Latin America\n# ℹ 176 more rows",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "09_Merging_and_Reshaping_Data.html#reshape-tidyr",
    "href": "09_Merging_and_Reshaping_Data.html#reshape-tidyr",
    "title": "9  Merging and Reshaping Data",
    "section": "9.3 Reshape: tidyr",
    "text": "9.3 Reshape: tidyr\nThe tidyr package provides an efficient way to reshape and reformat data.\n\ntib1 &lt;- read_csv(\"Data/Gapminder/gapminder_large.csv\")\n\nRows: 195 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): country, region\ndbl (19): gdp_2015, gini_2015, co2_2015, co2_2016, co2_2017, co2_2018, cpi_2...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(tib1)\n\n# A tibble: 6 × 21\n  country gdp_2015 gini_2015 region co2_2015 co2_2016 co2_2017 co2_2018 cpi_2012\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Afghan…      574      36.8 Asia …    0.262    0.245    0.247    0.254        8\n2 Albania     4520      29   Europe    1.6      1.57     1.61     1.59        33\n3 Algeria     4780      27.6 Arab …    3.8      3.64     3.56     3.69        34\n4 Andorra    42100      40   Europe    5.97     6.07     6.27     6.12        NA\n5 Angola      3750      42.6 Africa    1.22     1.18     1.14     1.12        22\n6 Antigu…    13300      40   South…    5.84     5.9      5.89     5.88        NA\n# ℹ 12 more variables: cpi_2013 &lt;dbl&gt;, cpi_2014 &lt;dbl&gt;, cpi_2015 &lt;dbl&gt;,\n#   cpi_2016 &lt;dbl&gt;, cpi_2017 &lt;dbl&gt;, lifeexp_2012 &lt;dbl&gt;, lifeexp_2013 &lt;dbl&gt;,\n#   lifeexp_2014 &lt;dbl&gt;, lifeexp_2015 &lt;dbl&gt;, lifeexp_2016 &lt;dbl&gt;,\n#   lifeexp_2017 &lt;dbl&gt;, lifeexp_2018 &lt;dbl&gt;\n\n\nWide data have one row per unit while long data have more than one row per unit. To convert wide data to long, use pivot_longer(). There are several different ways to get the same output.\n\n# Select columns using tidy-select\n# Dictate pattern with names_sep\nlong_tib1 &lt;- pivot_longer(tib1,\n                       contains(\"_\"),\n                       names_to = c(\"var\", \"year\"),\n                       names_sep = \"_\")\n\n# Select columns using column indices\npivot_longer(tib1,\n             5:21,\n             names_to = c(\"var\", \"year\"),\n             names_sep = \"_\")\n\n# A tibble: 3,315 × 7\n   country     gdp_2015 gini_2015 region         var   year   value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 Afghanistan      574      36.8 Asia & Pacific co2   2015   0.262\n 2 Afghanistan      574      36.8 Asia & Pacific co2   2016   0.245\n 3 Afghanistan      574      36.8 Asia & Pacific co2   2017   0.247\n 4 Afghanistan      574      36.8 Asia & Pacific co2   2018   0.254\n 5 Afghanistan      574      36.8 Asia & Pacific cpi   2012   8    \n 6 Afghanistan      574      36.8 Asia & Pacific cpi   2013   8    \n 7 Afghanistan      574      36.8 Asia & Pacific cpi   2014  12    \n 8 Afghanistan      574      36.8 Asia & Pacific cpi   2015  11    \n 9 Afghanistan      574      36.8 Asia & Pacific cpi   2016  15    \n10 Afghanistan      574      36.8 Asia & Pacific cpi   2017  15    \n# ℹ 3,305 more rows\n\n# Dictate pattern with names_pattern\npivot_longer(tib1,\n             5:21,\n             names_to = c(\"var\", \"year\"),\n             names_pattern = \"(.*)_(.*)\")\n\n# A tibble: 3,315 × 7\n   country     gdp_2015 gini_2015 region         var   year   value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 Afghanistan      574      36.8 Asia & Pacific co2   2015   0.262\n 2 Afghanistan      574      36.8 Asia & Pacific co2   2016   0.245\n 3 Afghanistan      574      36.8 Asia & Pacific co2   2017   0.247\n 4 Afghanistan      574      36.8 Asia & Pacific co2   2018   0.254\n 5 Afghanistan      574      36.8 Asia & Pacific cpi   2012   8    \n 6 Afghanistan      574      36.8 Asia & Pacific cpi   2013   8    \n 7 Afghanistan      574      36.8 Asia & Pacific cpi   2014  12    \n 8 Afghanistan      574      36.8 Asia & Pacific cpi   2015  11    \n 9 Afghanistan      574      36.8 Asia & Pacific cpi   2016  15    \n10 Afghanistan      574      36.8 Asia & Pacific cpi   2017  15    \n# ℹ 3,305 more rows\n\n\nTo go from long data to wide, use pivot_wider. There are several options to dictate the names of the newly created variables.\n\nwide_tib1 &lt;- pivot_wider(long_tib1,\n                      names_from = c(\"var\", \"year\"),\n                      values_from = \"value\")\nhead(wide_tib1)\n\n# A tibble: 6 × 21\n  country region gdp_2015 gini_2015 co2_2015 co2_2016 co2_2017 co2_2018 cpi_2012\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Afghan… Asia …      574      36.8    0.262    0.245    0.247    0.254        8\n2 Albania Europe     4520      29      1.6      1.57     1.61     1.59        33\n3 Algeria Arab …     4780      27.6    3.8      3.64     3.56     3.69        34\n4 Andorra Europe    42100      40      5.97     6.07     6.27     6.12        NA\n5 Angola  Africa     3750      42.6    1.22     1.18     1.14     1.12        22\n6 Antigu… South…    13300      40      5.84     5.9      5.89     5.88        NA\n# ℹ 12 more variables: cpi_2013 &lt;dbl&gt;, cpi_2014 &lt;dbl&gt;, cpi_2015 &lt;dbl&gt;,\n#   cpi_2016 &lt;dbl&gt;, cpi_2017 &lt;dbl&gt;, lifeexp_2012 &lt;dbl&gt;, lifeexp_2013 &lt;dbl&gt;,\n#   lifeexp_2014 &lt;dbl&gt;, lifeexp_2015 &lt;dbl&gt;, lifeexp_2016 &lt;dbl&gt;,\n#   lifeexp_2017 &lt;dbl&gt;, lifeexp_2018 &lt;dbl&gt;\n\npivot_wider(long_tib1,\n            names_from = c(\"var\", \"year\"),\n            values_from = \"value\",\n            names_sep = \".\")\n\n# A tibble: 195 × 21\n   country         region gdp.2015 gini.2015 co2.2015 co2.2016 co2.2017 co2.2018\n   &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Afghanistan     Asia …      574      36.8    0.262    0.245    0.247    0.254\n 2 Albania         Europe     4520      29      1.6      1.57     1.61     1.59 \n 3 Algeria         Arab …     4780      27.6    3.8      3.64     3.56     3.69 \n 4 Andorra         Europe    42100      40      5.97     6.07     6.27     6.12 \n 5 Angola          Africa     3750      42.6    1.22     1.18     1.14     1.12 \n 6 Antigua and Ba… South…    13300      40      5.84     5.9      5.89     5.88 \n 7 Argentina       South…    10600      41.8    4.64     4.6      4.55     4.41 \n 8 Armenia         Europe     3920      31.9    1.65     1.76     1.7      1.89 \n 9 Australia       Asia …    55100      32.3   16.8     17       17       16.9  \n10 Austria         Europe    47800      30.6    7.7      7.7      7.94     7.75 \n# ℹ 185 more rows\n# ℹ 13 more variables: cpi.2012 &lt;dbl&gt;, cpi.2013 &lt;dbl&gt;, cpi.2014 &lt;dbl&gt;,\n#   cpi.2015 &lt;dbl&gt;, cpi.2016 &lt;dbl&gt;, cpi.2017 &lt;dbl&gt;, lifeexp.2012 &lt;dbl&gt;,\n#   lifeexp.2013 &lt;dbl&gt;, lifeexp.2014 &lt;dbl&gt;, lifeexp.2015 &lt;dbl&gt;,\n#   lifeexp.2016 &lt;dbl&gt;, lifeexp.2017 &lt;dbl&gt;, lifeexp.2018 &lt;dbl&gt;\n\n\nIt might be useful to reference this chapter on strings and regular expressions. There are many ways to represent different patterns in character strings, and a standardized approach exists to minimize the need to type out everything explicitly.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "09_Merging_and_Reshaping_Data.html#further-reading",
    "href": "09_Merging_and_Reshaping_Data.html#further-reading",
    "title": "9  Merging and Reshaping Data",
    "section": "9.4 Further Reading",
    "text": "9.4 Further Reading\nThe above information comes from chapters 5.1-5.3, 6, and 21 of Boehmke (2016), chapters 2.2.5 and 3 of Zamora Saiz et al. (2020).\n\n9.4.1 References\n\n\n\n\n\nBoehmke, Bradley C. 2016. Data Wrangling with R. Use R! Springer. https://link.springer.com/book/10.1007/978-3-319-45599-0.\n\n\nZamora Saiz, Alfonso, Carlos Quesada González, Lluís Hurtado Gil, and Diego Mondéjar Ruiz. 2020. An Introduction to Data Analysis in R: Hands-on Coding, Data Mining, Visualization and Statistics from Scratch. https://link.springer.com/book/10.1007/978-3-030-48997-7.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "10_Summary_Statistics.html",
    "href": "10_Summary_Statistics.html",
    "title": "10  Summary Statistics",
    "section": "",
    "text": "10.1 Built-in Functions\nThis chapter covers statistics useful to understand and describe data that are already pre-processed. At the end of this chapter is an interactive exercise to write a function to output a table of descriptive statistics.\nHere are all the libraries you should install for this chapter.\nAgain, we will use the dataset gapminder_large.csv which contains measures of development, environment, and society from the countries in the world. GDP and the Gini Index are measured in 2015. The Corruption Perception Index (cpi) is measured between 2012 and 2017. A higher score means less corruption. Life expectancy (lifeexp) is measured between 2012 and 2018. It is measured in years and is the average number of years a newborn would live holding constant contemporaneous mortality patterns. C02 emissions (co2) is measured between 2015 and 2018. The units are metric tonnes of CO2 per person.\ndf &lt;- read.csv(\"Data/Gapminder/gapminder_large.csv\")\nFirst, we want to get a sense of the data. How many observations are there? How many variables? What are the names of the variables and what classes are they?\nhead(df) # Display the first 6 rows\n\n              country gdp_2015 gini_2015              region co2_2015 co2_2016\n1         Afghanistan      574      36.8      Asia & Pacific    0.262    0.245\n2             Albania     4520      29.0              Europe    1.600    1.570\n3             Algeria     4780      27.6         Arab States    3.800    3.640\n4             Andorra    42100      40.0              Europe    5.970    6.070\n5              Angola     3750      42.6              Africa    1.220    1.180\n6 Antigua and Barbuda    13300      40.0 South/Latin America    5.840    5.900\n  co2_2017 co2_2018 cpi_2012 cpi_2013 cpi_2014 cpi_2015 cpi_2016 cpi_2017\n1    0.247    0.254        8        8       12       11       15       15\n2    1.610    1.590       33       31       33       36       39       38\n3    3.560    3.690       34       36       36       36       34       33\n4    6.270    6.120       NA       NA       NA       NA       NA       NA\n5    1.140    1.120       22       23       19       15       18       19\n6    5.890    5.880       NA       NA       NA       NA       NA       NA\n  lifeexp_2012 lifeexp_2013 lifeexp_2014 lifeexp_2015 lifeexp_2016 lifeexp_2017\n1         60.8         61.3         61.2         61.2         61.2         63.4\n2         77.8         77.9         77.9         78.0         78.1         78.2\n3         76.8         76.9         77.0         77.1         77.4         77.7\n4         82.4         82.5         82.5         82.6         82.7         82.7\n5         61.3         61.9         62.8         63.3         63.8         64.2\n6         76.7         76.8         76.8         76.9         77.0         77.0\n  lifeexp_2018\n1         63.7\n2         78.3\n3         77.9\n4           NA\n5         64.6\n6         77.2\n\ndim(df) # Confirm the number of rows and columns\n\n[1] 195  21\n\nnames(df) # List the variable names\n\n [1] \"country\"      \"gdp_2015\"     \"gini_2015\"    \"region\"       \"co2_2015\"    \n [6] \"co2_2016\"     \"co2_2017\"     \"co2_2018\"     \"cpi_2012\"     \"cpi_2013\"    \n[11] \"cpi_2014\"     \"cpi_2015\"     \"cpi_2016\"     \"cpi_2017\"     \"lifeexp_2012\"\n[16] \"lifeexp_2013\" \"lifeexp_2014\" \"lifeexp_2015\" \"lifeexp_2016\" \"lifeexp_2017\"\n[21] \"lifeexp_2018\"\n\nsapply(df, typeof)\n\n     country     gdp_2015    gini_2015       region     co2_2015     co2_2016 \n \"character\"    \"integer\"     \"double\"  \"character\"     \"double\"     \"double\" \n    co2_2017     co2_2018     cpi_2012     cpi_2013     cpi_2014     cpi_2015 \n    \"double\"     \"double\"    \"integer\"    \"integer\"    \"integer\"    \"integer\" \n    cpi_2016     cpi_2017 lifeexp_2012 lifeexp_2013 lifeexp_2014 lifeexp_2015 \n   \"integer\"    \"integer\"     \"double\"     \"double\"     \"double\"     \"double\" \nlifeexp_2016 lifeexp_2017 lifeexp_2018 \n    \"double\"     \"double\"     \"double\"\nThe function mean() calculates the arithmetic mean. Here is a simple demonstration of it with a vector of 50 draws from the N(0,1) distribution.\nx &lt;- rnorm(50, mean = 0, sd = 1)\nmean(x)\n\n[1] -0.302709\nIf there are any elements of the input that are NA, you must specify the argument na.rm = TRUE. Otherwise, the result will be NA.\nmean(df$gdp_2015)\n\n[1] NA\n\nmean(df$gdp_2015, na.rm = TRUE)\n\n[1] 14298.43\nIf you will only be using one data frame and do not want to repeatedly call variables using the format above, you can attach the data and then refer just to the variable name.\nattach(df)\nmean(gdp_2015, na.rm = TRUE)\n\n[1] 14298.43\nWhile this is convenient, it is not always clear to which data frame the variable belongs. Also, if any variables have the same names as functions, those functions will be masked. This chapter thus relies on data$colname format for clarity. Let us detach the dataset and continue.\ndetach(df)\nTo calculate the mean for more than one column, we can use apply-like functions. Here, we are calculating the mean of every column except country and region, which are string variables.\nsapply(df[, -c(1, 4)], mean, na.rm = TRUE)\n\n    gdp_2015    gini_2015     co2_2015     co2_2016     co2_2017     co2_2018 \n14298.427807    38.932821     4.456147     4.423509     4.446485     4.455041 \n    cpi_2012     cpi_2013     cpi_2014     cpi_2015     cpi_2016     cpi_2017 \n   42.906977    42.306358    42.929825    42.339394    42.687861    42.790960 \nlifeexp_2012 lifeexp_2013 lifeexp_2014 lifeexp_2015 lifeexp_2016 lifeexp_2017 \n   71.309091    71.642781    71.867380    72.144385    72.448128    72.737433 \nlifeexp_2018 \n   72.969022\nThe median is calculated with a function that is very similar to the mean function.\nmedian(x)\n\n[1] -0.1886716\n\nmedian(df$gini_2015, na.rm = TRUE)\n\n[1] 39.1\nThe function quantile() allows you to calculate other percentiles. Without specifying the probabilities in the probs argument, the function automatically outputs the minimum and maximum values, and the 25th, 50th, and 75th percentiles.\nquantile(x)\n\n        0%        25%        50%        75%       100% \n-3.1116502 -0.9196044 -0.1886716  0.4293506  1.6400423 \n\nquantile(df$lifeexp_2015, probs = c(0.10, 0.90), na.rm = TRUE)\n\n 10%  90% \n61.3 81.5\nHere are some functions to calculate measures of dispersion. Note the importance of specifying na.rm = TRUE.\nmin(df$co2_2015, na.rm = TRUE)\n\n[1] 0.0367\n\nmax(df$co2_2015, na.rm = TRUE)\n\n[1] 41.3\n\nrange(df$co2_2015, na.rm = TRUE)\n\n[1]  0.0367 41.3000\n\nIQR(df$co2_2015, na.rm = TRUE) \n\n[1] 5.18525\n\nvar(df$co2_2015, na.rm = TRUE) # Unbiased estimator\n\n[1] 33.79678\n\nsd(df$co2_2015, na.rm = TRUE)\n\n[1] 5.813499\nThe function summary() is a fast way to calculate many summary statistics at once. There is no need to add the na.rm = TRUE argument, and the function actually counts the number of NA values, if there are any.\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-3.1117 -0.9196 -0.1887 -0.3027  0.4294  1.6400 \n\nsummary(df$cpi_2015)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   8.00   28.00   37.00   42.34   54.00   91.00      30\nThe covariance and correlation coefficient are calculated using cov() and corr(). Specifying what to do with NA values is a little more complicated for these functions. The argument use determines the strategy more precisely. If use = \"pairwise.complete.obs\", then the covariance/correlation is only calculated for observations with two non-missing values. .\ncov(df$gdp_2015, df$co2_2015)\n\n[1] NA\n\ncov(df$gdp_2015, df$co2_2015, use = \"pairwise.complete.obs\")\n\n[1] 65913.78\n\ncor(df$gdp_2015, df$co2_2015, use = \"pairwise.complete.obs\")\n\n[1] 0.6068677\nData frames can be input into these functions, producing pairwise correlations.\ncov(df[, c(2, 3, 5)], use = \"pairwise.complete.obs\")\n\n              gdp_2015     gini_2015     co2_2015\ngdp_2015  510161355.07 -42624.157967 65913.780200\ngini_2015    -42624.16     54.232732    -7.687662\nco2_2015      65913.78     -7.687662    33.796776\n\ncor(df[, c(2, 3, 5)], use = \"pairwise.complete.obs\")\n\n            gdp_2015  gini_2015   co2_2015\ngdp_2015   1.0000000 -0.2532801  0.6068677\ngini_2015 -0.2532801  1.0000000 -0.1782023\nco2_2015   0.6068677 -0.1782023  1.0000000\nThe function t.test() performs a t-test. The arguments augment the details of the test, including the null and alternative hypotheses.\nt.test(df$lifeexp_2012, mu = 72, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  df$lifeexp_2012\nt = -1.1733, df = 186, p-value = 0.2422\nalternative hypothesis: true mean is not equal to 72\n95 percent confidence interval:\n 70.14742 72.47076\nsample estimates:\nmean of x \n 71.30909 \n\nt.test(df$lifeexp_2012, df$lifeexp_2017, paired = TRUE, var.equal = FALSE, conf.level = 0.90)\n\n\n    Paired t-test\n\ndata:  df$lifeexp_2012 and df$lifeexp_2017\nt = -12.386, df = 186, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n90 percent confidence interval:\n -1.618977 -1.237707\nsample estimates:\nmean difference \n      -1.428342\nThe function ks.test() performs the Kolmogorov-Smirnov Test to compare two distributions\nks.test(df[df$region == \"Africa\", \"co2_2015\"],\n        df[df$region == \"Middle east\", \"co2_2015\"],\n        alternative = \"two.sided\")\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  df[df$region == \"Africa\", \"co2_2015\"] and df[df$region == \"Middle east\", \"co2_2015\"]\nD = 0.7803, p-value = 2.778e-06\nalternative hypothesis: two-sided",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary Statistics</span>"
    ]
  },
  {
    "objectID": "10_Summary_Statistics.html#built-in-functions",
    "href": "10_Summary_Statistics.html#built-in-functions",
    "title": "10  Summary Statistics",
    "section": "",
    "text": "10.1.1 Practice Exercises\n\nSave the below code to an object. What is the data structure of this object? How can you extract information from this object?\n\n\nt.test(df$lifeexp_2012, mu = 72, alternative = \"two.sided\")",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary Statistics</span>"
    ]
  },
  {
    "objectID": "10_Summary_Statistics.html#tidyverse-functions",
    "href": "10_Summary_Statistics.html#tidyverse-functions",
    "title": "10  Summary Statistics",
    "section": "10.2 tidyverse Functions",
    "text": "10.2 tidyverse Functions\nThe advantages of dplyr functions and pipes are especially clear for producing summary statistics. We read in the data as a tibble.\n\ntib &lt;- read_csv(\"Data/Gapminder/gapminder_large.csv\")\n\nRows: 195 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): country, region\ndbl (19): gdp_2015, gini_2015, co2_2015, co2_2016, co2_2017, co2_2018, cpi_2...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe function summarise() allows for many types of summary statistics. The output is itself a tibble. Here are examples naming the column of the output. Note that we need to specify na.rm = TRUE.\n\ntib %&gt;%\n  summarise(\"Mean GDP 2015\" = mean(gdp_2015, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  `Mean GDP 2015`\n            &lt;dbl&gt;\n1          14298.\n\ntib %&gt;%\n  summarise(MeanGDP2015 = mean(gdp_2015, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  MeanGDP2015\n        &lt;dbl&gt;\n1      14298.\n\n\nIt is fine to refrain from naming the column. R automatically assigns the name based on the statistic.\n\ntib %&gt;%\n  summarise(mean(gdp_2015, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  `mean(gdp_2015, na.rm = TRUE)`\n                           &lt;dbl&gt;\n1                         14298.\n\n\nIt is possible to calculate many statistics at once.\n\ntib %&gt;%\n  summarise(Median = median(gdp_2015, na.rm = TRUE),\n            Variance = var(gdp_2015, na.rm =TRUE),\n            SD = sd(gdp_2015, na.rm = TRUE),\n            Minimum = min(gdp_2015, na.rm = TRUE),\n            Maximum = max(gdp_2015, na.rm = TRUE),\n            N = n())\n\n# A tibble: 1 × 6\n  Median   Variance     SD Minimum Maximum     N\n   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1   5740 510161355. 22587.     228  190000   195\n\n\nThe function across() can be used inside summarise() and mutate(). In the first argument, specify the vector of column names or indices. In the second argument, specify the function(s) to apply. The format comes from the package purrr and allows you to specify the values of the other arguments of the function.\n\ntib %&gt;%\n  summarise(across(c(2, 3, 5), ~ mean(.x, na.rm = TRUE)))\n\n# A tibble: 1 × 3\n  gdp_2015 gini_2015 co2_2015\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1   14298.      38.9     4.46\n\ntib %&gt;%\n  summarise(across(starts_with(\"co2\"), ~ median(.x, na.rm = TRUE)),\n            across(starts_with(\"lifeexp\"), ~ median(.x, na.rm = TRUE)))\n\n# A tibble: 1 × 11\n  co2_2015 co2_2016 co2_2017 co2_2018 lifeexp_2012 lifeexp_2013 lifeexp_2014\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1     2.48     2.48     2.50     2.53         73.2         73.1         73.1\n# ℹ 4 more variables: lifeexp_2015 &lt;dbl&gt;, lifeexp_2016 &lt;dbl&gt;,\n#   lifeexp_2017 &lt;dbl&gt;, lifeexp_2018 &lt;dbl&gt;\n\ntib %&gt;%\n  summarise(across(c(2, 3, 5), list(mean = ~ mean(.x, na.rm = TRUE),\n                                    median = ~ median(.x, na.rm = TRUE))))\n\n# A tibble: 1 × 6\n  gdp_2015_mean gdp_2015_median gini_2015_mean gini_2015_median co2_2015_mean\n          &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n1        14298.            5740           38.9             39.1          4.46\n# ℹ 1 more variable: co2_2015_median &lt;dbl&gt;\n\n\nNow that we are comfortable with summarise(), let’s add layers using the pipe operator. Adding group_by() beforehand allows for this.\n\ntib %&gt;%\n  group_by(region) %&gt;%\n  summarise(MeanGINI = mean(gini_2015, na.rm = TRUE),\n            N = n(),\n            N_NA = sum(is.na(gini_2015)))\n\n# A tibble: 7 × 4\n  region              MeanGINI     N  N_NA\n  &lt;chr&gt;                  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 Africa                  43.8    44     0\n2 Arab States             38.3    10     0\n3 Asia & Pacific          36.7    45     0\n4 Europe                  32.8    49     0\n5 Middle east             36.8    12     0\n6 North America           36.5     2     0\n7 South/Latin America     45.7    33     0\n\n\nWe can also filter to only focus on certain observations.\n\ntib %&gt;%\n  filter(region %in% c(\"Africa\", \"Middle east\")) %&gt;%\n  group_by(region) %&gt;%\n  summarise(Mean_Gini = mean(gini_2015),\n            SD_Gini = sd(gini_2015))\n\n# A tibble: 2 × 3\n  region      Mean_Gini SD_Gini\n  &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 Africa           43.8    7.85\n2 Middle east      36.8    4.09\n\n\nThe data itself can be transformed in the pipe operations. Here, we are creating a variable that is then summarized.\n\ntib %&gt;%\n  mutate(gini_rescaled = gini_2015/100) %&gt;%\n  group_by(region) %&gt;%\n  summarise(InterQuartileRange = IQR(gini_rescaled))\n\n# A tibble: 7 × 2\n  region              InterQuartileRange\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Africa                          0.0865\n2 Arab States                     0.088 \n3 Asia & Pacific                  0.065 \n4 Europe                          0.0720\n5 Middle east                     0.0677\n6 North America                   0.0480\n7 South/Latin America             0.067",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary Statistics</span>"
    ]
  },
  {
    "objectID": "10_Summary_Statistics.html#tables",
    "href": "10_Summary_Statistics.html#tables",
    "title": "10  Summary Statistics",
    "section": "10.3 Tables",
    "text": "10.3 Tables\n\n10.3.1 Creating Tables with stargazer\nThe package stargazer provides a simple way to output summary statistics from data frames. The simplest way to use the stargazer() function is to input a data frame. By default, it will return the LaTeX code for a table with summary statistics for all numeric variables. The default statistics are the number of observations, the mean, the standard deviation, the minimum, the 25th percentile, the 75th percentile, and the maximum.\nStargazer will return a raw output to be copied into another file. The type will determine the formatting. The type latex is for use in PDF compilation of RMarkdown files or for LaTeX programming (outside the scope of this class). The type html can be used for HTML compilation of RMarkdown files.\n\nstargazer(df, type = \"latex\")\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n% Date and time: Sun, Aug 03, 2025 - 11:08:04\n\\begin{table}[!htbp] \\centering \n  \\caption{} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lccccc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \nStatistic & \\multicolumn{1}{c}{N} & \\multicolumn{1}{c}{Mean} & \\multicolumn{1}{c}{St. Dev.} & \\multicolumn{1}{c}{Min} & \\multicolumn{1}{c}{Max} \\\\ \n\\hline \\\\[-1.8ex] \ngdp\\_2015 & 187 & 14,298.430 & 22,586.750 & 228 & 190,000 \\\\ \ngini\\_2015 & 195 & 38.933 & 7.364 & 24.800 & 63.100 \\\\ \nco2\\_2015 & 192 & 4.456 & 5.813 & 0.037 & 41.300 \\\\ \nco2\\_2016 & 192 & 4.424 & 5.644 & 0.025 & 38.500 \\\\ \nco2\\_2017 & 192 & 4.446 & 5.652 & 0.024 & 39.800 \\\\ \nco2\\_2018 & 192 & 4.455 & 5.609 & 0.024 & 38.000 \\\\ \ncpi\\_2012 & 172 & 42.907 & 19.614 & 8 & 90 \\\\ \ncpi\\_2013 & 173 & 42.306 & 19.881 & 8 & 91 \\\\ \ncpi\\_2014 & 171 & 42.930 & 19.811 & 8 & 92 \\\\ \ncpi\\_2015 & 165 & 42.339 & 20.150 & 8 & 91 \\\\ \ncpi\\_2016 & 173 & 42.688 & 19.375 & 10 & 90 \\\\ \ncpi\\_2017 & 177 & 42.791 & 18.978 & 9 & 89 \\\\ \nlifeexp\\_2012 & 187 & 71.309 & 8.052 & 48.900 & 83.600 \\\\ \nlifeexp\\_2013 & 187 & 71.643 & 7.882 & 48.500 & 83.900 \\\\ \nlifeexp\\_2014 & 187 & 71.867 & 7.752 & 48.700 & 84.200 \\\\ \nlifeexp\\_2015 & 187 & 72.144 & 7.497 & 50.500 & 84.400 \\\\ \nlifeexp\\_2016 & 187 & 72.448 & 7.296 & 51.700 & 84.700 \\\\ \nlifeexp\\_2017 & 187 & 72.737 & 7.070 & 51.900 & 84.800 \\\\ \nlifeexp\\_2018 & 184 & 72.969 & 6.968 & 52.400 & 85.000 \\\\ \n\\hline \\\\[-1.8ex] \n\\end{tabular} \n\\end{table} \n\nstargazer(df, type = \"html\")\n\n\n&lt;table style=\"text-align:center\"&gt;&lt;tr&gt;&lt;td colspan=\"6\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;Statistic&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;Mean&lt;/td&gt;&lt;td&gt;St. Dev.&lt;/td&gt;&lt;td&gt;Min&lt;/td&gt;&lt;td&gt;Max&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"6\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;gdp_2015&lt;/td&gt;&lt;td&gt;187&lt;/td&gt;&lt;td&gt;14,298.430&lt;/td&gt;&lt;td&gt;22,586.750&lt;/td&gt;&lt;td&gt;228&lt;/td&gt;&lt;td&gt;190,000&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;gini_2015&lt;/td&gt;&lt;td&gt;195&lt;/td&gt;&lt;td&gt;38.933&lt;/td&gt;&lt;td&gt;7.364&lt;/td&gt;&lt;td&gt;24.800&lt;/td&gt;&lt;td&gt;63.100&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;co2_2015&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt;td&gt;4.456&lt;/td&gt;&lt;td&gt;5.813&lt;/td&gt;&lt;td&gt;0.037&lt;/td&gt;&lt;td&gt;41.300&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;co2_2016&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt;td&gt;4.424&lt;/td&gt;&lt;td&gt;5.644&lt;/td&gt;&lt;td&gt;0.025&lt;/td&gt;&lt;td&gt;38.500&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;co2_2017&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt;td&gt;4.446&lt;/td&gt;&lt;td&gt;5.652&lt;/td&gt;&lt;td&gt;0.024&lt;/td&gt;&lt;td&gt;39.800&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;co2_2018&lt;/td&gt;&lt;td&gt;192&lt;/td&gt;&lt;td&gt;4.455&lt;/td&gt;&lt;td&gt;5.609&lt;/td&gt;&lt;td&gt;0.024&lt;/td&gt;&lt;td&gt;38.000&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;cpi_2012&lt;/td&gt;&lt;td&gt;172&lt;/td&gt;&lt;td&gt;42.907&lt;/td&gt;&lt;td&gt;19.614&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;90&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;cpi_2013&lt;/td&gt;&lt;td&gt;173&lt;/td&gt;&lt;td&gt;42.306&lt;/td&gt;&lt;td&gt;19.881&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;91&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;cpi_2014&lt;/td&gt;&lt;td&gt;171&lt;/td&gt;&lt;td&gt;42.930&lt;/td&gt;&lt;td&gt;19.811&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;92&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;cpi_2015&lt;/td&gt;&lt;td&gt;165&lt;/td&gt;&lt;td&gt;42.339&lt;/td&gt;&lt;td&gt;20.150&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;91&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;cpi_2016&lt;/td&gt;&lt;td&gt;173&lt;/td&gt;&lt;td&gt;42.688&lt;/td&gt;&lt;td&gt;19.375&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;90&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;cpi_2017&lt;/td&gt;&lt;td&gt;177&lt;/td&gt;&lt;td&gt;42.791&lt;/td&gt;&lt;td&gt;18.978&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;89&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;lifeexp_2012&lt;/td&gt;&lt;td&gt;187&lt;/td&gt;&lt;td&gt;71.309&lt;/td&gt;&lt;td&gt;8.052&lt;/td&gt;&lt;td&gt;48.900&lt;/td&gt;&lt;td&gt;83.600&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;lifeexp_2013&lt;/td&gt;&lt;td&gt;187&lt;/td&gt;&lt;td&gt;71.643&lt;/td&gt;&lt;td&gt;7.882&lt;/td&gt;&lt;td&gt;48.500&lt;/td&gt;&lt;td&gt;83.900&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;lifeexp_2014&lt;/td&gt;&lt;td&gt;187&lt;/td&gt;&lt;td&gt;71.867&lt;/td&gt;&lt;td&gt;7.752&lt;/td&gt;&lt;td&gt;48.700&lt;/td&gt;&lt;td&gt;84.200&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;lifeexp_2015&lt;/td&gt;&lt;td&gt;187&lt;/td&gt;&lt;td&gt;72.144&lt;/td&gt;&lt;td&gt;7.497&lt;/td&gt;&lt;td&gt;50.500&lt;/td&gt;&lt;td&gt;84.400&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;lifeexp_2016&lt;/td&gt;&lt;td&gt;187&lt;/td&gt;&lt;td&gt;72.448&lt;/td&gt;&lt;td&gt;7.296&lt;/td&gt;&lt;td&gt;51.700&lt;/td&gt;&lt;td&gt;84.700&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;lifeexp_2017&lt;/td&gt;&lt;td&gt;187&lt;/td&gt;&lt;td&gt;72.737&lt;/td&gt;&lt;td&gt;7.070&lt;/td&gt;&lt;td&gt;51.900&lt;/td&gt;&lt;td&gt;84.800&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td style=\"text-align:left\"&gt;lifeexp_2018&lt;/td&gt;&lt;td&gt;184&lt;/td&gt;&lt;td&gt;72.969&lt;/td&gt;&lt;td&gt;6.968&lt;/td&gt;&lt;td&gt;52.400&lt;/td&gt;&lt;td&gt;85.000&lt;/td&gt;&lt;/tr&gt;\n&lt;tr&gt;&lt;td colspan=\"6\" style=\"border-bottom: 1px solid black\"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;\n\n\nIf we want the table to show up formatted when we Knit, we need to add the option results = \"asis\" to the code chunk.\n```{r, results = \"asis\"}\nstargazer(df, type = \"html\")\n```\nHere is what that code chunk looks like. I switched the type to be latex so that it will compile in a PDF. Table 1 may appear on a different page away from the code chunk. We can add header = FALSE to remove the comment with the citation for the stargazer package.\nstargazer(df, type = \"latex\")\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Sun, Aug 03, 2025 - 11:08:05\nInputting a selected set of variables will restrict the table.\n\nstargazer(df[, 5:8], header = FALSE)\n\n\n\\begin{table}[!htbp] \\centering \n  \\caption{} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lccccc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \nStatistic & \\multicolumn{1}{c}{N} & \\multicolumn{1}{c}{Mean} & \\multicolumn{1}{c}{St. Dev.} & \\multicolumn{1}{c}{Min} & \\multicolumn{1}{c}{Max} \\\\ \n\\hline \\\\[-1.8ex] \nco2\\_2015 & 192 & 4.456 & 5.813 & 0.037 & 41.300 \\\\ \nco2\\_2016 & 192 & 4.424 & 5.644 & 0.025 & 38.500 \\\\ \nco2\\_2017 & 192 & 4.446 & 5.652 & 0.024 & 39.800 \\\\ \nco2\\_2018 & 192 & 4.455 & 5.609 & 0.024 & 38.000 \\\\ \n\\hline \\\\[-1.8ex] \n\\end{tabular} \n\\end{table} \n\n\nThere are many options to alter the output. Here are a few examples. See ?stargazer and this document for more examples. Remember to change the type to html if you are compiling the below to HTML.\nstargazer(df[, 5:8], title = \"CO2 Emissions\", type = \"latex\", header = FALSE)\nstargazer(df[, 5:8], summary.stat = c(\"n\", \"mean\", \"sd\"), type = \"latex\", header = FALSE)\nstargazer(df[, 5:8], flip = TRUE, type = \"latex\", header = FALSE)",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary Statistics</span>"
    ]
  },
  {
    "objectID": "10_Summary_Statistics.html#advanced-creating-tables-from-scratch",
    "href": "10_Summary_Statistics.html#advanced-creating-tables-from-scratch",
    "title": "10  Summary Statistics",
    "section": "10.4 Advanced: Creating Tables from Scratch",
    "text": "10.4 Advanced: Creating Tables from Scratch\nThis is outside the scope of the class.\nThese exercises will take you through creating a function that outputs a table of summary statistics for inclusion in a LaTeX document.\n\nThis function will take a tibble of summary statistics as an input. Create a tibble that lists, for each region, the mean, standard deviation, minimum, maximum, and number of non-missing observations for 2015 life expectancy. Save this tibble to an object so you can access it.\n\n\ngapminder &lt;- read_csv(\"Data/Gapminder/gapminder_large.csv\")\nout &lt;- gapminder %&gt;%\n  group_by(region) %&gt;%\n  summarise(mean(lifeexp_2015, na.rm = TRUE),\n            sd(lifeexp_2015, na.rm = TRUE),\n            min(lifeexp_2015, na.rm = TRUE),\n            max(lifeexp_2015, na.rm = TRUE),\n            sum(!is.na(lifeexp_2015)))\n\n\nDefine the name of your function. The first argument will be a tibble like the one you produced in question 1.\n\n\ncreate_table &lt;- function(tib) {\n  \n  # Open file connection\n  # Define header lines\n  # Define body lines\n  # Define footer lines\n  # Write header, body, and footer lines\n  # Close file connection\n  \n}\n\n\nWe want to write an output to a LaTeX file. This will require a file connection. That is, you will open a file with a certain file name, write lines from the tibble of question 1, and close the file. The second argument will be the filename. A file connection is opened and closed with the following commands.\n\n\nconnection &lt;- file(filename) # Open a file\nclose(connection)\nAdd these to your function and include an argument for the filename.\ncreate_table &lt;- function(tib, filename) {\n  \n  # Open file connection\n  connection &lt;- file(filename)\n  \n  # Define header lines\n  # Define body lines\n  # Define footer lines\n  # Write header, body, and footer lines\n  \n  # Close file connection\n  close(connection)\n  \n}\n\n\nTables in LaTeX require a header and footer to open and close the tabular environment. Start by defining the footer as this is simplest. Add the following object to your function in between opening and closing the file connection. Why do we use two backslashes instead of one?\n\n\ncreate_table &lt;- function(tib, filename) {\n  # Open file connection\n  connection &lt;- file(filename)\n  \n  # Define header lines\n  # Define body lines\n  \n  # Define footer lines\n  foot &lt;- c(\"\\\\bottomrule\", \"\\\\end{tabular}\")\n  \n  # Write header, body, and footer lines\n  \n  # Close file connection\n  close(connection)\n  \n}\n\n\nTo actually write a line in the file, we will use the function writeLines(). Add this to your function. Start by writing the footer to the file. At this point, test the function out to see how the writeLines() function works.\n\n\ncreate_table &lt;- function(tib, filename) {\n  # Open file connection\n  connection &lt;- file(filename)\n  \n  # Define header lines\n  # Define body lines\n  \n  # Define footer lines\n  foot &lt;- c(\"\\\\bottomrule\", \"\\\\end{tabular}\")\n  \n  # Write header, body, and footer lines\n  writeLines(foot, connection)\n  \n  # Close file connection\n  close(connection)\n  \n}\ncreate_table(out, \"Data/Gapminder/Output_Data/test.tex\")\n\n\nNow let’s define the header. We need to begin the tabular environment, the title columns, and the alignment of the columns. The names of the columns will be the third argument. Let’s start with all centrally aligned columns. Add the header to the writeLines() function. We have a tibble with 1 column for the region and 5 columns for summary statistics. What does your file look like now? Make adjustments if there are some oddities.\n\n\ncreate_table &lt;- function(tib, filename, colnames) {\n  # Open file connection\n  connection &lt;- file(filename)\n  \n  # Define header lines\n  head &lt;- c(paste0(\"\\\\begin{tabular}{\", str_dup(\"c\", dim(tib)[2]), \"}\"),\n            \"\\\\toprule\",\n            paste(str_c(colnames, collapse = \" & \"), \"\\\\\\\\\"),\n            \"\\\\midrule\")\n  \n  # Define body lines\n  \n  # Define footer lines\n  foot &lt;- c(\"\\\\bottomrule\", \"\\\\end{tabular}\")\n  \n  # Write header, body, and footer lines\n  writeLines(c(head, foot), connection)\n  \n  # Close file connection\n  close(connection)\n  \n}\ncreate_table(out, \"Data/Gapminder/Output_Data/test.tex\", c(\"Region\", \"Mean\", \"SD\", \"Min.\", \"Max.\", \"N\"))\n\n\nFinally, loop through each row in the tibble to print each line.\n\n\ncreate_table &lt;- function(tib, filename, colnames) {\n  # Open file connection\n  connection &lt;- file(filename)\n  \n  # Define header lines\n  head &lt;- c(paste0(\"\\\\begin{tabular}{\", str_dup(\"c\", dim(tib)[2]), \"}\"),\n            \"\\\\toprule\",\n            paste(str_c(colnames, collapse = \" & \"), \"\\\\\\\\\"),\n            \"\\\\midrule\")\n  \n  # Define body lines\n  for (i in 1:dim(tib)[1]) {\n    \n    if (i == 1) {\n      body &lt;- paste(str_c(tib[i, ], collapse = \" & \"), \"\\\\\\\\\")\n    } else {\n      body &lt;- c(body, paste(str_c(tib[i, ], collapse = \" & \"), \"\\\\\\\\\"))\n    }\n    \n  }\n  \n  # Define footer lines\n  foot &lt;- c(\"\\\\bottomrule\", \"\\\\end{tabular}\")\n  \n  # Write header, body, and footer lines\n  writeLines(c(head, foot), connection)\n  \n  # Close file connection\n  close(connection)\n  \n}\ncreate_table(out, \"Data/Gapminder/Output_Data/test.tex\", c(\"Region\", \"Mean\", \"SD\", \"Min.\", \"Max.\", \"N\"))",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary Statistics</span>"
    ]
  },
  {
    "objectID": "10_Summary_Statistics.html#further-reading",
    "href": "10_Summary_Statistics.html#further-reading",
    "title": "10  Summary Statistics",
    "section": "10.5 Further Reading",
    "text": "10.5 Further Reading\nReference the dplyr cheat sheet. Higher-order moments are available in the moments package.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary Statistics</span>"
    ]
  },
  {
    "objectID": "11_Graphs.html",
    "href": "11_Graphs.html",
    "title": "11  Data Visualization",
    "section": "",
    "text": "11.1 Basic Graphs\nHere are all the libraries you should install for this chapter.\nOne of R’s biggest advantages is its potential for creating informative and high-quality plots. This chapter goes through the built-in functions to produce graphs. Even just these functions allow for a wide variety of types of plots and aesthetic adjustments. Then, this chapter goes through the functions in the package ggplot2. These functions add a professional polish to the plots that is harder to achieve with the built-in functions.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "11_Graphs.html#basic-graphs",
    "href": "11_Graphs.html#basic-graphs",
    "title": "11  Data Visualization",
    "section": "",
    "text": "11.1.1 Scatter and Line Plots\nThe function plot() is the most basic plot function. It takes many data structures as inputs, including vectors. If only one vector is specified, it is plotted against an index axis.\n\ny &lt;- 1:10\nplot(y)\n\n\n\n\n\n\n\n\nIf two vectors are specified, the first one is the horizontal axis and the second is the vertical axis.\n\nx &lt;- 11:20\nplot(x, y)\n\n\n\n\n\n\n\n\nThe argument pch adjust the symbol. See ?pch for more information, but the symbols are here for reference. You can also specify the symbol in quotation marks.\n\n\n\n\n\n\n\n\n\nThe argument cex adjusts the size of the point. The default is 1. Setting the argument larger than 1 will make it larger while setting the argument less than 1 will make it smaller. Values less than or equal to 0 will result in no points being plotted. The col argument adjusts the color of the point. To just get the names of the built-in colors, type colors(). Here is a reference that lists the names next to the shade.1\n\nplot(x, pch = 8, cex = 0.9, col = \"blue\")\n\n\n\n\n\n\n\n\n\nplot(x, pch = \"a\", cex = 1.2, col = \"mediumorchid\")\n\n\n\n\n\n\n\n\n\nplot(x, pch = 23, bg = \"palegreen\", col = \"palegreen4\", cex = 2)\n\n\n\n\n\n\n\n\nWe can change this scatter plot to a line plot by changing the type argument. The value \"p\" plots points and is the default; \"l\" plots lines (there are more types listed in the documentation). The argument lwd controls with width of the line and works the same as cex. The argument lty determines the line type. Here are the available types.\n\n\n\n\n\n\n\n\n\n\nplot(x, type = \"l\")\n\n\n\n\n\n\n\n\n\nplot(x, type = \"l\", lwd = 1.2, lty = 4)\n\n\n\n\n\n\n\n\nHere are the arguments to define the title, axes, axis labels, and the look of the plot environment. The default axis labels will correspond to the object name. The default limits of the axes will correspond to the values of the object. Changing them simply involves defining a lower and upper limit. The arguments xaxt = \"n\" and yaxt = \"n\" remove the axes. The argument bty = \"n\" removes the box surrounding the plot.\n\nplot(x, main = \"Title\", xlab = \"x-axis\", ylab = \"y-axis\")\n\n\n\n\n\n\n\n\n\nplot(x, xlim = c(0, 15))\n\n\n\n\n\n\n\n\n\nplot(x, xaxt = \"n\", yaxt = \"n\")\n\n\n\n\n\n\n\n\n\nplot(x, bty = \"n\")\n\n\n\n\n\n\n\n\nThere are many other aesthetic elements of plot() available in the function’s documentation. These basics are sufficient to demonstrate how plot() can be used with data. The tibble txhousing is one of the built-in datasets in the ggplot2 package.\n\nhead(txhousing)\n\n# A tibble: 6 × 9\n  city     year month sales   volume median listings inventory  date\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Abilene  2000     1    72  5380000  71400      701       6.3 2000 \n2 Abilene  2000     2    98  6505000  58700      746       6.6 2000.\n3 Abilene  2000     3   130  9285000  58100      784       6.8 2000.\n4 Abilene  2000     4    98  9730000  68600      785       6.9 2000.\n5 Abilene  2000     5   141 10590000  67300      794       6.8 2000.\n6 Abilene  2000     6   156 13910000  66900      780       6.6 2000.\n\ndf &lt;- as.data.frame(txhousing) # Convert to data frame to use built-in functions smoothly\n\nIt is very simple to graph the pairwise scatter plots to get a sense of the pairwise correlations.\n\ndf %&gt;%\n  select(sales, volume, median) %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nWith an actual dataset, we can use the features of plot() in a more meaningful way. The function points() allows you to layer scatter plots on the active plot.\n\nplot(df[df$city == \"Houston\", \"date\"], \n     df[df$city == \"Houston\", \"median\"],\n     pch = 16, col = \"purple\", ylim = c(75000, 220000),\n     xlab = \"Date\", ylab = \"Median Price\")\npoints(df[df$city == \"Fort Worth\", \"date\"],\n     df[df$city == \"Fort Worth\", \"median\"],\n     pch = 16, col = \"red\")\n\n\n\n\n\n\n\n\nThe function lines() works analogously.\n\nplot(df[df$city == \"Houston\", \"date\"], \n     df[df$city == \"Houston\", \"median\"],\n     type = \"l\", ylim = c(75000, 220000),\n     xlab = \"Date\", ylab = \"Median Price\")\nlines(df[df$city == \"Fort Worth\", \"date\"],\n     df[df$city == \"Fort Worth\", \"median\"],\n     lty = 5)\n\n\n\n\n\n\n\n\n\n\n11.1.2 Bar Plots\nBar plots are useful for visualizing categorical data. The built-in data frame USPersonalExpenditure is useful for illustration, following chapter 4.1 of Zamora Saiz et al. (2020).\n\nUSPersonalExpenditure\n\n                      1940   1945  1950 1955  1960\nFood and Tobacco    22.200 44.500 59.60 73.2 86.80\nHousehold Operation 10.500 15.500 29.00 36.5 46.20\nMedical and Health   3.530  5.760  9.71 14.0 21.10\nPersonal Care        1.040  1.980  2.45  3.4  5.40\nPrivate Education    0.341  0.974  1.80  2.6  3.64\n\n\nYou can plot one column of the data, in this case personal expenditure in 1960. Each bar represents a category of expenditure. The argument cex.names controls the size of the labels.\n\nbarplot(USPersonalExpenditure[, 5], \n        ylab = \"Billions USD\", cex.names = 0.3,\n        col = \"coral2\")\n\n\n\n\n\n\n\n\nPlotting over the whole dataset, the bars are automatically stacked so that now, each bar represents a column (year). The argument legend.text = TRUE makes the legend appear. The argument args.legend controls the position of the legend and the size of the text.\n\nbarplot(USPersonalExpenditure, legend.text = TRUE,\n        args.legend = c(x = 2, y = 150, cex = 0.5))\n\n\n\n\n\n\n\n\nTo unstack the plot, use the argument beside = TRUE.\n\nbarplot(USPersonalExpenditure, legend.text = TRUE,\n        args.legend = c(x = 10, y = 80, cex = 0.5),\n        beside = TRUE)\n\n\n\n\n\n\n\n\nDot charts achieve a similar goal as bar plots.\n\ndotchart(USPersonalExpenditure[, 5], main = \"Personal Expenditure, 1960\")\n\n\n\n\n\n\n\n\n\ndotchart(USPersonalExpenditure, cex = 0.5, pch = 2)\n\n\n\n\n\n\n\n\n\n\n11.1.3 Distributions\nBox and whisker plots display the median, inter-quartile range, and outliers. Recall that df is a data frame of Texas housing prices.\n\nhead(df)\n\n     city year month sales   volume median listings inventory     date\n1 Abilene 2000     1    72  5380000  71400      701       6.3 2000.000\n2 Abilene 2000     2    98  6505000  58700      746       6.6 2000.083\n3 Abilene 2000     3   130  9285000  58100      784       6.8 2000.167\n4 Abilene 2000     4    98  9730000  68600      785       6.9 2000.250\n5 Abilene 2000     5   141 10590000  67300      794       6.8 2000.333\n6 Abilene 2000     6   156 13910000  66900      780       6.6 2000.417\n\nboxplot(df$median ~ df$year, ylab = \"Median Sales\", xlab = \"Year\")\n\n\n\n\n\n\n\n\nThe plot can be saved to an object to extract the underlying numbers: the median, quartiles, and end of the whiskers (stats), sample size (n), outliers (out), and the number and names of groups (group, names).\n\nboxdf &lt;- boxplot(df$median ~ df$year)\n\n\n\n\n\n\n\nstr(boxdf)\n\nList of 6\n $ stats: num [1:5, 1:16] 55800 78700 88300 110800 158700 ...\n $ n    : num [1:16] 450 451 427 471 492 497 523 528 531 542 ...\n $ conf : num [1:2, 1:16] 85909 90691 87203 92797 90757 ...\n $ out  : num [1:59] 159500 167600 167600 169700 164000 ...\n $ group: num [1:59] 1 1 1 1 1 1 1 1 1 2 ...\n $ names: chr [1:16] \"2000\" \"2001\" \"2002\" \"2003\" ...\n\n\nThe function hist() produces histograms. The default is to plot absolute frequencies (set freq = FALSE to plot densities). The breaks are determined by R, but can be specified with the breaks argument.\n\nhist(df$median, xlab = \"Median Sales\")\n\n\n\n\n\n\n\n\n\nhist(df$median, xlab = \"Median Sales\",\n     breaks = seq(50000, 305000, 5000))\n\n\n\n\n\n\n\n\nJust like with the box and whisker plot, you can save the inputs for the histogram to an object.\n\ndfhist &lt;- hist(df$median, breaks = 15)\n\n\n\n\n\n\n\nstr(dfhist)\n\nList of 6\n $ breaks  : int [1:15] 40000 60000 80000 100000 120000 140000 160000 180000 200000 220000 ...\n $ counts  : int [1:14] 27 513 1517 1640 1684 1230 673 353 177 82 ...\n $ density : num [1:14] 1.69e-07 3.21e-06 9.50e-06 1.03e-05 1.05e-05 ...\n $ mids    : num [1:14] 50000 70000 90000 110000 130000 150000 170000 190000 210000 230000 ...\n $ xname   : chr \"df$median\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"\n\n\n\n\n11.1.4 Mathematical Functions\nThe function curve() allows you to plot mathematical functions. This can be done for simple functions.\n\ncurve(sin, from = 0, to = 2 * pi)\n\n\n\n\n\n\n\n\nIt can also be used on more complex functions, including those that are user-defined and have more than one argument.\n\ncrra &lt;- function(x, eta) {\n  if (eta == 1) {\n    log(x)\n  } else{\n    (x^(1 - eta) - 1) / (1 - eta)\n  }\n}\n\ncurve(crra(x, eta = 1.2), xlim = c(1, 3), type = \"l\")\ncurve(crra(x, eta = 2), add = TRUE)\n\n\n\n\n\n\n\n\n\n\n11.1.5 Practice Exercises\n\nCreate a plot that has 5 lines for each of the values of \\(\\eta\\) in the CRRA utility function: 0, 0.5, 1, 5, and 10.\n\n\netas &lt;- c(0, 0.5, 1, 5, 10)\n\nfor (i in 1:length(etas)) {\n  curve(crra(x, eta = etas[i]), xlim = c(1, 3),  type = \"l\", add = (i != 1))\n}",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "11_Graphs.html#ggplot2",
    "href": "11_Graphs.html#ggplot2",
    "title": "11  Data Visualization",
    "section": "11.2 ggplot2",
    "text": "11.2 ggplot2\nMany separate components are implicit in the graphs created by the built-in functions. These components, such as the axes, come together to form a unified visual representation of the data. The package ggplot2 makes these different components explicit, providing easy access to more advanced customization for more precise graphs. Even without much customization, the default styles are aesthetically pleasing, or at least more so than the baseline functions resulting from the built-in functions.\n\n11.2.1 Overview\nThe approach of ggplot2 specifies the layers of a plot, providing a “grammar” that can be applied to any situation. Compared to the built-in functions, it is much easier to combine different elements in one graph. All plots contain three main components.2\n\nData to visualize\nAesthetic mappings relating the data to aesthetics\nGeometric objects (e.g., lines and points) to represent the data in a layer (geom)\n\nIn the example below, the data are the built-in mpg dataset. The aesthetic mapping is between engine size (displ) and highway miles per gallon (hwy). The geometric layer consists of points, geom_point(). The data are specified in ggplot(), the aesthetic mapping in aes(), and a layer is added with +. This is the basic structure of a ggplot2 graph.\n\nggplot(data = mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo be clear, we can build the plot one component at a time. The below creates a plot with a dataset and default aesthetic mappings. There is no geometric element though.\n\nggplot(mpg, aes(displ, hwy))\n\n\n\n\n\n\n\n\nAdding geom_point() adds the geometric element to make a scatter plot. Without specifying anything in geom_point(), it inherits the data and aesthetic mappings of ggplot(). Note that the variables are typed directly.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n11.2.1.1 Scatter Plots\nThere are other aesthetic elements besides x and y, including color, shape, and size. Add the color element to the geometric layer, geom_point(), to change the color of the geographic elements. Note that this is outside aes().\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(color = \"cyan4\")\n\n\n\n\n\n\n\n\nThe color can also communicate another dimension of the data. Here, ggplot2 maps directly from type of car (class) to a color scale (described in the legend).\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe variable class is a character string and so the scale is discrete. If we use a continuous variable, then the scale changes.\n\nggplot(mpg, aes(x = displ, y = hwy, color = year)) +\n  geom_point()\n\n\n\n\n\n\n\n\nShape and size are controlled in similar ways as color. The shapes are the same as for the built-in functions.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(shape = 21)\n\n\n\n\n\n\n\n\nYou cannot map between a continuous variable and shape. While it is possible to map between a discrete variable and size, it is not advised and R will throw a warning.\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy, size = cyl)) +\n  geom_point(alpha = 1 / 3) # alpha adjusts the opacity\n\n\n\n\n\n\n\n\nAdding more layers in ggplot2 is straightforward. For scatter plots, it may be relevant to add a (curved) line depicting some average. The geom geom_smooth() fits a smooth plot to the data, including the standard error.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSpecify se = FALSE to remove the standard error.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe default smoothing is LOWESS. The argument scan controls the distance between a given observation and the smoothed line and must be between 0 and 1. A distance of 1 results in a very smooth curve, while a distance of 0 results in a less smooth curve. With a large dataset, you will need to load the package mgcv to fit another smoothing model.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(span = 0.2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTo fit a linear model, specify method = \"lm\". Note that the message changes to reflect the fact that a linear model is being fit.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAesthetic elements of the line can be controlled in the geom_smooth() geom.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", \n              alpha = 0.5, color = \"firebrick4\", fill = \"firebrick1\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n11.2.1.2 Practice Exercises\n\nCorrect and simplify the following specifications.\n\n\n# 1\nggplot(mpg) + \n  geom_point(aes(mpg$displ, mpg$hwy))\n\n# 2\nggplot() + \n  geom_point(mapping = aes(y = hwy, x = cty), data = mpg) +\n  geom_smooth(data = mpg, mapping = aes(cty, hwy))\n\n# 3\nggplot(diamonds, aes(carat, price)) +\n  geom_point(aes(log(brainwt), log(bodywt)), data = msleep)\n\n\nIf we add an aesthetic to the base layer, it affects all subsequent layers. What is the difference between these three plots?\n\n\n# 1\nggplot(mpg, aes(x = displ, y = hwy, color = factor(cyl))) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n# 2\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = factor(cyl))) +\n  geom_smooth(method = \"lm\")\n\n# 3\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(aes(color = factor(cyl)), method = \"lm\")\n\n\n\n11.2.1.3 Line Plots\nLine plots are created with the geom geom_line().\n\nggplot(economics, aes(date, uempmed)) +\n  geom_line()\n\n\n\n\n\n\n\n\nIn a panel dataset, you will have multiple observations for each individual unit. In this case, it is necessary to specify what is the ID variable. To exemplify, load the Oxboys dataset from the nlme package (you do not need to have this package installed to access the data).\n\ndata(Oxboys, package = \"nlme\")\nOxboys &lt;- tibble(Oxboys) # Optional step\n\nNote the difference between these two plots. If there are multiple grouping variables, aes(group = interaction(var1, var2) will combine them.\n\nggplot(Oxboys, aes(x = age, y = height)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nggplot(Oxboys, aes(x = age, y = height, group = Subject)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe aesthetic characteristics can be moved to different layers of the plot.\n\nggplot(Oxboys, aes(x = age, y = height, group = Subject)) +\n  geom_line() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(Oxboys, aes(x = age, y = height)) +\n  geom_line(aes(group = Subject)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n11.2.1.4 Categorical Data\ngeom_bar() graphs bars that are proportional to the number of units by group.\n\nggplot(mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nGraphing over groups is very intuitive using the fill aesthetic as long as the variable supplied is discrete.\n\nggplot(mpg, aes(class)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(y = class, fill = drv)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nIt is possible graph other statistics besides the count.\n\nggplot(mpg, aes(drv, displ)) +\n  geom_bar(stat = \"summary_bin\", fun = \"mean\")\n\n\n\n\n\n\n\n\nHere is another way to make the same plot as above. geom_col() also graphs bars, but the height represents the values in the data. Here, we are calculating the means to be the height of the bars. This approach may be more useful if the statistic to be calculated is more complicated.\n\ndispl_means &lt;- mpg %&gt;%\n  group_by(drv) %&gt;%\n  summarise(means = mean(displ))\ndispl_means\n\n# A tibble: 3 × 2\n  drv   means\n  &lt;chr&gt; &lt;dbl&gt;\n1 4      4.00\n2 f      2.56\n3 r      5.18\n\nggplot(displ_means, aes(x = drv, y = means)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n11.2.1.5 Distributions\nHistograms are done with geom_histogram(). Frequrency polygons are identical to histograms except with lines instead of bars. These are created with geom_freqpoly(). Both use a binwidth or bins argument to bin the data. The default is 30 bins.\n\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(bins = 40)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = hwy)) +\n  geom_freqpoly()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nDensity plots are created with geom_density(). See ?density for options to adjust the kernel. The default is \"gaussian\".\n\nggplot(mpg, aes(x = hwy)) +\n  geom_density()\n\n\n\n\n\n\n\n\nTo plot a histogram using the density rather than the number of observations (as is the default), specify that the y argument is ..density...\n\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(aes(y = ..density..), binwidth = 2)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nComparing the distributions across categorical variables can be done by including other aesthetics.\n\nggplot(mpg, aes(x = hwy, color = drv)) +\n  geom_freqpoly()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = hwy, linetype = drv)) +\n  geom_freqpoly()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThere are other plot types that can be used to examine the distribution of continuous variables across discrete categories. These are geom_jitter(), geom_boxplot(), and geom_violin().\n\nggplot(mpg, aes(x = drv, y = hwy)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = drv, y = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = drv, y = hwy)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nThe geoms geom_contour() and geom_raster() allow you to represent 3-dimensional in 2-dimensional plots.\n\n\n11.2.1.6 Practice Exercises\n\nInspect the diamonds dataset, which is built-in to ggplot2. Create a histogram of carat. Modify the bins in a way that you think best fits the data.\nHow does the density of price vary by clarity? Create a plot to demonstrate this relationship.\n\n\n\n\n11.2.2 Modifying the Look of the Plot\n\n11.2.2.1 Axes\nThe scale of the axes relate the aesthetics back to the data. These two plots are equivalent.\n\nggplot(mpg, aes(displ, hwy)) + \n  geom_point(aes(color = class))\n\n\nggplot(mpg, aes(displ, hwy)) + \n  geom_point(aes(color = class)) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  scale_color_discrete()\n\n\n\n\n\n\n\n\nIt is possible to override these default scales.\n\nggplot(mpg, aes(displ, hwy)) + \n  geom_point(aes(color = class)) +\n  scale_x_continuous(\"Engine Displacement\") +\n  scale_y_continuous(\"Highway MPG\") +\n  scale_colour_brewer(\"Class\")\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(displ, hwy)) + \n  geom_point(aes(color = class)) +\n  scale_y_continuous(trans = \"log10\")\n\n\n\n\n\n\n\n\nBecause naming the axes is such a common step, there is a short cut to easily change the axis labels with xlab, ylab, and labs. See ?plotmath for a reference on how to add mathematical expressions to the axis labels.\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  xlab(\"City Driving (MPG)\") +\n  ylab(\"Highway Driving (MPG)\") +\n  geom_point(alpha = 1 / 3)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(aes(color = drv), alpha = 1 / 3) +\n  labs(x = \"City Driving (MPG)\", \n       y = \"Highway Driving (MPG)\",\n       color = \"Drive\") \n\n\n\n\n\n\n\n\nThese arguments can be set to NULL to completely remove them.\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  xlab(NULL) +\n  ylab(NULL) +\n  geom_point(alpha = 1 / 3)\n\n\n\n\n\n\n\n\nBreaks and labels are also controlled with the scale elements. Here is how breaks and labels can be determined for the axes.\n\n# Continuous\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(alpha = 1 / 3) +\n  scale_x_continuous(breaks = seq(8, 36, 2)) +\n  scale_y_continuous(breaks = c(20, 30, 40),\n                     labels = c(\"20 MPG\", \"30 MPG\", \"40 MPG\"))\n\n\n\n\n\n\n\n\n\n# Discrete\nggplot(mpg, aes(drv, displ))  + \n  geom_bar(stat = \"summary_bin\", fun = \"mean\") +\n  scale_x_discrete(labels = c(\"4-wheel\", \"Front-wheel\", \"Rear-wheel\"))\n\n\n\n\n\n\n\n\nTo completely remove breaks and labels, set them to NULL.\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(alpha = 1 / 3) +\n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(labels = NULL)\n\n\n\n\n\n\n\n\nRefer to the scales package for additional flexibility in specifying the axis scales and labels.\nThe arguments xlim and ylim alter the limits of the axes. Note that data points will be excluded if they fall outside of the limits. These are shortcuts as limits is an argument in the scale layers.\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  xlim(c(10, 30)) +\n  ylim(c(20, 50)) +\n  geom_point(alpha = 1 / 3)\n\nWarning: Removed 80 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(alpha = 1 / 3) +\n  scale_x_continuous(limits = c(10, 30)) +\n  scale_y_continuous(limits = c(20, 50)) \n\nAdd na.rm = TRUE to suppress the warning message.\n\nggplot(mpg, aes(x = cty, y = hwy), na.rm = TRUE) + \n  xlim(c(10, 30)) +\n  ylim(c(20, 50)) +\n  geom_point(alpha = 1 / 3)\n\nThe arguments xlim and ylim can also be used for discrete axes.\n\nggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_jitter(alpha = 1 / 3)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = drv, y = hwy)) + \n  xlim(c(\"f\", \"r\")) +\n  ylim(c(NA, 30)) +\n  geom_jitter(alpha = 1 / 3)\n\nWarning: Removed 127 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n11.2.2.2 Color Scales\nContinuous scales can be changed with several functions. The colorbrewer2 colors (included in ggplot2 and accessible through the RColorBrewer package) is useful with some predefined scales, even though the scales are technically designed for maps.\n\n# Change the color of a continuous scale \nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(aes(color = displ)) +\n  scale_color_gradient(low = \"deeppink\", high = \"black\")\n\n\n\n\n\n\n\n\n\n# Palette from ColorBrewer scales \nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(aes(color = displ)) +\n  scale_color_distiller(palette = \"RdPu\")\n\n\n\n\n\n\n\n\n\n# Specify the NA color\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(aes(color = displ)) +\n  scale_color_distiller(palette = \"RdPu\", na.value = \"grey53\")\n\nTo demonstrate changing the scale of discrete data, we will use this toy plot.\n\ndf &lt;- data.frame(x = c(\"a\", \"b\", \"c\", \"d\"), y = c(3, 4, 1, 2))\nbars &lt;- ggplot(df, aes(x, y, fill = x)) +\n  geom_bar(stat = \"identity\") + \n  labs(x = NULL, y = NULL) + \n  theme(legend.position = \"none\")\nbars\n\n\n\n\n\n\n\n\nThe scale_colour_hue() function picks evenly spaced hues around the color wheel. You can specify the ranges of hues, the chroma, and the luminance. Search “HCL color space” to read more about this system. If colors have the same luninance and chroma, black and white printing will not distinguish the colors.\n\n# Adjust the chroma\nbars +\n  scale_fill_hue(c = 40)\n\n\n\n\n\n\n\n\n\n# Adjust the hue\nbars +\n  scale_fill_hue(h = c(180, 300))\n\n\n\n\n\n\n\n\n\n# Adjust the luminance\nbars +\n  scale_fill_hue(l = 100)\n\n\n\n\n\n\n\n\nThe ColorBrewer palettes can be accessed with scale_fill_brewer. Use display.brewer.all() to see the available scales.\n\nbars +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nbars +\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scale_fill_grey() maps discrete data to a grayscale (great for printing!).\n\nbars +\n  scale_fill_grey(start = 0.5, end = 1)\n\n\n\n\n\n\n\n\nYou can create your own discrete scale (or find one online) and use scale_fill_manual.\n\nbars +\n  scale_fill_manual(values = c(\"maroon3\", \"slateblue3\", \"yellow3\", \"plum3\"))\n\n\n\n\n\n\n\n\n\n# Color palettes from Wes Anderson movies\nlibrary(wesanderson)\nbars +\n  scale_fill_manual(values = wes_palette(\"Royal1\"))\n\n\n\n\n\n\n\n\n\n# Manual discrete scale with color instead of fill\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = drv)) +\n  scale_color_manual(values = c(\"maroon3\", \"slateblue3\", \"yellow3\", \"plum3\"))\n\n\n\n\n\n\n\n\nGenerally, bright colors work better for points and lines and pastel colors work better for filling in areas. You should be conscious of how the graph will be viewed (i.e., on a computer screen or printed in black and white) when selecting colors. Finally, many people are color blind. This article provides some options to ensure that your color selections are able to be understood by everyone.\n\n\n11.2.2.3 Labels\nTo add text to a plot, geom_text() offers many options to specify exactly what you would like to appear.\n\nmpg_sample &lt;- slice_sample(mpg, n = 20)\nggplot(mpg_sample, aes(x = cty, y = hwy)) +\n  geom_text(aes(label = manufacturer))\n\n\n\n\n\n\n\n\nYou can change the font of the labels with the family argument. The three fonts that will work on all computers are \"sans\" (default), \"serif\", and \"mono\". The argument fontface allows you to bold or italicize the text. The options are \"plain\" (default), \"bold\", and \"italic\".\n\nggplot(mpg_sample, aes(x = cty, y = hwy)) +\n  geom_text(aes(label = manufacturer), family = \"serif\")\n\n\n\n\n\n\n\n\n\nggplot(mpg_sample, aes(x = cty, y = hwy)) +\n  geom_text(aes(label = manufacturer), fontface = \"bold\")\n\n\n\n\n\n\n\n\nThe alignment of the text is adjusted with the hjust (\"left\", \"center\", \"right\", \"inward\", \"outward\") and vjust (\"bottom\", \"middle\", \"top\", \"inward\", \"outward\") arguments. The \"inward\" options are useful to ensure that the text stays within the limits of the plot.\n\nggplot(mpg_sample, aes(x = cty, y = hwy)) +\n  geom_text(aes(label = manufacturer), vjust = \"inward\", hjust = \"inward\")\n\n\n\n\n\n\n\n\nThe aesthetics size and angle control the fontsize and font angle of the labels. The size aesthetic uses millimeters.\n\nggplot(mpg_sample, aes(x = cty, y = hwy)) +\n  geom_text(aes(label = manufacturer), size = 2.5, angle = 30)\n\n\n\n\n\n\n\n\nLabels make more sense in combination with other geoms. The arguments nudge_x and nudge_y allow you to specify the spacing between the label and the plot elements. The argument check_overlap, if set to TRUE, will remove overlapping points, only keeping the first that appear.\n\nggplot(mpg_sample, aes(x = cty, y = hwy)) +\n  geom_point() +\n  geom_text(aes(label = manufacturer), \n            size = 3, nudge_y = -0.25, nudge_x = 0.25, \n            hjust = \"inward\", vjust = \"inward\")\n\n\n\n\n\n\n\n\n\nggplot(mpg_sample, aes(x = cty, y = hwy)) +\n  geom_point() +\n  geom_text(aes(label = manufacturer), \n            size = 3, nudge_y = -0.25, nudge_x = 0.25, hjust = \"inward\", vjust = \"inward\", check_overlap = TRUE)\n\n\n\n\n\n\n\n\nIf you have a plot with a busier background (e.g., a heat map), you may want to add labels within a rectangle. See the geom geom_label() for this scenario. The package directlabels is another extremely useful source for labeling plots.\n\n\n11.2.2.4 Annotations\nApart from labels, you may want to annotate your plots with notes, descriptions, highlighted areas, etc. These are called metadata and can be treated just as you treat your data. That is, you can use the same geoms for annotation as you would to plot data. The following geoms are particularly helpful for annotation: geom_text(), geom_rect(), geom_line(), geom_vline(), geom_hline(), and geom_abline().\n\nggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_vline(xintercept = 20, color = \"grey39\") +\n  geom_hline(yintercept = 30, color = \"grey39\") +\n  geom_point() \n\n\n\n\n\n\n\n\n\n\n11.2.2.5 Legends\nThe default creation and formatting of legends is an advantage of ggplot2. That is, the formatting relies on the structure of the data and the aesthetic mapping. This is convenient, but can also present difficulties if the data are in a different format than what you would like for the legend.\nThe legend will only include a layer if it includes some mapped aesthetic with aes(). You can remove layers from the legend by setting show.legend = FALSE or add layers to the legend by setting show.legend = TRUE.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(pch = 1, color = \"ivory4\", size = 4) +\n  geom_point(aes(color = drv)) \n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(pch = 1, color = \"ivory4\", size = 4, show.legend = TRUE) +\n  geom_point(aes(color = drv))\n\n\n\n\n\n\n\n\nThe layout of the legend is determined in the broader theme. To change the position, use legend.position (\"right\", \"left\", \"bottom\", \"top\", \"none\").\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nYou can also input a coordinate for the legend to appear inside the plot area. The bottom-left corner is (0,0) and the top-right corner is (1,1). The argument legend.justification adjusts which corner of the legend is being positioned by legend.position.\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv)) +\n  theme(legend.position = c(0.8, 0.8))\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv)) +\n  theme(legend.position = c(0.8, 0.8), \n        legend.justification = c(1, 1))\n\n\n\n\n\n\n\n\nOther arguments of theme() that control the legend layout are legend.direction, legend.box, and legend.box.just.\nThe guide functions guide_colourbar() (only for continuous scales) and guide_legend() provide additional control over the legend. The function guides() is a helper function to override the options.\n\n# These are all equivalent and plot the default legend\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = displ))\n\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = displ)) +\n  scale_fill_continuous(guide = guide_colourbar())\n\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = displ)) +\n  guides(color = guide_colourbar())\n\nHere are some examples of changes that can be made to discrete scales.\n\n# Change the number of columns\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = class)) +\n  guides(color = guide_legend(ncol = 2))\n\n\n\n\n\n\n\n\n\n# Reverse the order\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = class)) +\n  guides(color = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\n# Specify the size of the legend keys\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = class)) +\n  guides(color = guide_legend(keywidth = 2, keyheight = 2))\n\n\n\n\n\n\n\n\n\n# Override aesthetic element of plot\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = class), alpha = 0.3) +\n  guides(color = guide_legend(override.aes = list(alpha = 1)))\n\n\n\n\n\n\n\n\nHere are some examples of changes that can be made to continuous scales.\n\n# Reverse the order\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = displ)) +\n  guides(color = guide_colorbar(reverse = TRUE))\n\n\n\n\n\n\n\n\n\n# Change the size of the bar\nggplot(mpg, aes(cty, hwy)) +\n  geom_point(aes(color = displ)) +\n  guides(color = guide_colorbar(barheight = 7, barwidth = 4))\n\n\n\n\n\n\n\n\n\n\n11.2.2.6 Themes\nThemes do not substantively change the plot. Rather, they allow you to customize the plot to match your aesthetic preferences.\nWe will create this plot for demonstration. It has all of the elements discussed above already formatted.\n\nnice &lt;- ggplot(mpg, aes(cty, hwy, color = factor(cyl))) +\n  geom_jitter() +\n  geom_abline(color = \"grey50\", size = 2) + \n  labs(x = \"City mileage/gallon\",\n       y = \"Highway mileage/gallon\",\n       color = \"Cylinders\",\n       title = \"Mileage by Number of Cylinders\") +\n  scale_color_brewer(palette = \"Spectral\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nThere are complete themes that are built-in or available through packages. Here are all the built-in themes.\n\nnice +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nnice +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nnice + \n  theme_light()\n\n\n\n\n\n\n\n\n\nnice + \n  theme_dark()\n\n\n\n\n\n\n\n\n\nnice + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nnice + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nnice + \n  theme_void()\n\n\n\n\n\n\n\n\nModifying elements of the theme is done with the following format.\n\nplot + theme(element.name = element_function())\n\nThe function element_text() controls how labels and headings appear. The family, face, color, size, hjust, vjust, angle, lineheight, and margins can all be adjusted in this function.\n\nnice + \n  theme(plot.title = element_text(size = 18, color = \"khaki4\"))\n\n\n\n\n\n\n\n\nNote that any adjustment can be made to any theme.\n\nnice + \n  theme_bw() + \n  theme(plot.title = element_text(size = 18, color = \"khaki4\"))\n\n\n\n\n\n\n\n\nHere is another example adjusting the axis titles as well.\n\nnice + \n  theme(plot.title = element_text(size = 18, color = \"khaki4\"),\n        axis.title.y = element_text(family = \"serif\"),\n        axis.title.x = element_text(face = \"italic\"))\n\n\n\n\n\n\n\n\nThe function element_line() draws lines with color, size, and linetype specified. The grid lines can be adjusted with panel.grid.major and panel.grid.minor.\n\nnice +\n  theme(panel.grid.major = element_line(color = \"navy\"),\n        panel.grid.minor = element_line(size = 2))\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\nThe function element_rect() draws rectangles with fill, color, size, and linetype specified.\n\nnice +\n  theme(plot.background = element_rect(color = \"lightblue4\", size = 2))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\nnice +\n  theme(panel.background = element_rect(fill = \"mistyrose\"))\n\n\n\n\n\n\n\n\nTo remove elements, use the function element_blank().\n\nnice +\n  theme(legend.title = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\n\n11.2.2.7 Practice Exercises\n\nAdditional themes are available in the package ggthemes. Run the following to see the complete list.\n\n\nls(\"package:ggthemes\")[grepl(\"theme_\", ls(\"package:ggthemes\"))]\n\nWhich theme among the built-in themes and the ggthemes is closest to your most-preferred look? Choose one that you will then modify. What would you like to change about the theme?\n\nRefer to the Themes chapter of Wickham’s book. This chapter lists all the the elements that can be adjusted. Create your theme! Make note of any particularly pleasing (or displeasing) changes along the way.\nDefine a function with the name of your theme. This tutorial explains how to do that towards the end.\n\n\n\n\n11.2.3 Save\nOutside of loops and functions, plots are rendered directly on the screen. You can save them using the ggsave() command.\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(alpha = 1 / 3)\nggsave(\"Example1.jpg\")\n\nInside a loop or function, you can print the graph first before saving.\n\ng &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point(alpha = 1 / 3) \nprint(g)\nggsave(\"Example2.png\")\n\nThe available extensions are .eps, .pdf, .svg, .wmf, .png, .jpg, .bmp, and .tiff.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "11_Graphs.html#a-in-class-activity-practice-with-graphing",
    "href": "11_Graphs.html#a-in-class-activity-practice-with-graphing",
    "title": "11  Data Visualization",
    "section": "11.a In-Class Activity: Practice with Graphing",
    "text": "11.a In-Class Activity: Practice with Graphing\nThis activity will help you practice regression in R. Write your code in a .R file and upload it to Canvas at the end of class for participation points in today’s class. You can write answers to the questions using comments. Do not worry about finishing it, just get as far as you can. You will be graded for completion.\n\nMake sure you have ggplot2 installed and loaded. Use the functions from that package in all of the below questions.\nDownload the dataset GSS2022.dta to your computer. It is on Canvas &gt; Modules &gt; Module 1 &gt; Datasets. Open the dataset in R using read_dta() from the package haven.\nMake a bar plot of the variable tax. Refer to the documentation to learn what this variable means. Make sure you look for the place in the PDF where it lists what the values mean.\nEdit your plot from 3 to have labels for the values of tax. To do this, you can create a new variable tax_str that takes the values “Too High”, “About Right”, “Too Low”. It can be a character variable or a factor variable.\nAdd these lines to your plot to see how it changes the aesthetics of the plot.\n\n\n  labs(x = \"Do you consider the amount of federal income tax you pay as too high, about right, or too low?\",\n     y = \"Count\") +\n  theme_bw() +\n  theme(axis.text = element_text(color = \"black\", size = 12),\n        axis.title = element_text(size = 14))\n\n\nMake a bar plot of the variable partyid. Look in the documentation to see what that variable means. What is the most common response? Drop observations if partyid is 7 (other party).\nRun the below code to see how it changes the aesthetics. Do these aesthetics improve the interpretation of the graph? If you have trouble distinguishing red and blue, the colors go from blue to red as partyid goes from 0 to 6.\n\n\nggplot(gss) + \n  geom_bar(aes(x = partyid, \n               fill = as.factor(partyid))) +\n  scale_fill_manual(values = c(\"#0713F7\", \"#2615D8\", \"#4616B9\",\"#65189A\", \n                              \"#84197B\", \"#C31C3D\", \"#E21E1E\")) +\n  labs(x = \"Political Party\",\n       y = \"Count\") +\n  theme_bw() +\n  theme(axis.text = element_text(color = \"black\", size = 12),\n        axis.title = element_text(size = 14),\n        legend.position = \"none\")\n\n\nLet’s try to graphically answer the following question: Does political party correlate with opinions on the federal income tax?. Create a bar plot that graphs the count of responses to tax for each level of partyid. Use the argument position = \"dodge\" in geom_bar().\nLet’s make this even easier to interpret. Create a variable that simplifies the partyid variable into two or three parties (Democrat, Independent, Republican). Use your judgement on how to categorize these parties.\nMake a bar plot of the response to tax_str by the variable you created in question 9. The x-axis should be the values of tax_str and there should be two or three bars for each value corresponding to the parties.\nChallenging! By political party calculated in 9, calculate the percent who respond to each value of tax. Create a variable. Repeat 10 but use this variable.\n\nThis analysis is done in this paper! Check it out if you are interested.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "11_Graphs.html#further-reading",
    "href": "11_Graphs.html#further-reading",
    "title": "11  Data Visualization",
    "section": "11.3 Further Reading",
    "text": "11.3 Further Reading\nIf you prefer built-in plotting over ggplot2, see Rahlf (2017). To better understand the “grammar” of ggplot2, see Wickham (2016).\n\n11.3.1 References\n\n\n\n\n\nRahlf, Thomas. 2017. Data Visualisation with R: 100 Examples. Springer. https://web.archive.org/web/20170520070824/http://www.datavisualisation-r.com/.\n\n\nWickham, Hadley. 2016. Ggplot2. Use R! Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-24277-4.\n\n\nZamora Saiz, Alfonso, Carlos Quesada González, Lluís Hurtado Gil, and Diego Mondéjar Ruiz. 2020. An Introduction to Data Analysis in R: Hands-on Coding, Data Mining, Visualization and Statistics from Scratch. https://link.springer.com/book/10.1007/978-3-030-48997-7.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "11_Graphs.html#footnotes",
    "href": "11_Graphs.html#footnotes",
    "title": "11  Data Visualization",
    "section": "",
    "text": "If you want even more flexibility in designing color palettes, refer to this cheatsheet for some ideas of different packages.↩︎\nThe additional components are as follows. (1) Layers create the objects that actually appear on the plot. Layers have five components: data, aesthetic mappings, statistical transformation, geometric object, position adjustment. (2) Statistical transformations (e.g., binning). (3) Scales, including a legend or axes, relate the aesthetics back to the data. (4) Coordinate system relates the data to the plane of the graphic. (5) Faceting allows for the data to be broken into subsets (also called latticing or trellising). (6) Theme relates to the font size, background color.↩︎",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html",
    "href": "12_Geographic_Data_and_Mapping.html",
    "title": "12  Geographic Data and Mapping",
    "section": "",
    "text": "12.1 Geographic Data Vocabulary\nHere are the libraries you will need for this chapter.\nGeographic datasets have particular features that are useful to understand before using them for analysis and graphing. There are two models of geographic data: the vector data model and the raster data model. The vector data model uses points, lines, and polygons to represent geographic areas. The raster data model uses equally-sized cells to represent geographic areas. Vector data is usually adequate for maps in economics, as it is able to represent human-defined areas precisely. That being said, raster data is needed for some contexts (e.g., environmental studies), and can provide richness and context to maps.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html#geographic-data-vocabulary",
    "href": "12_Geographic_Data_and_Mapping.html#geographic-data-vocabulary",
    "title": "12  Geographic Data and Mapping",
    "section": "",
    "text": "12.1.1 Vector Data Model\nVector data require a coordinate reference system (CRS). There are many different options for the CRS, with different countries and regions using their own systems. The differences between the different systems include the reference point (where is \\((0,0)\\) located) and the units of the distances (e.g., km, degrees).\nThe package sf contains functions to handle different types of vector data. The trio of packages sp, rgdal, and rgeos used to be the go-to packages for vector data in R, but have been superseded by sf. It has efficiency advantages and the data can be accessed more conveniently than in the other packages. You may still see examples and StackExchange forums with these other packages, however. If needed, the following code can be used to convert an sf object to a Spatial object used in the package sp, and back to an sf object.\n\nlibrary(sp)\nexample_sp &lt;- as(example, Class = \"Spatial\")\nexample_sf &lt;- st_as_sf(example_sp)",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html#manage-geographic-data",
    "href": "12_Geographic_Data_and_Mapping.html#manage-geographic-data",
    "title": "12  Geographic Data and Mapping",
    "section": "12.2 Manage Geographic Data",
    "text": "12.2 Manage Geographic Data\nGeographic data in R are stored in a data frame (or tibble) with a column for geographic data. These are called spatial data frames (or sf object). This column is called geom or geometry. Here is an example with the world dataset from spData.\n\ndata(\"world\")\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nThe last variable, geom, is a list column. If you inspect the column using View(world$geom), you will see that each element is a list of varying lengths, corresponding to the vector data of the country. It is possible to interact with this sf object as one would a non-geographic tibble or data frame.\n\nsummary(world)\n\n    iso_a2           name_long          continent          region_un        \n Length:177         Length:177         Length:177         Length:177        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  subregion             type              area_km2             pop           \n Length:177         Length:177         Min.   :    2417   Min.   :5.630e+04  \n Class :character   Class :character   1st Qu.:   46185   1st Qu.:3.755e+06  \n Mode  :character   Mode  :character   Median :  185004   Median :1.040e+07  \n                                       Mean   :  832558   Mean   :4.282e+07  \n                                       3rd Qu.:  621860   3rd Qu.:3.075e+07  \n                                       Max.   :17018507   Max.   :1.364e+09  \n                                                          NA's   :10         \n    lifeExp        gdpPercap                   geom    \n Min.   :50.62   Min.   :   597.1   MULTIPOLYGON :177  \n 1st Qu.:64.96   1st Qu.:  3752.4   epsg:4326    :  0  \n Median :72.87   Median : 10734.1   +proj=long...:  0  \n Mean   :70.85   Mean   : 17106.0                      \n 3rd Qu.:76.78   3rd Qu.: 24232.7                      \n Max.   :83.59   Max.   :120860.1                      \n NA's   :10      NA's   :17                            \n\nattributes(world)\n\n$names\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n\n$sf_column\n[1] \"geom\"\n\n$agr\n   iso_a2 name_long continent region_un subregion      type  area_km2       pop \n     &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt; \n  lifeExp gdpPercap \n     &lt;NA&gt;      &lt;NA&gt; \nLevels: constant aggregate identity\n\n$class\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nworld\n\nSimple feature collection with 177 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 11\n   iso_a2 name_long continent region_un subregion type  area_km2     pop lifeExp\n * &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 FJ     Fiji      Oceania   Oceania   Melanesia Sove…   1.93e4  8.86e5    70.0\n 2 TZ     Tanzania  Africa    Africa    Eastern … Sove…   9.33e5  5.22e7    64.2\n 3 EH     Western … Africa    Africa    Northern… Inde…   9.63e4 NA         NA  \n 4 CA     Canada    North Am… Americas  Northern… Sove…   1.00e7  3.55e7    82.0\n 5 US     United S… North Am… Americas  Northern… Coun…   9.51e6  3.19e8    78.8\n 6 KZ     Kazakhst… Asia      Asia      Central … Sove…   2.73e6  1.73e7    71.6\n 7 UZ     Uzbekist… Asia      Asia      Central … Sove…   4.61e5  3.08e7    71.0\n 8 PG     Papua Ne… Oceania   Oceania   Melanesia Sove…   4.65e5  7.76e6    65.2\n 9 ID     Indonesia Asia      Asia      South-Ea… Sove…   1.82e6  2.55e8    68.9\n10 AR     Argentina South Am… Americas  South Am… Sove…   2.78e6  4.30e7    76.3\n# ℹ 167 more rows\n# ℹ 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt;\n\n\nThe attributes of world show the variable and row names, the name of the sf column, the attribute-geometry-relationship (agr), and the information about the object itself (class). Notably, there are some geographic characteristics attached to the data: the geometry type (most commonly, POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON, GEOMETRYCOLLECTION), the dimension, the bbox (limits of the plot), the CRS, and the name of the geographic column.\n\n12.2.1 Load Spatial Data\nAs seen above, it is possible to load geographic data built-in to R or other packages using data(). There are other packages with more complete data, rather than small examples. For example, the package osmdata connects you to the OpenStreetMap API, the package rnoaa imports NOAA climate data, and rWBclimate imports World Bank data. If you need any geographic data that might be collected by a governmental agency, check to see if there is an R package before downloading it. There may be some efficiency advantages to using the package.\nYou can also import geographic data into R directly from your computer. These will usually be stored in spatial databases, often with file extensions that one would also use in ArcGIS or a related program. The most popular format is ESRI Shapefile (.shp). The function to import data is st_read(). To see what types of files can be imported with that function, check st_drivers().\n\nsf_drivers &lt;- st_drivers()\nhead(sf_drivers)\n\n\nchi_streets &lt;- st_read(\"Data/Major_Streets/Major_Streets.shp\")\n\nReading layer `Major_Streets' from data source \n  `/Users/aziff/Desktop/1_PROJECTS_W/projects/data-science-for-economic-and-social-issues.github.io/Data/Major_Streets/Major_Streets.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 16065 features and 65 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 1093384 ymin: 1813892 xmax: 1205147 ymax: 1951666\nProjected CRS: NAD83 / Illinois East (ftUS)\n\nhead(chi_streets[, 1:10], 3)\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 1163267 ymin: 1887115 xmax: 1180175 ymax: 1904103\nProjected CRS: NAD83 / Illinois East (ftUS)\n  OBJECTID FNODE_ TNODE_ LPOLY_ RPOLY_    LENGTH TRANS_ TRANS_ID SOURCE_ID\n1        1  16580  16726      0      0 519.31945  52842   115733     15741\n2        2  18237  18363      0      0 432.44697  45410   149406     49489\n3        3  12874  12840      0      0  80.97939  23831   149515     49599\n  OLD_TRANS_                       geometry\n1     118419 LINESTRING (1163267 1893234...\n2     160784 LINESTRING (1176733 1887541...\n3     152590 LINESTRING (1180163 1904023...\n\n\nSee st_write() to output a shapefile, or other type of spatial data file. The function saveRDS() is very useful here as well, as it compresses the files.\n\n\n12.2.2 Attribute Data\nThe non-spatial (or attribute) data can be treated as you would treat any other data frame or tibble. You can use the tidyverse or built-in functions to clean, summarize, and otherwise manage the attribute data.\nThere is are a few details that can prevent frustration.\n\nBoth raster and dplyr have the function select(). If you have raster loaded, make sure you are specifying dplyr::select() when you want the tidyverse function.\nIf you want to drop the spatial element of the dataset, this can be done with st_drop_geometry(). If you are not using the data for its spatial elements, you should drop the geometry as it the list column can take up a lot of memory.\n\n\nworld_tib &lt;- st_drop_geometry(world)\nnames(world_tib)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\"\n\n\n\n\n12.2.3 CRS\nThe CRS can be accessed with an ESPG code (espg) or a projection (proj4string). The ESPG is usually shorter, but less flexible than the analogous projection. To inspect the CRS of an sf object, use the st_crs() function.\n\nst_crs(world)\n\nThe CRS can either be geographic (i.e., latitude and longitude with degrees) or projected. Many of the functions used for sf objects assume that there is some CRS, and it may be necessary to set one. Operations involving distances depend heavily on the projection, and may not work with geographic CRSs.\n\nlondon &lt;- tibble(lon = -0.1, lat = 51.5) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"))\nst_is_longlat(london) # NA means that there is no set CRS\n\n[1] NA\n\n\nTo set the CRS, use the st_set_crs() function.\n\nlondon &lt;- st_set_crs(london, 4326)\nst_is_longlat(london)\n\n[1] TRUE\n\nst_crs(london)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nIf you are changing the CRS rather than setting one, use the st_transform() function. This is necessary when comparing two sf objects with different projections (a common occurrence).\n\nlondon_27700 &lt;- st_transform(london, 27700)\nst_distance(london, london_27700)\n\nError in st_distance(london, london_27700): st_crs(x) == st_crs(y) is not TRUE\n\n\nThe most common geographic CRS is WGS84, or EPSG code 4326. When in doubt, this may be a good place to start. Selecting a projected CRS requires more context of the specific data. Different sources will use different projections. See chapter 6.3 of Lovelace, Nowosad, and Muenchow (2021) and the EPSG repsitory for more information on this.\n\n\n12.2.4 Spatial Operations\nSpatial subsetting involves selecting features based on if they relate to other objects. It is analogous to attribute subsetting. As an example, we will use the New Zealand data.\n\n# Map of New Zealand and demographic data\ndata(\"nz\") \nnames(nz)\n\n[1] \"Name\"          \"Island\"        \"Land_area\"     \"Population\"   \n[5] \"Median_income\" \"Sex_ratio\"     \"geom\"         \n\nplot(nz[1])\n\n\n\n\n\n\n\n# 101 highest points in new Zealand\ndata(\"nz_height\")\nnames(nz_height)\n\n[1] \"t50_fid\"   \"elevation\" \"geometry\" \n\n\nWhereas in attribute subsetting, with the general format x[y, ], the y would be a logical value, integer, or character string. In spatial subsetting, the y is an sf object itself.\n\n canterbury &lt;- nz %&gt;%\n  filter(Name == \"Canterbury\")\n\ncanterbury_height &lt;- nz_height[canterbury, ]\n\nThere are different options for operators for subsetting. Intersects is the default, and is quite general. For example, if the object touches, crosses, or is within, the object will also intersect. Here are some examples of specifying the operator using the op argument.\n\nnz_height[canterbury, , op = st_intersects]\n\nSimple feature collection with 70 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1365809 ymin: 5158491 xmax: 1654899 ymax: 5350463\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\nFirst 10 features:\n   t50_fid elevation                geometry\n5  2362630      2749 POINT (1378170 5158491)\n6  2362814      2822 POINT (1389460 5168749)\n7  2362817      2778 POINT (1390166 5169466)\n8  2363991      3004 POINT (1372357 5172729)\n9  2363993      3114 POINT (1372062 5173236)\n10 2363994      2882 POINT (1372810 5173419)\n11 2363995      2796 POINT (1372579 5173989)\n13 2363997      3070 POINT (1373796 5174144)\n14 2363998      3061 POINT (1373955 5174231)\n15 2363999      3077 POINT (1373984 5175228)\n\nnz_height[canterbury, , op = st_disjoint]\n\nSimple feature collection with 31 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1204143 ymin: 5048309 xmax: 1822492 ymax: 5650492\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\nFirst 10 features:\n   t50_fid elevation                geometry\n1  2353944      2723 POINT (1204143 5049971)\n2  2354404      2820 POINT (1234725 5048309)\n3  2354405      2830 POINT (1235915 5048745)\n4  2369113      3033 POINT (1259702 5076570)\n12 2363996      2759 POINT (1373264 5175442)\n25 2364028      2756 POINT (1374183 5177165)\n26 2364029      2800 POINT (1374469 5176966)\n27 2364031      2788 POINT (1375422 5177253)\n46 2364166      2782 POINT (1383006 5181085)\n47 2364167      2905 POINT (1383486 5181270)\n\nnz_height[canterbury, , op = st_within]\n\nSimple feature collection with 70 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1365809 ymin: 5158491 xmax: 1654899 ymax: 5350463\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\nFirst 10 features:\n   t50_fid elevation                geometry\n5  2362630      2749 POINT (1378170 5158491)\n6  2362814      2822 POINT (1389460 5168749)\n7  2362817      2778 POINT (1390166 5169466)\n8  2363991      3004 POINT (1372357 5172729)\n9  2363993      3114 POINT (1372062 5173236)\n10 2363994      2882 POINT (1372810 5173419)\n11 2363995      2796 POINT (1372579 5173989)\n13 2363997      3070 POINT (1373796 5174144)\n14 2363998      3061 POINT (1373955 5174231)\n15 2363999      3077 POINT (1373984 5175228)\n\nnz_height[canterbury, , op = st_touches]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n[1] t50_fid   elevation geometry \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nGrouping for creating summary statistics or other calculations can be done using built-in functions (aggregate) or dplyr functions. In aggregate, the by argument is the grouping source and the x argument is the target output.\n\n# Built-in\nnz_avgheight &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\nnz_avgheight\n\nSimple feature collection with 16 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748537 xmax: 2089533 ymax: 6191874\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\nFirst 10 features:\n   t50_fid elevation                       geometry\n1       NA        NA MULTIPOLYGON (((1745493 600...\n2       NA        NA MULTIPOLYGON (((1803822 590...\n3  2408405  2734.333 MULTIPOLYGON (((1860345 585...\n4       NA        NA MULTIPOLYGON (((2049387 583...\n5       NA        NA MULTIPOLYGON (((2024489 567...\n6       NA        NA MULTIPOLYGON (((2024489 567...\n7       NA        NA MULTIPOLYGON (((1740438 571...\n8  2408394  2777.000 MULTIPOLYGON (((1866732 566...\n9       NA        NA MULTIPOLYGON (((1881590 548...\n10 2368390  2889.455 MULTIPOLYGON (((1557042 531...\n\nplot(nz_avgheight[\"elevation\"])\n\n\n\n\n\n\n\n\n\n# dplyr\nnz_avgheight &lt;- nz %&gt;%\n  st_join(nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarise(elevation = mean(elevation, na.rm = TRUE))\nplot(nz_avgheight[\"elevation\"])\n\n\n\n\n\n\n\n\nIt is possible to measure the geographic distance between spatial objects using st_distance(). Notice that it returns the units!\n\n# Get the highest point in New Zealand\nnz_highest &lt;- nz_height %&gt;%\n  slice_max(order_by = elevation)\n\n# Get the centroid of Canterbury\ncanterbury_centroid &lt;- nz %&gt;%\n  filter(Name == \"Canterbury\") %&gt;%\n  st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# Calculate the distance between these two points\nst_distance(nz_highest, canterbury_centroid)\n\nUnits: [m]\n       [,1]\n[1,] 115540\n\n\nThe function st_distance() can also be used to calculate distance matrices.\n\n# Get the 3 highest points in New Zealand\nnz_3highest &lt;- nz_height %&gt;%\n  arrange(desc(elevation)) %&gt;%\n  slice_head(n = 3)\n\n# Get the centroids of all states\nall_centroid &lt;- nz %&gt;%\n  st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# Calculate the distance matrix with centroids\nst_distance(nz_3highest, all_centroid)\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n[1,] 951233.0 856625.2 765070.3 825245.2 881491.0 723575.1 593453.8 621157.3\n[2,] 952019.6 857339.3 765680.7 825764.2 881937.4 724002.7 594057.0 621634.9\n[3,] 951563.1 856927.5 765332.0 825470.7 881687.6 723764.2 593712.6 621366.3\n         [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n[1,] 520471.9 108717.6 115540.0 191499.2 294665.7 315036.6 373697.0 348221.3\n[2,] 520789.4 109366.2 115390.2 190666.6 294032.1 315602.5 374195.6 348620.9\n[3,] 520616.6 108993.9 115493.6 191151.6 294394.6 315280.6 373914.3 348399.0\n\n# Without centroids\nst_distance(nz_3highest, nz)\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n[1,] 865750.4 797431.4 659107.4 730881.6 807936.8 630429.8 551289.0 522987.7\n[2,] 866509.0 798128.8 659627.6 731367.7 808407.5 630769.4 551922.6 523389.0\n[3,] 866069.8 797727.2 659333.3 731094.0 808143.1 630583.5 551559.8 523166.2\n         [,9]    [,10] [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n[1,] 447927.2 2184.049     0 54305.52 163297.5 232290.5 355518.1 263784.7\n[2,] 448273.4 2991.754     0 53735.24 163011.5 232811.0 356027.2 264131.4\n[3,] 448083.5 2507.071     0 54058.45 163164.7 232516.4 355739.6 263941.1\n\n\nOther geographic measurements include st_area() and st_length().\n\n# Area\nnz %&gt;%\n  group_by(Name) %&gt;%\n  st_area()\n\nUnits: [m^2]\n [1] 12890576439  4911565037 24588819863 12271015945  8364554416 14242517871\n [7]  7313990927 22239039580  8149895963 23409347790 45326559431 31903561583\n[13] 32154160601  9594917765   408075351 10464846864\n\n# Length\nseine %&gt;% \n  group_by(name) %&gt;%\n  st_length()\n\nUnits: [m]\n[1] 363610.5 635526.7 219244.6\n\n\n\n\n12.2.5 Geometry Operations\nThe functions in this section interact with the geom variable.\nThe function st_simplify() reduces the number of verticies in a spatial object. This results in a “smoothing” of the geography as well as an object that takes up less memory. Use ms_simplify() from the rmapshaper to avoid spacing issues.\n\nplot(nz[1])\n\n\n\n\n\n\n\n\n\nplot(st_simplify(nz[1], dTolerance = 10000)) #Smooth by 10 km\n\n\n\n\n\n\n\n\nWe have already seen how to compute the centroid.\n\nall_centroid &lt;- st_centroid(nz)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# Plot\nplot(nz[1], reset = FALSE)\nplot(st_geometry(all_centroid), add = TRUE, pch = 3, cex = 1.4)\n\n\n\n\n\n\n\n\nThe function st_point_on_surface() alters the point so that a point appears on the parent object. This may be more useful than centroids for labels.\n\nnz_ptsonsurface &lt;- st_point_on_surface(nz)\n\nWarning: st_point_on_surface assumes attributes are constant over geometries\n\nplot(nz[1], reset = FALSE)\nplot(st_geometry(nz_ptsonsurface), add = TRUE, pch = 3, cex = 1.4, col = \"red\")\nplot(st_geometry(all_centroid), add = TRUE, pch = 3, cex = 1.4) # Add centroids for comparison\n\n\n\n\n\n\n\n\nThe function st_buffer() allows you to compute buffers around geographies. It returns the same sf object, but the geometry column now contains a buffer around the original geometry of the specified distance (in meters).\n\nnz_height_buff_5km &lt;- st_buffer(nz_height, dist = 5000)\nplot(nz[1], reset = FALSE)\nplot(nz_height_buff_5km[1], add = TRUE)\n\n\n\n\n\n\n\n\n\nnz_height_buff_50km &lt;- st_buffer(nz_height, dist = 50000)\nplot(nz[1], reset = FALSE)\nplot(nz_height_buff_50km[1], add = TRUE)\n\n\n\n\n\n\n\n\nAffine transformations can be done with spatial data. They should be approached with caution as angles and length are not always preserved even though lines are.\n\nnz_g &lt;- st_geometry(nz)\n\n# Shift\nnz_shift &lt;- nz_g + c(0, 100000)\n\n# Scaling around the centroid\nnz_scale &lt;- (nz_g - st_centroid(nz_g)) * 0.5 + st_centroid(nz_g)\n\n# Set the new geography\nnz_changed &lt;- st_set_geometry(nz, nz_shift)\n\n\nplot(nz_g, reset = FALSE)\nplot(nz_shift, col = \"magenta\", add = TRUE)\n\n\n\n\n\n\n\n\n\nplot(nz_g, reset = FALSE)\nplot(nz_scale, col = \"magenta\", add = TRUE)\n\n\n\n\n\n\n\n\nSpatial subsetting with lines or polygons involving changes to the geometry columns is called spatial clipping. Forillustaration, consider two circles created as follows.\n\n# Create two points and buffers \nb &lt;- st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) \nb &lt;- st_buffer(b, dist = 1) \nx &lt;- b[1]\ny &lt;- b[2]\n\n# Plot\nplot(b)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"))\n\n\n\n\n\n\n\n\nWith these circles, we can see the possible spatial clippings.\n\n# Intersection\nx_int_y &lt;- st_intersection(x, y)\nplot(b)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"))\nplot(x_int_y, add = TRUE, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n# Difference\nx_diff_y &lt;- st_difference(x, y)\nplot(b)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"))\nplot(x_diff_y, add = TRUE, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\ny_diff_x &lt;- st_difference(y, x)\nplot(b)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"))\nplot(y_diff_x, add = TRUE, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n# Both differences\nx_sym_y &lt;- st_sym_difference(x, y)\nplot(b)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"))\nplot(x_sym_y, add = TRUE, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n# Union\nx_uni_y &lt;- st_union(x, y)\nplot(b)\ntext(x = c(-0.5, 1.5), y = 1, labels = c(\"x\", \"y\"))\nplot(x_uni_y, add = TRUE, col = \"steelblue\")\n\n\n\n\n\n\n\n\nThe function st_sample() randomly selects points within an area.\n\nb_sample &lt;- st_sample(b, size = 10)\nplot(b)\nplot(b_sample, add = TRUE)\n\n\n\n\n\n\n\n\n\n\n12.2.6 Merge\n\n12.2.6.1 Attribute\nSuppose you have two datasets with a common identifier variable (e.g., ID, name). It is possible to merge these datasets with this variable following chapter 3, even if there are spatial columns. Here is an example.\n\n# Load coffee_data\ndata(\"coffee_data\")\nnames(coffee_data)\n\n[1] \"name_long\"              \"coffee_production_2016\" \"coffee_production_2017\"\n\nclass(coffee_data)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# Join the world and coffee data\nworld_coffee &lt;- full_join(world, coffee_data, by = \"name_long\")\n\n# Plot coffee production 2017\nplot(world_coffee[\"coffee_production_2017\"])\n\n\n\n\n\n\n\n\n\n\n12.2.6.2 Spatial\nIf you have two (or more) data sets with spatial elements, it is natural to think about merging them based on spatial concepts. Instead of sharing an identifier variable, they may share geographic space. The function st_join() allows for many types of spatial overlays. For example, it is possible to join points to multipolygons.\n\nnames(nz)\n\n[1] \"Name\"          \"Island\"        \"Land_area\"     \"Population\"   \n[5] \"Median_income\" \"Sex_ratio\"     \"geom\"         \n\nnames(nz_height)\n\n[1] \"t50_fid\"   \"elevation\" \"geometry\" \n\nnz_joined &lt;- st_join(nz, nz_height)\n\nThe default is a left join. Inspecting the data reveals that regions with more than one point in nz_height appear more than once. The corresponding attribute data from nz_height are added to nz. The geography type is multipolygon, following the first argument’s type. If we switch the order, the geography type changes to point.\n\nnz_joined_rev &lt;- st_join(nz_height, nz)\n\nTo do an inner join, set left = FALSE. The default operator is st_intersects(). To change the operator, change the join argument. The sf cheat sheet is a great resource to think about spatial overlays and the possible operators.\nThere may be contexts in which two spatial datasets are related, but do not actually contain overlapping elements. Augmenting the st_join() function allows for a buffer distance to create near matches. Here is an example using the cycle_hire and cycle_hire_osm datasets from the spData package.\n\ndata(\"cycle_hire\")\ndata(\"cycle_hire_osm\")\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n\n[1] FALSE\n\n# Change the CRS to be able to use meters\ncycle_hire &lt;- cycle_hire %&gt;%\n  st_transform(crs = 27700)\n\ncycle_hire_osm &lt;- cycle_hire_osm %&gt;%\n  st_transform(crs = 27700)\n\n# Join within 20 meters\ncycle_joined &lt;- st_join(cycle_hire, cycle_hire_osm,\n                        join = st_is_within_distance,\n                        dist = 20)\n\n\n\n\n12.2.7 Practice Exercises\n\nThe below code randomly selects ten points distributed across Earth. Create an object named random_sf that merges these random points with the world dataset. In which countries did your random points land? (Review: How can you make the below code reproducible?)\n\n\n# Coordinate bounds of the world\nbb_world &lt;- st_bbox(world)\n\nrandom_df &lt;- tibble(\n  x = runif(n = 10, min = bb_world[1], max = bb_world[3]),\n  y = runif(n = 10, min = bb_world[2], max = bb_world[4])\n)\n\n# Set coordinates and CRS\nrandom_points &lt;- random_df %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\")) %&gt;% \n  st_set_crs(4326) \n\n\nWe saw that 70 of the 101 highest points in New Zealand are in Canterbury. How many points in nz_height() are within 100 km of Canterbury?\nFind the geographic centroid of New Zealand. How far is it from the geographic centroid of Canterbury?",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html#visualize",
    "href": "12_Geographic_Data_and_Mapping.html#visualize",
    "title": "12  Geographic Data and Mapping",
    "section": "12.3 Visualize",
    "text": "12.3 Visualize\nThe function plot() can be used with the sf object directly. It relies on the same arguments as for non-geographic data.\n\nplot(world)\n\n\n\n\n\n\n\n\n\nplot(world[3:6])\n\n\n\n\n\n\n\n\n\nplot(world[\"pop\"])\n\n\n\n\n\n\n\n\n\n# Example adding layers\nworld_asia &lt;- world[world$continent == \"Asia\", ] %&gt;% st_union()\nplot(world[2], reset = FALSE)\nplot(world_asia, add = TRUE, col = \"green\")\n\n\n\n\n\n\n\n\n\n# More complex example\nplot(world[\"continent\"], reset = FALSE)\nworld_centroids &lt;- st_centroid(world, of_largest_polygon = TRUE)\nplot(st_geometry(world_centroids), add = TRUE, cex = sqrt(world$pop) / 10000)\n\n\n\n\n\n\n\n\nThe package tmap has the same logic as ggplot2, but specialized for maps. There are options for interactive maps as well. The functions of ggplot2 can also be used with the geom geom_sf().\n\nggplot(nz) +\n  geom_sf()\n\n\n\n\n\n\n\n\n\nggplot(nz) +\n  geom_sf(aes(fill = Population)) \n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = nz) +\n  geom_sf(data = st_geometry(st_point_on_surface(nz)), aes(size = nz$Population))\n\nWarning: st_point_on_surface assumes attributes are constant over geometries",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html#raster-data",
    "href": "12_Geographic_Data_and_Mapping.html#raster-data",
    "title": "12  Geographic Data and Mapping",
    "section": "12.4 Raster Data",
    "text": "12.4 Raster Data\nRaster data are comprised of equally sized cells and corresponding data values. Because there can only be one value per cell, raster data can be ill-suited to represent human-invented borders. Because of the matrix representation of the geography (rather than coordinate points), raster data is well-suited to efficiently represent continuous spatial data. The package raster allows you to load, analyze, and map raster objects.\nPrinting the example raster displays the raster header and information. This example raster is from Zion National Park in Utah, U.S.. Just as for vector data, the plot() function can be used with raster data.To access and set the CRS for raster objects, use projection().\nAccessing and managing the attribute data of rasters is different than the usual approaches. For example, variables cannot be character strings. See chapter 3 of Lovelace, Nowosad, and Muenchow (2021) for information on this. See chapter 4 for spatial operations on raster data and chapter 5 for geometry operations on raster data.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html#resources",
    "href": "12_Geographic_Data_and_Mapping.html#resources",
    "title": "12  Geographic Data and Mapping",
    "section": "12.5 Resources",
    "text": "12.5 Resources\nThese incredible datasets originally comes from John Snow, but have been digitized and recreated by the GeoDa Data and Lab.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html#a-in-class-activity-spatial-analysis-with-john-snow",
    "href": "12_Geographic_Data_and_Mapping.html#a-in-class-activity-spatial-analysis-with-john-snow",
    "title": "12  Geographic Data and Mapping",
    "section": "12.a In-Class Activity: Spatial Analysis with John Snow",
    "text": "12.a In-Class Activity: Spatial Analysis with John Snow\nBelieve it or not, you can use your R tools to recreate Dr.Snow’s analysis! In this activity, you will also learn how to use spatial data. For that, you will need the sf package.\n\nlibrary(ggplot2)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nThe following object is masked from 'package:raster':\n\n    extract\n\nlibrary(sf)\n\n\n12.5.1 Geographic Data Vocabulary\nGeographic datasets have particular features that are useful to understand before using them for analysis and graphing. There are two models of geographic data: the vector data model and the raster data model. The vector data model uses points, lines, and polygons to represent geographic areas. The raster data model uses equally-sized cells to represent geographic areas. Vector data is usually adequate for maps in economics, as it is able to represent human-defined areas precisely. That being said, raster data is needed for some contexts (e.g., environmental studies), and can provide richness and context to maps.\n\n12.5.1.1 Vector Data Model\nVector data require a coordinate reference system (CRS). There are many different options for the CRS, with different countries and regions using their own systems. The differences between the different systems include the reference point (where is \\((0,0)\\) located) and the units of the distances (e.g., km, degrees).\nThe package sf contains functions to handle different types of vector data. The trio of packages sp, rgdal, and rgeos used to be the go-to packages for vector data in R, but have been superseded by sf. It has efficiency advantages and the data can be accessed more conveniently than in the other packages. You may still see examples and StackExchange forums with these other packages, however. If needed, the following code can be used to convert an sf object to a Spatial object used in the package sp, and back to an sf object.\n\nexample_sp &lt;- as(example, Class = \"Spatial\")\nexample_sf &lt;- st_as_sf(example_sp)\n\n\n\n\n12.5.2 Manage Geographic Data\nGeographic data in R are stored in a data frame (or tibble) with a column for geographic data. These are called spatial data frames (or sf object) with a column called geom or geometry.\n\n12.5.2.1 Load Spatial Data\nIt is possible to load geographic data built-in to R or other packages using data(). For example, the package osmdata connects you to the OpenStreetMap API, the package rnoaa imports NOAA climate data, and rWBclimate imports World Bank data. If you need any geographic data that might be collected by a governmental agency, check to see if there is an R package before downloading it. There may be some efficiency advantages to using the package.\nYou can also import geographic data into R directly from your computer. These will usually be stored in spatial databases, often with file extensions that one would also use in ArcGIS or a related program. The most popular format is ESRI Shapefile (.shp). The function to import data is st_read(). To see what types of files can be imported with that function, check st_drivers().\n\nsf_drivers &lt;- st_drivers()\nhead(sf_drivers)\n\nDownload the zip file Snow.zip on Canvas &gt; Modules &gt; Module 2 &gt; Data. Unzip the folder and navigate to the directory. Use the function st_read() to load the data. Notice that even though there are several file extensions in the folder snow1, we use the .shp file. This is called a shapefile.\n\ndeaths_house &lt;- st_read(\"Data/Snow/snow1/deaths_nd_by_house.shp\")\n\nReading layer `deaths_nd_by_house' from data source \n  `/Users/aziff/Desktop/1_PROJECTS_W/projects/data-science-for-economic-and-social-issues.github.io/Data/Snow/snow1/deaths_nd_by_house.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1852 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 528996.1 ymin: 180646.9 xmax: 529696.2 ymax: 181319.9\nProjected CRS: Transverse_Mercator\n\n\nLet us investigate the data. Notably, there are some geographic characteristics attached to the data: the geometry type (most commonly, POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON, GEOMETRYCOLLECTION), the dimension, the bbox (limits of the plot), the CRS, and the name of the geographic column. Notice that the last variable, geometry, is a list column. If you inspect the column, you will see that each element is a list of varying lengths, corresponding to the vector data of the country. It is possible to interact with this sf object as one would a non-geographic tibble or data frame.\n\nhead(deaths_house)\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529286.4 ymin: 181061.3 xmax: 529297.4 ymax: 181084.1\nProjected CRS: Transverse_Mercator\n  ID deaths_r deaths_nr deaths pestfield dis_pestf dis_sewers dis_bspump\n1  1        0         0      0         1     10.08      10.08     125.00\n2  2        1         0      1         1     14.64      14.64     119.94\n3  3        0         0      0         1     18.47      18.47     116.27\n4  4        0         0      0         1     22.98      22.98     112.56\n5  5        0         0      0         1     27.47      27.47     109.10\n6  6        5         0      5         1     32.21      32.21     105.54\n  death_dum  COORD_X  COORD_Y                  geometry\n1         0 529286.4 181084.1 POINT (529286.4 181084.1)\n2         1 529289.8 181079.7 POINT (529289.8 181079.7)\n3         0 529291.7 181075.3 POINT (529291.7 181075.3)\n4         0 529293.5 181070.3 POINT (529293.5 181070.3)\n5         0 529295.4 181065.8 POINT (529295.4 181065.8)\n6         1 529297.4 181061.3 POINT (529297.4 181061.3)\n\n\nAccording to the documentation, deaths is the total deaths per house. Let us explore this variable like we would for any other dataset.\n\nsummary(deaths_house)\n\n       ID            deaths_r         deaths_nr           deaths       \n Min.   :   1.0   Min.   : 0.0000   Min.   : 0.0000   Min.   : 0.0000  \n 1st Qu.: 463.8   1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.: 0.0000  \n Median : 926.5   Median : 0.0000   Median : 0.0000   Median : 0.0000  \n Mean   : 926.5   Mean   : 0.3569   Mean   : 0.0243   Mean   : 0.3812  \n 3rd Qu.:1389.2   3rd Qu.: 0.0000   3rd Qu.: 0.0000   3rd Qu.: 0.0000  \n Max.   :1852.0   Max.   :12.0000   Max.   :18.0000   Max.   :18.0000  \n   pestfield         dis_pestf        dis_sewers      dis_bspump    \n Min.   :0.00000   Min.   :  7.56   Min.   : 2.85   Min.   :  7.26  \n 1st Qu.:0.00000   1st Qu.:106.93   1st Qu.:11.13   1st Qu.:149.21  \n Median :0.00000   Median :177.17   Median :17.04   Median :203.73  \n Mean   :0.04914   Mean   :173.73   Mean   :19.39   Mean   :206.21  \n 3rd Qu.:0.00000   3rd Qu.:237.94   3rd Qu.:24.69   3rd Qu.:260.60  \n Max.   :1.00000   Max.   :441.34   Max.   :92.40   Max.   :442.99  \n   death_dum         COORD_X          COORD_Y                geometry   \n Min.   :0.0000   Min.   :528996   Min.   :180647   POINT        :1852  \n 1st Qu.:0.0000   1st Qu.:529247   1st Qu.:180892   epsg:NA      :   0  \n Median :0.0000   Median :529386   Median :181005   +proj=tmer...:   0  \n Mean   :0.1992   Mean   :529371   Mean   :181004                       \n 3rd Qu.:0.0000   3rd Qu.:529500   3rd Qu.:181132                       \n Max.   :1.0000   Max.   :529696   Max.   :181320                       \n\n\nCombining ggplot2 and sf, we can very easily graph the geographic data.\n\nggplot(data = deaths_house) +\n  geom_sf()\n\n\n\n\n\n\n\n\nNow let us use the aes() option to understand our variables of interest.\n\nggplot(data = deaths_house) +\n  geom_sf(aes(color = as.factor(deaths)))\n\n\n\n\n\n\n\n\nWe can improve this visual by creating a binned variable.\n\ndeaths_house$deaths_bins &lt;- cut(deaths_house$deaths, \n                                breaks=c(0, 1, 3, 6, 9, 18),\n                                include.lowest = TRUE) \n\nggplot(data = deaths_house) +\n  geom_sf(aes(color = deaths_bins), size = 0.8) +\n  scale_color_manual(values = c(\"grey\", \"green\", \"yellow\", \"orange\", \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe see the houses and the number of deaths by house. But what about the pumps? Let us load that dataset too.\n\npumps &lt;- st_read(\"Data/Snow/snow6/pumps.shp\")\n\nReading layer `pumps' from data source \n  `/Users/aziff/Desktop/1_PROJECTS_W/projects/data-science-for-economic-and-social-issues.github.io/Data/Snow/snow6/pumps.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529183.7 ymin: 180670 xmax: 529752.2 ymax: 181193.7\nProjected CRS: Transverse_Mercator\n\n\nWe can do the same data exploration that we always do.\n\nsummary(pumps)\n\n       ID           name              COORD_X          COORD_Y      \n Min.   :1.00   Length:8           Min.   :529184   Min.   :180795  \n 1st Qu.:2.75   Class :character   1st Qu.:529270   1st Qu.:180865  \n Median :4.50   Mode  :character   Median :529400   Median :180890  \n Mean   :4.50                      Mean   :529385   Mean   :180947  \n 3rd Qu.:6.25                      3rd Qu.:529475   3rd Qu.:181039  \n Max.   :8.00                      Max.   :529613   Max.   :181194  \n          geometry\n POINT        :8  \n epsg:NA      :0  \n +proj=tmer...:0  \n                  \n                  \n                  \n\n\nWe can map the data.\n\nggplot(data = pumps) + \n  geom_sf()\n\n\n\n\n\n\n\n\nHere is where we add the spatial analysis that John Snow accomplished. We can super-impose these graphs on top of each other.\n\nggplot() +\n  geom_sf(data = deaths_house, aes(color = deaths_bins), size = 0.8) +\n  geom_sf(data = pumps) + \n  geom_sf_text(data = pumps, aes(label = name), nudge_y = -20, size = 3) +\n  scale_color_manual(values = c(\"grey\", \"green\", \"yellow\", \"orange\", \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThat was John Snow’s seminal map. Now, let us look at the other data he collected from the rest of London to further corroborate his theory.\n\nsds &lt;- st_read(\"Data/Snow/snow8/subdistricts.shp\")\n\nReading layer `subdistricts' from data source \n  `/Users/aziff/Desktop/1_PROJECTS_W/projects/data-science-for-economic-and-social-issues.github.io/Data/Snow/snow8/subdistricts.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 32 features and 32 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 521033.3 ymin: 169712.5 xmax: 536916.2 ymax: 180562.5\nProjected CRS: Transverse_Mercator\n\nsummary(sds) \n\n     dis_ID         district             sub_ID        subdist         \n Min.   : 1.000   Length:32          Min.   : 1.00   Length:32         \n 1st Qu.: 3.000   Class :character   1st Qu.: 8.75   Class :character  \n Median : 4.000   Mode  :character   Median :16.50   Mode  :character  \n Mean   : 4.844                      Mean   :16.50                     \n 3rd Qu.: 7.250                      3rd Qu.:24.25                     \n Max.   :10.000                      Max.   :32.00                     \n                                                                       \n    pop1851        pop_house       supplier           supplierID  \n Min.   : 1632   Min.   :5.600   Length:32          Min.   :1.00  \n 1st Qu.:11160   1st Qu.:6.100   Class :character   1st Qu.:1.00  \n Median :15942   Median :6.450   Mode  :character   Median :2.00  \n Mean   :15217   Mean   :6.753                      Mean   :1.75  \n 3rd Qu.:18519   3rd Qu.:7.500                      3rd Qu.:2.00  \n Max.   :29861   Max.   :9.100                      Max.   :3.00  \n                                                                  \n    perc_sou         perc_lam        perc_other      lam_degree       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Length:32         \n 1st Qu.:0.1750   1st Qu.:0.0000   1st Qu.:0.0000   Class :character  \n Median :0.4450   Median :0.2258   Median :0.1732   Mode  :character  \n Mean   :0.4558   Mean   :0.2810   Mean   :0.2632                     \n 3rd Qu.:0.6809   3rd Qu.:0.5119   3rd Qu.:0.3605                     \n Max.   :1.0000   Max.   :0.8302   Max.   :0.9822                     \n NA's   :1        NA's   :1        NA's   :1                          \n   d_overall          d_sou            d_lam            d_pump      \n Min.   :  0.00   Min.   :  0.00   Min.   : 0.000   Min.   :0.0000  \n 1st Qu.: 16.25   1st Qu.:  8.25   1st Qu.: 0.000   1st Qu.:0.0000  \n Median : 46.50   Median : 34.00   Median : 1.000   Median :0.0000  \n Mean   : 47.31   Mean   : 39.47   Mean   : 3.062   Mean   :0.9062  \n 3rd Qu.: 71.00   3rd Qu.: 61.50   3rd Qu.: 5.250   3rd Qu.:1.0000  \n Max.   :125.00   Max.   :115.00   Max.   :13.000   Max.   :5.0000  \n                                                                    \n    d_thames         d_unasc         rate_sou7w      rate_lam7w    \n Min.   : 0.000   Min.   :0.0000   Min.   : 0.00   Min.   : 0.000  \n 1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.:35.46   1st Qu.: 2.380  \n Median : 0.000   Median :0.0000   Median :48.87   Median : 5.889  \n Mean   : 3.188   Mean   :0.6875   Mean   :45.39   Mean   : 6.419  \n 3rd Qu.: 3.250   3rd Qu.:1.0000   3rd Qu.:54.45   3rd Qu.: 9.817  \n Max.   :35.000   Max.   :5.0000   Max.   :87.23   Max.   :16.822  \n                                   NA's   :4       NA's   :13      \n   rate_oth7w       deaths1849      deaths1854       rate1849      \n Min.   : 0.000   Min.   :  1.0   Min.   :  0.0   Min.   :  5.335  \n 1st Qu.: 0.000   1st Qu.:113.2   1st Qu.: 75.5   1st Qu.: 95.402  \n Median : 3.853   Median :192.5   Median :161.0   Median :139.784  \n Mean   :10.491   Mean   :197.8   Mean   :162.3   Mean   :126.827  \n 3rd Qu.:10.633   3rd Qu.:257.5   3rd Qu.:238.5   3rd Qu.:159.916  \n Max.   :82.938   Max.   :544.0   Max.   :388.0   Max.   :209.513  \n NA's   :10                       NA's   :1       NA's   :1        \n    rate1854         pop1849         pop1854        rAvSupR_49   \n Min.   :  0.00   Min.   : 1632   Min.   : 1632   Min.   :130.1  \n 1st Qu.: 53.94   1st Qu.:12021   1st Qu.:13336   1st Qu.:130.1  \n Median : 78.44   Median :15635   Median :17138   Median :130.1  \n Mean   : 92.31   Mean   :14936   Mean   :16388   Mean   :132.1  \n 3rd Qu.:134.87   3rd Qu.:17792   3rd Qu.:19152   3rd Qu.:134.9  \n Max.   :200.87   Max.   :28392   Max.   :30080   Max.   :134.9  \n NA's   :1        NA's   :1       NA's   :1       NA's   :4      \n   rAvSupR_54       pred_Snow       pred_DiD49       pred_DiD54    \n Min.   : 84.86   Min.   : 27.0   Min.   : 42.72   Min.   : 43.23  \n 1st Qu.: 84.86   1st Qu.: 64.0   1st Qu.:113.35   1st Qu.: 57.45  \n Median : 84.86   Median :105.0   Median :140.74   Median : 90.43  \n Mean   :111.32   Mean   :108.2   Mean   :134.87   Mean   :101.29  \n 3rd Qu.:146.61   3rd Qu.:153.5   3rd Qu.:158.37   3rd Qu.:127.09  \n Max.   :146.61   Max.   :160.0   Max.   :199.39   Max.   :201.78  \n NA's   :4        NA's   :1       NA's   :4        NA's   :4       \n          geometry \n MULTIPOLYGON :32  \n epsg:NA      : 0  \n +proj=tmer...: 0  \n                   \n                   \n                   \n                   \n\n\nWe can make the map without thinking about any of the variables. Notice that now the type of geometry is MULTIPOLYGON rather than POINT.\n\nggplot(data = sds) +\n  geom_sf() +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can color in these areas by the proportion that was served by Southwark & Vauxhall.\n\nggplot(data = sds) +\n  geom_sf(aes(fill = perc_sou)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can also look at the death rate in 1849 (before Lambeth changed the location of its pipes).\n\nggplot(data = sds) +\n  geom_sf(aes(fill = rate1849)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nAnd in 1854, the rate is concentrated in a more specific way.\n\nggplot(data = sds) +\n  geom_sf(aes(fill = rate1854)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow, go to your quiz to think about how we can use modern econometric tools (e.g, IV, difference-in-differences) to analyze these data.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "12_Geographic_Data_and_Mapping.html#further-reading",
    "href": "12_Geographic_Data_and_Mapping.html#further-reading",
    "title": "12  Geographic Data and Mapping",
    "section": "12.6 Further Reading",
    "text": "12.6 Further Reading\nThe above information comes from Lovelace, Nowosad, and Muenchow (2021) and Wickham (2016).\n\n12.6.1 References\n\n\n\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2021. Geocomputation with R. https://bookdown.org/robinlovelace/geocompr/.\n\n\nWickham, Hadley. 2016. Ggplot2. Use R! Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-24277-4.",
    "crumbs": [
      "Cleaning and Describing Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geographic Data and Mapping</span>"
    ]
  },
  {
    "objectID": "13_CLT.html",
    "href": "13_CLT.html",
    "title": "13  Central Limit Theorem",
    "section": "",
    "text": "13.1 Formal Definition of the Central Limit Theorem\nUnlike previous chapters, this chapter takes you through an activity to practice R syntax and functions yourself. The activity is structured around reviewing the Central Limit Theorem. Feel free to work with a classmate or discuss issues that come up. Create an .R script for the activity. Do not worry about formatting the output into a .Rmd file as you will not be turning in this work. It is for your own practice!\nThere are actually several theorems that relate to the central tendency of sampling distributions. We will focus on the Lindberg-Levy Central Limit Theorem, which is what Wheelan discussed in Naked Statistics.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "13_CLT.html#formal-definition-of-the-central-limit-theorem",
    "href": "13_CLT.html#formal-definition-of-the-central-limit-theorem",
    "title": "13  Central Limit Theorem",
    "section": "",
    "text": "13.1.1 Statement of the Central Limit Theorem\nSuppose there is an independent and identically distributed (i.i.d.) sequence of random variables, \\(X_1, X_2, X_3, \\ldots\\). For all \\(i\\) indexing the random variables\n\\[\\begin{align*}\n\\mathbb{E}[X_i] &= \\mu \\\\\n\\text{Var}[X_i] &= \\sigma^2 &lt; \\infty.\n\\end{align*}\\]\nThen, as \\(n\\) approaches \\(\\infty\\), the random variables \\(\\sqrt{n}(\\bar{X}_n - \\mu)\\) converges to a normal distribution with a mean of 0 and a variance of \\(\\sigma^2\\), \\(\\mathcal{N}(0 \\sigma^2)\\).\n\n\n13.1.2 Explanation\n\n13.1.2.1 “Random Variable”\nRecall that a random variable takes values according to an underlying distribution. For example, if \\(Y\\) is the outcome of a coin flip, the variable can take the values ${\\(0, 1\\)}$ corresponding to heads and tails. The probability of each outcome is 0.5.\n\n\n13.1.2.2 “Independent and Identically Distributed (i.i.d.)”\nThis is a common assumption in statistics when dealing with sequences of random variables.\n\nIf the sequence is identically distributed then all the variables have the same distribution. That implies that they have the same mean and variance (\\(\\mu\\) and \\(\\sigma^2\\) in the formal statement above).\nIf the sequence is independent then none of the random variables are related to each other. Suppose we know that we know that the first coin flip is heads so that \\(Y_1 = 0\\). Does this change the probability that \\(Y_2\\) will be heads? No! The underlying probability distribution remains the same regardless of the realized values of the variables.\n\n\n\n13.1.2.3 “As \\(n\\) approaches \\(\\infty\\)”\nThe term \\(n\\) refers to the sample size. For example, if we flip a coin 10 times, then \\(n = 10\\). In theory, we want to imagine \\(n\\) getting bigger and bigger.\n\n\n13.1.2.4 “The Random Variables \\(\\sqrt{n}(\\bar{X}_n - \\mu)\\)”\nWe can break this down further\n\n\\(\\sqrt{n}\\): the square root of the sample size\n\\(\\bar{X}_n\\): the sample mean for the sample of size \\(n\\). We calculate this by adding up all the values from the sample and dividing by \\(n\\):\n\n\\[\\begin{equation*}\n\\bar{X}_n = \\frac{1}{n} \\sum_{j = 1}^n X_j.\n\\end{equation*}\\]\n\n\\(\\mu\\): this is the mean, or expected value, of the underlying distribution. Suppose that the probability density function representing that underlying distribution is \\(f(X)\\). Then, the expected value is formally\n\n\\[\\begin{equation*}\n\\mathbb{E}[X] = \\int_{-\\infty}^\\infty x f(x) dx.\n\\end{equation*}\\]\nIf the random variable is discrete, then we replace the integral with a sum.\n\n\n13.1.2.5 “A Normal Distribution with a Mean of 0 and a Variance of \\(\\sigma^2\\)”\nWe already went over the mean. Under the same notation, the variance is\n\\[\\begin{equation*}\n\\text{Var}[X] = \\int_{-\\infty}^\\infty x^2 f(x) dx.\n\\end{equation*}\\]\nThe Normal distribution is a particular continuous probability distribution. It has convenient statistical properties, including its role in the Central Limit Theorem! Because of this, it is commonly used across statistics, economics, and other fields. Here are the key facts to remember about the Normal Distribution.\n\nThe probability density function (PDF) is\n\n\\[\\begin{equation*}\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\left\\{\\frac{-(x - \\mu)^2}{2 \\sigma^2}\\right\\}}.\n\\end{equation*}\\]\n\nThe normal distribution is defined across all real numbers. That means that as the graph extends to \\(\\infty\\) and \\(-\\infty\\), the PDF is never 0.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "13_CLT.html#activity-1-simulated-data",
    "href": "13_CLT.html#activity-1-simulated-data",
    "title": "13  Central Limit Theorem",
    "section": "13.2 Activity 1: Simulated Data",
    "text": "13.2 Activity 1: Simulated Data\n\nIn this activity, you will be using functions in R that rely on randomness. For example, you will need to draw a vector of random numbers. It is often useful to draw the same vector each time you run your code. At the top of your script, write the following code.\n\n\nset.seed(470500)\n\nComputers generate random numbers using an algorithm that starts at a certain point, or seed. By setting the seed, you are defining where the computer should start the algorithm. Different seeds (try a number other than 470500) will produce different results.\n\nDefine a vector called unif_pop that contains 1,000,000 draws from the uniform distribution on the interval \\([0,1]\\). This will the population. Below, you will take samples from this population.\nPlot a histogram to visualize unif_pop.\nRecall that the mean and variance of the uniform distribution on the interval \\((a, b)\\) is \\(\\frac{1}{2} (a + b)\\) and the variance is \\(\\frac{1}{12}(b-a)^2\\). What is the mean and variance of unif_pop?\nNow you want to take samples from the population. Look at the manual for the function sample(). Try drawing a random sample of size n=10 from unif_pop. Should the argument replace be TRUE or FALSE?\nTake the mean of the sample you drew in question 5. This is the sample mean. In the mathematical notation above, it is \\(\\bar{X}_{10}\\) because \\(n = 10\\).\nYou actually want to take many draws from the population. Write a function that takes as arguments the sample size (10 in the example from question 5) and the population vector (unif_pop in the example from question 5). The function should return the sample mean. Test your function for n=10.\nYou will want to call your function from question 7 many times. Output 1,000 instances of the sample means when you sample 10 elements from the population.\n\n\nWrite a loop to call your function 1,000 times.\n[optional, hard!] Use an apply-like function (e.g., sapply() or lapply()) to call your function 1,000 times. You will need to use the function(x) approach from notes 03.\n\n\nYou were able to call your function, but how are you saving the output? Save the output to a vector. You may need to define an empty vector of length 0 before your loop. You can use vector(length = 0)to do so.\nWhat is the mean and standard deviation of the vector from question 9?\nPlot the histogram of the vector from question 9.\n\n\nPlot the histogram of the sample means.\nCreate a vector of \\(\\sqrt{n}(\\bar{X}_n - \\mu)\\) and plot that histogram. Add the normal distribution over your plot. I put the code below for base and ggplot2 graphing functions.\n\n\n# Base\ncurve(dnorm(x, mean = 0, sd = SD_FROM_4, add = TRUE)\n\n# ggplot2 (remember the plus sign)\nstat_function(fun = dnorm, args = list(mean = 0, sd = SD_FROM_4))\n\n\nNow you want to experiment with different sample sizes. Write a function that takes as arguments:\n\n\nn: The size of the sample. Above this was 10.\npopulation: The population vector. Above this was unif_pop.\nnum_samples: The number of samples to draw. Above this was 1,000. Set the default to be 1,000.\n\nThe function should combine the steps in 8 (select either loop or apply-like function) and 9.\nThe function should sample n observations from the population, calculate the sample mean, and save that sample mean to an element in a vector of size num_samples. The output of the function should be the vector of the transformed sample means \\(\\sqrt{n}(\\bar{X}_n - \\mu)\\).\n\nRun the function from 12 for n = 2, n = 10, n = 50, n = 100, and n = 1000. You should get 5 vectors from this. Save them to a data frame or a tibble. There should be 1,000 rows corresponding to the number of samples (num_samples). There should be 5 columns corresponding to each of the sample sizes.\nPlot the densities of each of the columns in 13. If you use ggplot2 you may find it easier to reshape the data to be in long format. The function is pivot_longer().",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "13_CLT.html#activity-2-actual-data",
    "href": "13_CLT.html#activity-2-actual-data",
    "title": "13  Central Limit Theorem",
    "section": "13.3 Activity 2: Actual Data",
    "text": "13.3 Activity 2: Actual Data\n\nNow you will visualize the Central Limit Theorem using actual data. Go to Canvas &gt; Modules &gt; Data and download tx_lottery.csv. Save the data to your computer and open the data in R. Take a moment to get familiar with the data. Each row corresponds to one winning event for the Texas state lottery.\nLimit the data to only the year 2024.\nGet key summary statistics for the variable AmountWon: mean, median, variance, minimum, and maximum.\nPlot a histogram or density of variable AmountWon. This is your population distribution.\nTest that the function you wrote for question 12 works for n=10 with the population AmountWon.\nRepeat questions 13 and 14 for AmountWon.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "13_CLT.html#references",
    "href": "13_CLT.html#references",
    "title": "13  Central Limit Theorem",
    "section": "13.4 References",
    "text": "13.4 References\nThe data on lottery winners come from the Texas Data Portal.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "14_Inference.html",
    "href": "14_Inference.html",
    "title": "14  Inference",
    "section": "",
    "text": "14.1 Classic Inference\nThe word “inference” in general refers to drawing conclusions from logical reasoning. In statistics, it refers to drawing conclusions in the presence of uncertainty. Why is there uncertainty in the first place? The main reason is sampling uncertainty. Recall the following definitions.\nSampling uncertainty arises whenever there is a sample because there is no guarantee that a sample is representative of the population. Even if all of the observed characteristics are identical, there could be unobserved characteristics that diverge between the sample and the population. Despite this, we rely on data from samples because in most cases, it is very difficult or impossible to collect data on the entire population of interest. As long as we use statistics to quantify the uncertainty, samples do the trick.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "14_Inference.html#classic-inference",
    "href": "14_Inference.html#classic-inference",
    "title": "14  Inference",
    "section": "",
    "text": "Population. The universe of units that are of interest. There is a parameter of interest in the population, such as the true probability that a flipped coin lands on heads.\nSample. A subset of the population. We use the sample to estimate the parameter of interest using a statistic, such as the proportion of times that a flipped coin landed on heads after many trials.\n\n\n\n14.1.1 Example: Fair/Unfair Coin\nSuppose you procure a coin that you believe is unfair. You flip it 10,000 times and get heads 52% of the time. We will use this sample to make some inference about the parameter of interest.\n\nEstablish the research question. Is the coin fair? That is, you get heads on average 50% of the time?\nDefine the parameter of interest. The true probability, \\(\\pi\\), that the suspicious coin is heads.\nCalculate the statistic. We observe heads 52% of the time in the sample. That means that the statistic is the estimated probability that the coin is heads, \\(\\hat{\\pi} = 0.52\\).\nSet up the null and alternative hypotheses. A hypothesis should be in reference to the population parameter. You should put the statement you are challenging in the null hypothesis. The aim is not to “accept” the alternative hypothesis. Rather, we see if we can reject the null hypothesis at some level of confidence.\n\n\\[\\begin{align*}\n  H_0: \\pi &= 0.5 \\\\\n  H_1: \\pi &\\neq 0.5\n  \\end{align*}\\]\n\nCompute the test statistic for the hypothesis test. This involves calculating a \\(t\\)- or \\(Z\\)-statistic, which can then be used to calculate a \\(p\\)-value or a confidence interval. Technically, the \\(Z\\)-statistic is appropriate when the population variance is known and the \\(t\\)-statistic is appropriate when the population variance is unknown. Usually, they yield the same results because the Student’s \\(t\\) distribution converges to the Normal distribution (required for the \\(Z\\)-test). I write both below for clarity. You can see that they produce very similar results. Only for small sample sizes will the choice of test make a difference.\n\nRecall that for a Bernoulli random variable, which describes a coin flip, the mean is \\(\\pi\\) and the standard deviation is \\(\\pi(1-\\pi)\\). Recall that the standard error is the standard deviation divided by the square root of the sample size, \\(n\\).\n\\(Z\\)-statistic (Normal distribuion):\n\\[\\begin{equation*}\nZ =  \\frac{\\hat{\\pi} - \\pi}{\\sqrt{\\frac{\\pi(1-\\pi)}{n}}} = \\frac{0.52 - 0.5}{\\sqrt{\\frac{0.25}{10000}}}\n\\end{equation*}\\]\n\n(0.52 - 0.5) / sqrt(0.25 / 10000)\n\n[1] 4\n\n\n\\(t\\)-statistic (Student’s \\(t\\) distribution with \\(n-1\\) degrees of freedom):\n\\[\\begin{equation*}\nt =  \\frac{\\hat{\\pi} - \\pi}{\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}} = \\frac{0.52 - 0.5}{\\sqrt{\\frac{0.2496}{10000}}}\n\\end{equation*}\\]\n\n(0.52 - 0.5) / sqrt(0.2496 / 10000)\n\n[1] 4.003204\n\n\n\nCompute \\(p\\)-value or confidence to report the conclusion from the hypothesis test. For a two-sided test (remember that the distributions for both tests are symmetric), consider \\(Z\\) to be a random variable distributed according to a standard Normal distribution. The \\(p\\)-value is\n\n\\[\\begin{align*}\n\\Pr\\left(Z &gt;  \\frac{\\hat{\\pi} - \\pi}{\\sqrt{\\frac{\\pi(1-\\pi)}{n}}} \\quad \\bigcup \\quad Z &lt;  \\frac{\\hat{\\pi} - \\pi}{\\sqrt{\\frac{\\pi(1-\\pi)}{n}}}\\right) = 2 \\Pr\\left(Z &gt;  \\frac{\\vert\\hat{\\pi} - \\pi\\vert}{\\sqrt{\\frac{\\pi(1-\\pi)}{n}}}\\right).\n\\end{align*}\\]\nYou may recall using statistical tables in the back of your textbook or online to compute \\(p\\)-values. You can use R for it too! The below functions compute the lower tail probability. So for some input quantile \\(x\\), pnorm(x) gives \\(\\Pr(Z \\leq x)\\). You can use the symmetry to get the \\(p\\)-value or change the argument lower.tail=FALSE. I show both ways below.\n\n# Normal distribution\n2 * pnorm(4, lower.tail = FALSE)\n\n[1] 6.334248e-05\n\n2 * (1 - pnorm(4))\n\n[1] 6.334248e-05\n\n# Student's t distribution with n-1 degrees of freedom\n2 * pt(4.003, df = 99, lower.tail = FALSE)\n\n[1] 0.0001209255\n\n2 * (1 - pt(4.003, df = 99))\n\n[1] 0.0001209255\n\n\nIt is common to use certain cut-offs for \\(p\\)-values, such as 0.1, 0.05, 0.01, or 0.001. Given how small the \\(p\\)-value is here, it is common to see them reported as \\(p &lt; 0.001\\). We can reject the null hypothesis at the 99.9% level.\nAnalogously, we can construct a confidence interval around the estimated statistic. For a two-sided \\(Z\\)-test:\n\\[\\begin{align*}\nCI_{\\alpha} = \\left[\\hat{\\pi} - z^*_{\\alpha/2} \\sqrt{\\frac{\\pi(1-\\pi)}{n}}, \\hat{\\pi} + z^*_{\\alpha/2} \\sqrt{\\frac{\\pi(1-\\pi)}{n}} \\right]\n\\end{align*}\\]\n\n\\(\\alpha\\) is the threshold to reject the null hypothesis. For example, if \\(\\alpha = 0.05\\), then the \\(p\\)-value has to be less than 0.05 to reject the null hypothesis.\n\\(z^*_{\\alpha/2}\\) the “critical value” in the standard normal distribution such that \\(\\Pr(Z &gt; z^*) = \\alpha/2\\). In the normal distribution for \\(\\alpha = 0.05\\), \\(z^* = 1.96\\).\n\nThe confidence interval is analogous for the \\(t\\)-test, but use the critical value from the Student’s \\(t\\)-distribution. The confidence interval for a one-sided test will sue the critical value \\(z^*_{\\alpha}\\). The lower bound will be \\(-\\infty\\) if the alternative hypothesis is \\(\\leq\\) or the upper bound will be \\(\\infty\\) if the alternative hypothesis is \\(\\geq\\).\nLet us use R to construct the 95% confidence interval. Notice we use qnorm() and `qt() to get the quantile of the distribution given a probability. For both cases, it is approximately \\([0.495, 0.545]\\).\n\n# Normal distribution\n0.52 - qnorm(0.05/2, lower.tail = FALSE) * sqrt(0.25 / 10000)\n\n[1] 0.5102002\n\n0.52 + qnorm(0.05/2, lower.tail = FALSE) * sqrt(0.25 / 10000)\n\n[1] 0.5297998\n\n# Student's t distribution\n0.52 - qt(0.05/2, df = 99, lower.tail = FALSE) * sqrt(0.2496 / 10000)\n\n[1] 0.5100869\n\n0.52 + qt(0.05/2, df = 99, lower.tail = FALSE) * sqrt(0.2496 / 10000)\n\n[1] 0.5299131\n\n\nIf the confidence interval overlaps the true parameter, then we cannot reject the null hypothesis. Because 0.5 is not in the confidence interval, we can reject the null hypothesis with 95% confidence.\n\nConclude. Because we reject the null hypothesis, we reject that the coin is fair at the 99.9% confidence level using the \\(p\\)-value.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "14_Inference.html#bootstrapping",
    "href": "14_Inference.html#bootstrapping",
    "title": "14  Inference",
    "section": "14.2 Bootstrapping",
    "text": "14.2 Bootstrapping\nIn many cases, the \\(Z\\)-test or \\(t\\)-test are adequate for hypothesis testing. However, they rely on assumptions about the distribution of the estimator. If we have the sample mean, like we do for the coin example above, that is completely fine due to the Central Limit Theorem. What if we are interested in another statistic, like the median? Bootstrapping provides a non-parametric approach for these situations.\n\n14.2.1 Example: Bootsrapping the Fair/Unfair Coin\nSuppose the same set-up as above. Let us do the same test with bootstrapping. However, we need the data in order to do the bootstrapping, so let me simulate it here. In reality, we do not know \\(\\pi\\), the true probability of getting heads. To simulate data, we need to specify this probability. Suppose it is 0.53. We get a sample mean of 0.523.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nset.seed(470500) # Remember to set the seed so that the results are reproducible\n\n# Define the data\n\npi_true &lt;- 0.53\n\n# 0 is tails, 1 is heads\nflips &lt;- rbinom(10000, size = 1, prob = pi_true)\n\n# pi hat is the sample average\npi_hat &lt;- mean(flips)\npi_hat\n\n[1] 0.523\n\n# I will make flips a tibble to be clear about the bootstrap steps below\nflips_tib &lt;- tibble(flip_id = 1:10000,\n                    outcome = flips)\n\n\n(Same as above) Establish the research question. Is the coin fair? That is, you get heads on average 50% of the time?\n(Same as above) Define the parameter of interest. The true probability, \\(\\pi\\), that the suspicious coin is heads.\n(Same as above) Calculate the statistic. We observe heads 52% of the time in the sample. That means that the statistic is the estimated probability that the coin is heads, \\(\\hat{\\pi} = 0.523\\).\n(Same as above) Set up the null and alternative hypotheses. A hypothesis should be in reference to the population parameter. You should put the statement you are challenging in the null hypothesis. The aim is not to “accept” the alternative hypothesis. Rather, we see if we can reject the null hypothesis at some level of confidence.\n\n\\[\\begin{align*}\n  H_0: \\pi &= 0.5 \\\\\n  H_1: \\pi &\\neq 0.5\n  \\end{align*}\\]\n\nBootstrap. The idea behind bootsrap is that we take the distribution in the data from the sample as given and do not impose any assumptions. This means that if we resample the sample we are essentially taking a new draw from the underlying distribution. Each bootstrap is one resample. We resample with replacement. That means we can get repeated observations. For example, the bootstrap could contain four instances of the first coin flip. First of all, if we take the sample distribution seriously, then each instance is random. That means that it is also random to get more than one of the isntance. Second, if we were resample without replacement, then we would get the exact same sample each time. Because we are interested in the average of the population as the parameter of interest, we take the sample mean of the bootstrap.\n\n\nset.seed(407500)\n\n# Resample the IDs\nb1_ids &lt;- sample(flips_tib$flip_id, size = nrow(flips_tib), replace = TRUE)\n\n# Notice how we could theoretically get more than one instance\nhead(table(b1_ids))\n\nb1_ids\n1 2 3 4 5 7 \n1 1 2 2 2 1 \n\n# We don't want the IDs, we just want the outcomes of the flips\nb1 &lt;- flips_tib$outcome[b1_ids]\n\n# Calculate the sample mean for this bootstrap\nb1_mean &lt;- mean(b1)\nb1_mean\n\n[1] 0.5167\n\n\nThat was one bootstrap. We want many bootstrap resamples. Let us select \\(B = 1,024\\). It is conventional to choose exponents of 2 for the number of bootstraps. This has efficiency reasons that are outside the scope of this class.\n\nlibrary(magrittr) # optional, but I want to use pipes\n\nset.seed(470500)\nB &lt;- 1024\n\nbs &lt;- vector()\n\nfor (b in 1:B) {\n  \n  bs[b] &lt;- sample(flips_tib$outcome, size = nrow(flips_tib), replace = TRUE) %&gt;%\n              mean()\n  \n}\n\n# Let us plot the density of the sample means from all the bootstraps\n\nplot(density(bs), main = \"Density of Bootstrap Means\")\n\n\n\n\n\n\n\n\n\nCompute \\(p\\)-value or confidence to report the conclusion from the hypothesis test. Instead of using the Normal distribution or Student’s \\(t\\) distribution, we will use the bootstrap distribution. A similar logic holds. We want to know if what we observe in the sample is unlikely if the null hypothesis were to be true. The \\(\\alpha\\)-level confidence interval is simply the \\(\\alpha/2\\) and the \\(1-\\alpha/2\\) quantiles.\n\n\n# 95% confidence interval\nquantile(bs, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.5130725 0.5323000 \n\n\nThe bootstrapped \\(p\\)-value is outside the scope of this class.\n\nConclude. Because 0.5 is outside the 95% confidence interval, we reject the null hypothesis, we reject that the coin is fair at the 95% confidence level.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "14_Inference.html#activity-bootstrapping",
    "href": "14_Inference.html#activity-bootstrapping",
    "title": "14  Inference",
    "section": "14.3 Activity: Bootstrapping",
    "text": "14.3 Activity: Bootstrapping\n\nCalculate the median of the distribution of amount won.\nSuppose we want to test if the median is equal to 1,100. What are the null and alternative hypotheses?\nWrite a loop to draw 1,024 bootstrap resamples. Here is an outline of the code. You fill in the commands.\n\n\n# Set a seed\n\n# Set the number of bootstraps\n\n# Define an empty vector to store the output\n\n# Start the loop\n\n    # Inside the loop:\n        \n    # Resample df$AmountWon with replacement\n\n    # Take the mean of the resample\n\n    # Store the mean to the vector\n\n\nCalculate the 95% confidence interval. What do you conclude?",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "14_Inference.html#references",
    "href": "14_Inference.html#references",
    "title": "14  Inference",
    "section": "14.4 References",
    "text": "14.4 References\nThe data on lottery winners come from the Texas Data Portal. See Casella and Berger (2002) for a formal discussion of inference.\n\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. Second edition. Duxbury Advanced Series. Andover Melbourne Mexico City Stamford, CT Toronto Hong Kong New Delhi Seoul Singapore Tokyo: CENGAGE Learning. https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "15_Regression.html",
    "href": "15_Regression.html",
    "title": "15  Regression",
    "section": "",
    "text": "15.1 Regression\nRegression is a powerful tool to investigate the dependence of one variable on another Casella and Berger (2002). In this chapter, we will review linear regression, its common pitfalls, and some useful non-linear specifications (logit and probit).\nIn the population, suppose there are three random variables \\(X\\), \\(U\\), and \\(Y\\). Because they are radnom variables, they have some underlying probability distributions and may have a statistical relationship. Generally, we may think of \\(Y\\) as a function of \\(X\\) and \\(U\\):\n\\[\\begin{equation*}\nY = f(X, U).\n\\end{equation*}\\]\nWe are not saying that \\(X\\) and \\(U\\) cause \\(Y\\) or anything like that. All we are saying is that we think there is a way to write \\(Y\\) as depending on the values that \\(X\\) and \\(U\\) take. In practice, having a general function \\(f()\\) makes the problem harder. We can assume that the function is linear. Even if it is not actually linear, a linear function is often an adequate approximation.\nSuppose we have a sample of size \\(N\\) units, indexed by \\(i\\). That is, \\(i = 1, 2, 3, \\ldots, N\\). Each unit \\(i\\) has a realization of the random variables: \\(Y_i\\), \\(X_i\\), and \\(U_i\\). Suppose that \\(U\\) is unobservable. That means that our dataset contains \\((X_1, X_2, \\ldots, X_N)\\) and \\((Y_1, Y_2, \\ldots, Y_N)\\). We can set up the linear regression as\n\\[\\begin{equation*}\nY = \\beta_0 + \\beta_1 X + U.\n\\end{equation*}\\]\nNote that \\(U\\) is unobserved so it does not matter if it has a coefficient attached to it. It is common to assume that \\(\\mathbb{E}(U) = 0\\). Even if this were not the case, the intercept \\(\\beta_0\\) can be rescaled to accommodate it. Then, the interpretation is that the expected value of \\(Y\\) given the value of \\(X = x\\) is:\n\\[\\begin{equation*}\n\\mathbb{E}(Y \\vert X = x) = \\beta_0 + \\beta_1 x.\n\\end{equation*}\\]\nThe estimation of \\(\\beta_0\\) and \\(\\beta_1\\) can be done using Ordinary Least Squares (OLS). Let us use R for this. We will use data from the General Social Survey (GSS) available on Canvas for download. Suppose we are interested of the relationship between education and income.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(haven)\nlibrary(magrittr)\n\n# Remember to set your working directory first\n#setwd(\"Your/Working/Directory/Here\")\ngss &lt;- read_dta(\"Data/Regression/GSS2022.dta\")\n\n# Summarize education and income\ngss %&gt;%\n  select(educ, conrinc) %&gt;%\n  summary()\n\n      educ          conrinc        \n Min.   : 0.00   Min.   :   316.5  \n 1st Qu.:12.00   1st Qu.: 15033.8  \n Median :14.00   Median : 28485.0  \n Mean   :14.15   Mean   : 40508.0  \n 3rd Qu.:16.00   3rd Qu.: 52222.5  \n Max.   :20.00   Max.   :163572.7  \n NA's   :29      NA's   :1804\nWhile it would work to write out the expressions for the OLS estimator explicitly in R, it is practical to use a function for linear regression. The built-in function lm() is great. Note that it uses the formula specification where ~ means equals. Given that we specify the data, we should not reference variables using gss$conrinc or similar subsetting methods. Note that it automatically estimates a regression with an intercept.\nlm(formula = conrinc ~ educ,\n   data = gss)\n\n\nCall:\nlm(formula = conrinc ~ educ, data = gss)\n\nCoefficients:\n(Intercept)         educ  \n     -28642         4761\nIf you want to estimate the regression without the intercept, subtract 1 or add 0.\nlm(formula = conrinc ~ educ - 1,\n   data = gss)\n\n\nCall:\nlm(formula = conrinc ~ educ - 1, data = gss)\n\nCoefficients:\neduc  \n2862  \n\nlm(formula = conrinc ~ educ + 0,\n   data = gss)\n\n\nCall:\nlm(formula = conrinc ~ educ + 0, data = gss)\n\nCoefficients:\neduc  \n2862\nWhile the print out of this command is useful to get the broad strokes the regression, there is a lot more information hidden in the object that lm() returns. Assign the output to an object in the environment to see it. That way you can access elements of it as a list.\nout &lt;- lm(formula = conrinc ~ educ,\n          data = gss)\n\n#str(out)\nnames(out)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"\nThese elements may be particularly useful.\nWe can take this a step furhter using summary(). This is a more convenient output.\nsummary(out)\n\n\nCall:\nlm(formula = conrinc ~ educ, data = gss)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-66253 -22788  -9521   9725 173172 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -28641.9     3961.2  -7.231  6.5e-13 ***\neduc          4760.6      267.5  17.796  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36390 on 2329 degrees of freedom\n  (1818 observations deleted due to missingness)\nMultiple R-squared:  0.1197,    Adjusted R-squared:  0.1193 \nF-statistic: 316.7 on 1 and 2329 DF,  p-value: &lt; 2.2e-16\n\nout_summary &lt;- summary(out)\n\n#str(out_summary)\nnames(out_summary)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"  \"na.action\"\nNotice that the object from summary() has other elements. These elements may be particularly useful.\nNow that we are comfortable with the basics of lm(), let us explore adding more variables and using other specifications. To add variables, use +.\n# Create a variable for experience\ngss &lt;- gss %&gt;%\n  mutate(exp = age - (educ + 5))\n\nlm(formula = conrinc ~ educ + exp,\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = conrinc ~ educ + exp, data = gss)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-73546 -20917  -7928   9913 170963 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -42069.86    4337.30   -9.70  &lt; 2e-16 ***\neduc          5030.36     270.28   18.61  &lt; 2e-16 ***\nexp            383.12      51.91    7.38 2.22e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35790 on 2237 degrees of freedom\n  (1909 observations deleted due to missingness)\nMultiple R-squared:  0.1425,    Adjusted R-squared:  0.1418 \nF-statistic: 185.9 on 2 and 2237 DF,  p-value: &lt; 2.2e-16\nThe classic Mincer equation regresses the log of the wage on education, experience, and experience squared. We can do these mathematical transformations within the formula call. It can also be done by creating a new variable or transforming the existing variable in the dataset. Just be mindful of the values. For example, given we are taking the log of income, we should check that there are no incomes less than or equal to zero. The function I() ensures that the formula interprets the expression as mathematical (i.e., squared) rather than an expression for the formula.\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2),\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2), data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3915 -0.4422  0.1464  0.6327  3.5533 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.143e+00  1.268e-01   56.32   &lt;2e-16 ***\neduc         1.443e-01  7.551e-03   19.11   &lt;2e-16 ***\nexp          7.815e-02  5.146e-03   15.19   &lt;2e-16 ***\nI(exp^2)    -1.254e-03  9.171e-05  -13.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9974 on 2236 degrees of freedom\n  (1909 observations deleted due to missingness)\nMultiple R-squared:  0.2156,    Adjusted R-squared:  0.2145 \nF-statistic: 204.9 on 3 and 2236 DF,  p-value: &lt; 2.2e-16\nAlso remember that linear regression means linear in the coefficients. That means that it is fine to have non-linear functions of the covariates like we do for experience squared. These are all continuous variables. It is common to want to include dummies. It does not matter if the variable is numeric (0 or 1) or logical (TRUE or FALSE).\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex,\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex, data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1993 -0.4232  0.1642  0.5970  3.3478 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.740e+00  1.378e-01   56.17   &lt;2e-16 ***\neduc         1.478e-01  7.401e-03   19.98   &lt;2e-16 ***\nexp          7.623e-02  5.042e-03   15.12   &lt;2e-16 ***\nI(exp^2)    -1.227e-03  8.983e-05  -13.66   &lt;2e-16 ***\nsex         -4.128e-01  4.136e-02   -9.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9763 on 2234 degrees of freedom\n  (1910 observations deleted due to missingness)\nMultiple R-squared:  0.2491,    Adjusted R-squared:  0.2477 \nF-statistic: 185.2 on 4 and 2234 DF,  p-value: &lt; 2.2e-16\nLet us add a categorical variable that takes more than two values (not an indicator). Notice that the values of race are 1, 2, 3, and NA corresponding to white, black, other, and missing. If we incldue the variable as-is in the regression, interpretation will be difficult. An easy fix is to transform it to a factor variable.\ntable(gss$race, useNA = \"always\")\n\n\n   1    2    3 &lt;NA&gt; \n2651  775  659   64 \n\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex + race,\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex + race, \n    data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1945 -0.4223  0.1623  0.6018  3.3459 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.732e+00  1.459e-01  52.985   &lt;2e-16 ***\neduc         1.481e-01  7.428e-03  19.936   &lt;2e-16 ***\nexp          7.604e-02  5.063e-03  15.020   &lt;2e-16 ***\nI(exp^2)    -1.223e-03  9.026e-05 -13.550   &lt;2e-16 ***\nsex         -4.161e-01  4.157e-02 -10.009   &lt;2e-16 ***\nrace         5.859e-03  2.745e-02   0.213    0.831    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9776 on 2216 degrees of freedom\n  (1927 observations deleted due to missingness)\nMultiple R-squared:  0.2496,    Adjusted R-squared:  0.2479 \nF-statistic: 147.4 on 5 and 2216 DF,  p-value: &lt; 2.2e-16\n\n# Transformed to a factor variable\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex + as.factor(race),\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex + as.factor(race), \n    data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2060 -0.4303  0.1736  0.5881  3.3330 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       7.760e+00  1.398e-01  55.502   &lt;2e-16 ***\neduc              1.467e-01  7.435e-03  19.730   &lt;2e-16 ***\nexp               7.612e-02  5.056e-03  15.057   &lt;2e-16 ***\nI(exp^2)         -1.225e-03  9.014e-05 -13.595   &lt;2e-16 ***\nsex              -4.076e-01  4.164e-02  -9.789   &lt;2e-16 ***\nas.factor(race)2 -1.258e-01  5.604e-02  -2.246   0.0248 *  \nas.factor(race)3  5.791e-02  5.745e-02   1.008   0.3135    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9762 on 2215 degrees of freedom\n  (1927 observations deleted due to missingness)\nMultiple R-squared:  0.252, Adjusted R-squared:   0.25 \nF-statistic: 124.4 on 6 and 2215 DF,  p-value: &lt; 2.2e-16\n\n# More careful transformation to a factor variable\ngss &lt;- gss %&gt;%\n  mutate(race_factor = factor(race, labels = c(\"WHITE\", \"BLACK\", \"OTHER\")))\n\n# Note that it automatically omits the first level of the factor, in this case WHITE\n\n# Use relevel to set a different reference\ngss &lt;- gss %&gt;%\n  mutate(race_factor = relevel(race_factor, ref = \"BLACK\"))\n\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex + race_factor,\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + sex + race_factor, \n    data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2060 -0.4303  0.1736  0.5881  3.3330 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       7.635e+00  1.444e-01  52.855   &lt;2e-16 ***\neduc              1.467e-01  7.435e-03  19.730   &lt;2e-16 ***\nexp               7.612e-02  5.056e-03  15.057   &lt;2e-16 ***\nI(exp^2)         -1.225e-03  9.014e-05 -13.595   &lt;2e-16 ***\nsex              -4.076e-01  4.164e-02  -9.789   &lt;2e-16 ***\nrace_factorWHITE  1.258e-01  5.604e-02   2.246   0.0248 *  \nrace_factorOTHER  1.838e-01  7.148e-02   2.571   0.0102 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9762 on 2215 degrees of freedom\n  (1927 observations deleted due to missingness)\nMultiple R-squared:  0.252, Adjusted R-squared:   0.25 \nF-statistic: 124.4 on 6 and 2215 DF,  p-value: &lt; 2.2e-16\nInteractions can be added with * or :. Note the difference below.\nlm(formula = log(conrinc) ~ sex*educ + exp + I(exp^2) + race_factor,\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ sex * educ + exp + I(exp^2) + race_factor, \n    data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2681 -0.4289  0.1682  0.5915  3.2842 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       8.117e+00  3.465e-01  23.425  &lt; 2e-16 ***\nsex              -7.343e-01  2.173e-01  -3.379 0.000739 ***\neduc              1.134e-01  2.300e-02   4.929 8.86e-07 ***\nexp               7.612e-02  5.054e-03  15.062  &lt; 2e-16 ***\nI(exp^2)         -1.225e-03  9.011e-05 -13.593  &lt; 2e-16 ***\nrace_factorWHITE  1.248e-01  5.602e-02   2.228 0.025957 *  \nrace_factorOTHER  1.854e-01  7.147e-02   2.594 0.009537 ** \nsex:educ          2.250e-02  1.468e-02   1.532 0.125667    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9759 on 2214 degrees of freedom\n  (1927 observations deleted due to missingness)\nMultiple R-squared:  0.2528,    Adjusted R-squared:  0.2505 \nF-statistic:   107 on 7 and 2214 DF,  p-value: &lt; 2.2e-16\n\nlm(formula = log(conrinc) ~ sex:educ + exp + I(exp^2) + race_factor,\n          data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ sex:educ + exp + I(exp^2) + race_factor, \n    data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8045 -0.5021  0.1699  0.6831  3.1987 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       8.9383502  0.1034027  86.442  &lt; 2e-16 ***\nexp               0.0811234  0.0055512  14.614  &lt; 2e-16 ***\nI(exp^2)         -0.0013716  0.0000988 -13.882  &lt; 2e-16 ***\nrace_factorWHITE  0.2438159  0.0612984   3.978 7.19e-05 ***\nrace_factorOTHER  0.3035977  0.0783431   3.875  0.00011 ***\nsex:educ          0.0050231  0.0026394   1.903  0.05715 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.073 on 2216 degrees of freedom\n  (1927 observations deleted due to missingness)\nMultiple R-squared:  0.09584,   Adjusted R-squared:  0.0938 \nF-statistic: 46.98 on 5 and 2216 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "15_Regression.html#regression",
    "href": "15_Regression.html#regression",
    "title": "15  Regression",
    "section": "",
    "text": "coefficients. This vector contains the estimated coefficients.\nresiduals. For each unit, the residual is \\(Y_i - \\hat{Y}_i\\). In this example, that is \\(Y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\).\nfitted.values. For each unit, \\(\\hat{Y}_i\\).\ndf.residual. Residual degrees of freedom. This is the number of units minus the number of coefficients.\nna.action. Information about NA values and how they were handled.\nxlevels. If the variables are factors, then information about the levels.\n\n\n\n\n\ncoefficients. Note that this looks different and has more information than out$coefficients. It also contains the standard errors, \\(t\\)-statistics (the estimated coefficient divided by the standard error), and the two-sided \\(p\\)-value testing if the coefficient is different than 0.\nsigma. Mean square error where \\(p\\) is the number of coefficients: \\(\\frac{1}{n - p} \\sum_{i}^n Y_i - \\hat{Y}_i\\).\ndf. A vector with three numbers: the total number of coefficients from covariates that are not linearly dependent \\((p)\\), the number of observations minus this \\((n-p)\\), and the total number of coefficients includign those that are linearly dependent. Note that you want the first and last numbers to be the same. Otherwise, you are specifying a linear regression with multicollinearity.\nr.squared. If there is an intercept, it can be intuitively understood as the fraction of variance that the model explains. The formula is \\(1 - \\frac{\\frac{1}{n - p} \\sum_{i}^n Y_i - \\hat{Y}_i}{\\sum_i^n (Y_i - \\bar{Y})^2}\\).\nadj.r.squared. This \\(R^2\\) is the same idea but it penalizes additional covariates.\nfstatistic. A vector with three numbers: the \\(F\\)-statistic and its degrees of freedom.\n\n\n\n\n\n\n\n\n\n\n\n\n15.1.1 Interpretation of Linear and Non-Linear Specifications\nThe interpretation of the coefficients depends on the specification. I go through common specifications below.\n\nLinear. \\(Y = \\beta_0 + \\beta_1 X + U\\). Increasing \\(X\\) by one unit changes \\(Y\\) by \\(\\hat{\\beta}_1\\) units on average.\nLog-Linear. \\(\\ln(Y) = \\beta_0 + \\beta_1 X+ U\\). Increasing \\(X\\) by one unit approximately changes \\(Y\\) by \\(\\hat{\\beta}_1 \\times 100\\) percent on average.\nLinear-Log. \\(Y = \\beta_0 + \\beta_1 \\ln(X)+ U\\). Increasing \\(X\\) by one percent approximately changes \\(Y\\) by \\(\\hat{\\beta}_1 \\times 0.01\\) percent on average.\nLog-Log. \\(\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X)+ U\\). Increasing \\(X\\) by one percent approximately changes \\(Y\\) by \\(\\hat{\\beta}_1 \\times 100\\) percent on average.\nMultivariate Regression. \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1 + \\ldots + U\\). Increasing \\(X_1\\) by one unit changes changes \\(Y\\) by \\(\\hat{\\beta}_1\\) units, holding all other covariates constant, changes \\(Y\\) by \\(\\hat{\\beta}_1\\) on average.\n\n\n\n15.1.2 Regression Assumptions\nNow that we are empowered with R to run regressions, it can be easy to lose sight of the theory. Recall the core assumptions of the classic framework.\n\nLinear Model. This assumption states that the true model of the relationship between \\(X\\) and \\(Y\\) is linear.\nStrict Exogeneity. \\(\\mathbb{E}(U \\vert X) = \\mathbb{E}(U) = 0.\\) This is a stronger way of saying that \\(U\\) and \\(X\\) are uncorrelated. A common way to break this assumption is to have ommited variable bias.\nNo Perfect Multicollinearity. The covariates need to be linearly independent. For example, the below regression drops age because it is a linear function of experience. But, experience squared is allowed because it is not a linear function of experience.\n\n\nlm(log(conrinc) ~ educ + exp + I(exp^2) + age,\n   data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = log(conrinc) ~ educ + exp + I(exp^2) + age, data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3915 -0.4422  0.1464  0.6327  3.5533 \n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.143e+00  1.268e-01   56.32   &lt;2e-16 ***\neduc         1.443e-01  7.551e-03   19.11   &lt;2e-16 ***\nexp          7.815e-02  5.146e-03   15.19   &lt;2e-16 ***\nI(exp^2)    -1.254e-03  9.171e-05  -13.67   &lt;2e-16 ***\nage                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9974 on 2236 degrees of freedom\n  (1909 observations deleted due to missingness)\nMultiple R-squared:  0.2156,    Adjusted R-squared:  0.2145 \nF-statistic: 204.9 on 3 and 2236 DF,  p-value: &lt; 2.2e-16\n\n\n\nHomoscedasticity. The variance of the error term is constant and finite.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "15_Regression.html#categorical-outcome-variables",
    "href": "15_Regression.html#categorical-outcome-variables",
    "title": "15  Regression",
    "section": "15.2 Categorical Outcome Variables",
    "text": "15.2 Categorical Outcome Variables\nAbove, we always assumed that \\(Y\\) was continuous. If \\(Y\\) is discrete, then we want to consider alternative specifications. We can certainly use the variable as is. However, the interpretation may not capture the goal of the analysis.\n\n# Diabetes \ntable(gss$diabetes, useNA = \"always\")\n\n\n   1    2 &lt;NA&gt; \n 293 2060 1796 \n\n#  Interpreting variable as if it were numeric\nlm(diabetes ~ educ,\n   data = gss) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = diabetes ~ educ, data = gss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8756  0.1246  0.1248  0.1251  0.1260 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.874e+00  3.611e-02  51.893   &lt;2e-16 ***\neduc        8.692e-05  2.433e-03   0.036    0.972    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3307 on 2336 degrees of freedom\n  (1811 observations deleted due to missingness)\nMultiple R-squared:  5.465e-07, Adjusted R-squared:  -0.0004275 \nF-statistic: 0.001277 on 1 and 2336 DF,  p-value: 0.9715\n\n\nFor the case of a binary \\(Y\\) variable (it only takes two values), common specifications are probit and logit. Probit uses the cumulative distribution function (CDF) of the Normal distribution\n\\[\\begin{equation*}\n\\mathbb{E}(Y \\vert X = x) = \\Pr(Y = 1 \\vert X = x) = \\Phi(\\beta_0 + \\beta_1 X),\n\\end{equation*}\\]\nwhere \\(\\Phi(x) = \\Pr(Z \\leq x)\\) for \\(Z\\) distributed according to the standard Normal distribution (mean is 0 and standard deviation is 1). We can use glm() to estimate probit regressions.\n\n# Transform the variable to be 0/1 instead of 1/2\ngss &lt;- gss %&gt;%\n  mutate(diabetes_binary = diabetes - 1)\ntable(gss$diabetes_binary)\n\n\n   0    1 \n 293 2060 \n\nglm(diabetes_binary ~ educ,\n    family = binomial(link = \"probit\"),\n    data = gss) %&gt;%\n  summary()\n\n\nCall:\nglm(formula = diabetes_binary ~ educ, family = binomial(link = \"probit\"), \n    data = gss)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 1.1448377  0.1753822   6.528 6.68e-11 ***\neduc        0.0004139  0.0118182   0.035    0.972    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1760.8  on 2337  degrees of freedom\nResidual deviance: 1760.8  on 2336  degrees of freedom\n  (1811 observations deleted due to missingness)\nAIC: 1764.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe interpretation is that if \\(X\\) increases by 1, the \\(Z\\)-score of \\(Y\\) changes by \\(\\hat{\\beta}_1\\). That is:\n\\[\\begin{equation*}\n\\widehat{\\Pr(Y = 1 \\vert X = x)} = \\Phi(\\hat{\\beta}_0 + \\hat{\\beta}_1 X).\n\\end{equation*}\\]\nWe can implement logit by changing the link function. The equation for the logistic regression is\n\\[\\begin{equation*}\n\\mathbb{E}(Y \\vert X = x) = \\Pr(Y = 1 \\vert X = x) = \\frac{\\exp(\\beta_0 + \\beta_1 X)}{1 + \\exp(\\beta_0 + \\beta_1 X)}.\n\\end{equation*}\\]\n\nlibrary(margins)\n\nglm(diabetes_binary ~ educ,\n    family = binomial(link = \"logit\"),\n    data = gss) %&gt;%\n  summary()\n\n\nCall:\nglm(formula = diabetes_binary ~ educ, family = binomial(link = \"logit\"), \n    data = gss)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 1.9353024  0.3300540   5.864 4.53e-09 ***\neduc        0.0007951  0.0222435   0.036    0.971    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1760.8  on 2337  degrees of freedom\nResidual deviance: 1760.8  on 2336  degrees of freedom\n  (1811 observations deleted due to missingness)\nAIC: 1764.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe interpretation is that if \\(X\\) increases by one unit, the log of the odds ratio changes by \\(\\hat{\\beta}_1\\). The odds ratio is\n\\[\\begin{equation*}\n\\ln \\left[ \\frac{\\Pr(Y = 1)}{\\Pr(Y = 0)} \\right].\n\\end{equation*}\\]\nAnother way to put this is that if \\(X\\) increases by one unit the odds ratio changs by \\(\\exp(\\hat{\\beta}_1)\\).\nThese are two examples of regression models with a binary outcome variable. If \\(Y\\) is discrete but takes more than two values, then another model is needed. Multinomial logit is popular. In R, it is convenint to use the package nnet.\n\nlibrary(nnet)\n\n# Would you say that in general your health is Excellent, Very good, Good, Fair, or Poor?\ntable(gss$health1, useNA = \"always\")\n\n\n   1    2    3    4    5 &lt;NA&gt; \n 383  711  848  385   36 1786 \n\nmultinom(health1 ~ educ,\n         data = gss) %&gt;%\n  summary()\n\n# weights:  15 (8 variable)\ninitial  value 3777.350780 \niter  10 value 3222.809202\nfinal  value 3221.902559 \nconverged\n\n\nCall:\nmultinom(formula = health1 ~ educ, data = gss)\n\nCoefficients:\n  (Intercept)        educ\n2   0.8222059 -0.01341558\n3   2.0885715 -0.08794609\n4   2.1510784 -0.14766551\n5   0.2127827 -0.17879801\n\nStd. Errors:\n  (Intercept)       educ\n2   0.3578367 0.02343904\n3   0.3438709 0.02276279\n4   0.3898079 0.02635518\n5   0.8370180 0.05932815\n\nResidual Deviance: 6443.805 \nAIC: 6459.805 \n\n\nHere we can interpret the coefficients as follows. We are comparing the log odds ratio of each value of \\(Y\\) to a baseline value of \\(Y\\). In this example, it is \\(HEALTH1 = 1\\), which corresponds to excellent health. You can convert the variable to a factor and use relevel() to specify a different baseline category. The intercept and coefficient on \\(X\\) depends on the level of \\(Y\\).",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "15_Regression.html#the-goal-of-program-evaluation",
    "href": "15_Regression.html#the-goal-of-program-evaluation",
    "title": "15  Regression",
    "section": "15.3 The Goal of Program Evaluation",
    "text": "15.3 The Goal of Program Evaluation\nThe goal of program evaluation is to study the effectiveness of interventions. Decision makers, including policymakers and industry leaders, need to know to what extent (possibly costly) interventions will have their intended effects.\nIn public policy and economics, emblematic questions in program evaluation include:\n\nDo programs that transfer cash to individuals reduce poverty?\nDoes increasing the minimum wage affect employment?\nDoes health insurance improve health outcomes?\n\nIn industry, questions requiring casual analysis might be:\n\nDoes changing the price of one item affect the overall amount customers spend?\nHow does changing an interface affect user engagement?\nDo personalized discounts or recommendations result in additional spending?\n\nTo consider these questions, we cannot rely solely on basic regression analysis.\n\n15.3.1 Causality vs. Correlation\nRegression gets at correlation, not at causality. While this may be intuitive, we can review the fundamental concepts to have a more complete understanding of why regression may be inappropriate for drawing certain conclusions from the data.\n\n15.3.1.1 Causality\nIn everyday situations, there can be a clear understanding of cause and effect. If I enter the pin of my door code, that will cause the front door of my house to unlock. There is no question here what was the cause and what was the effect. Decision makers are interested in understanding causality when it is not so obvious.\nCausality is the effects within a causal model keeping other conditions the same (ceteris paribus). A causal model basically contains the following elements\n\nVariables determined inside the model (\\(Y\\)). These are called outcome or dependent variables.\nVariables determined outside the model \\((X, U)\\). These are called covariates, regressors, or independent variables.\nFunctional relationships between \\((X, U)\\) and \\(Y\\). This can be written generally as \\(Y = g(X,U)\\).\n\nCausality refers to some inherent relationship of cause \\((X, U)\\) and effect \\(Y\\).\n\n\n15.3.1.2 Correlation\nCorrelation refers to a statistical relationship between \\(X\\) and \\(Y\\). Mathematically, it is\n\\[\\begin{equation*}\n\\rho_{X, Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}.\n\\end{equation*}\\]\n\n\\(\\text{Cov}(X, Y)\\) is the covariance between \\(X\\) and \\(Y\\). It is equal to \\(\\mathbb{E}\\left[(X - \\mu_X)(Y - \\mu_Y)\\right]\\), where \\(\\mu_X\\) and \\(\\mu_Y\\) are the means of \\(X\\) and \\(Y\\). The covariance measures how \\(X\\) varies with \\(Y\\) and how \\(Y\\)varies with \\(X\\).\n\\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\).\n\nBecause the correlation is the covariance divided by the standard deviations, it can be thought of as a rescaled covariance. There are some important properties of correlation.\n\nCorrelation is symmetric. This is one clear reason why the concepts of correlation and causality are separate.\nThe sign of the correlation is equal to the sign of the covariance.\nThe correlation is between \\(-1\\) and \\(1\\). This helps give a sense of the direction and magnitude of the linear relationship between \\(X\\) and \\(Y\\).",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "15_Regression.html#activity-practice-with-regression",
    "href": "15_Regression.html#activity-practice-with-regression",
    "title": "15  Regression",
    "section": "15.4 Activity: Practice with Regression",
    "text": "15.4 Activity: Practice with Regression\nThis activity will help you practice regression in R. Write your code in a .R file and upload it to Canvas at the end of class for participation points in today’s class. You can write answers to the questions using comments. Do not worry about finsihing it, just get as far as you can. You will be graded for completion.\n\nGo to Canvas &gt; Modules &gt; Module 1 &gt; Data. Download GS2022.dta and the codebook. Save them to a convenient folder.\nWhat format is the data in? Load the data into R. Hint: Use the haven package.\nIn this activity, you will be looking at the variables vote20 and vote16. Search these variables in the codebook until you get to the part that shows what the values represent.\nExplore these variables. What is the voting turn-out in the GSS for the years 2016 and 2020? Using Google, how do they compare to the general population turn-out? What might this say about the GSS sample?\nCreate two new variables vote_binary20 and vote_binary16. They should take the value TRUE if the individual voted and FALSE if they did not vote. If the individual is ineligible, the value should be NA. Using the command table(), check that your new variables are correctly defined.\nSuppose you are interested in understanding how different characteristics are correlated with voting in the 2020 presidential election. Check the documentation and run summary statistics on the following variables about basic demographic characteristics: conrinc, educ, hispanic, race, marital, age, sex, born, partyid. Run a linear regression using these variables as predictors for voting in 2020. Make sure you use the variable you created in question 5.\nRun probit and logit regressions using these varaibles as predictors for voting in 2020.\nBased on 6 and 7, do you find anything interesting or surprising?\nDo your own exploration of the codebook and look for other variables you think could be useful to predict voter turn-out for the 2020 presidential election. Feel free to keep or drop the variables from question 6. For each variable you use, make sure you first check the basic summary statistics of the variable.\nDid you find anything interesting or surprising? If someone were interested in predicting voter turn-out in 2024 and 2028, what other variables might they need that are not included in this dataset?",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "15_Regression.html#references",
    "href": "15_Regression.html#references",
    "title": "15  Regression",
    "section": "15.5 References",
    "text": "15.5 References\nThe extract from the Census comes from the NORC. See Chatterjee and Hadi (2006) for an accessible background on regression. The notes on causality draw from Matt Mastens’s Identification and Causality notes.\n\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. Second edition. Duxbury Advanced Series. Andover Melbourne Mexico City Stamford, CT Toronto Hong Kong New Delhi Seoul Singapore Tokyo: CENGAGE Learning. https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/.\n\n\nChatterjee, Samprit, and Ali S. Hadi. 2006. Regression Analysis by Example. 4th ed. Wiley Series in Probability and Statistics. Hoboken (N.J.): Wiley-Interscience. https://onlinelibrary.wiley.com/doi/book/10.1002/0470055464.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "16_Program_Evaluation.html",
    "href": "16_Program_Evaluation.html",
    "title": "16  Program Evaluation",
    "section": "",
    "text": "16.1 The Goal of Program Evaluation\nThe goal of program evaluation is to study the effectiveness of interventions. Decision makers, including policymakers and industry leaders, need to know to what extent (possibly costly) interventions will have their intended effects.\nIn public policy and economics, emblematic questions in program evaluation include:\nIn industry, questions requiring casual analysis might be:\nTo consider these questions, we cannot rely solely on basic regression analysis.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Program Evaluation</span>"
    ]
  },
  {
    "objectID": "16_Program_Evaluation.html#the-goal-of-program-evaluation",
    "href": "16_Program_Evaluation.html#the-goal-of-program-evaluation",
    "title": "16  Program Evaluation",
    "section": "",
    "text": "Do programs that transfer cash to individuals reduce poverty?\nDoes increasing the minimum wage affect employment?\nDoes health insurance improve health outcomes?\n\n\n\nDoes changing the price of one item affect the overall amount customers spend?\nHow does changing an interface affect user engagement?\nDo personalized discounts or recommendations result in additional spending?",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Program Evaluation</span>"
    ]
  },
  {
    "objectID": "16_Program_Evaluation.html#causality-vs.-correlation",
    "href": "16_Program_Evaluation.html#causality-vs.-correlation",
    "title": "16  Program Evaluation",
    "section": "16.2 Causality vs. Correlation",
    "text": "16.2 Causality vs. Correlation\nRegression gets at correlation, not at causality. While this may be intuitive, we can review the fundamental concepts to have a more complete understanding of why regression may be inappropriate for drawing certain conclusions from the data.\n\n16.2.1 Causality\nIn everyday situations, there can be a clear understanding of cause and effect. If I enter the pin of my door code, that will cause the front door of my house to unlock. There is no question here what was the cause and what was the effect. Decision makers are interested in understanding causality when it is not so obvious.\nCausality is the effects within a causal model keeping other conditions the same (ceteris paribus). A causal model basically contains the following elements\n\nVariables determined inside the model (\\(Y\\)). These are called outcome or dependent variables.\nVariables determined outside the model \\((X, U)\\). These are called covariates, regressors, or independent variables.\nFunctional relationships between \\((X, U)\\) and \\(Y\\). This can be written generally as \\(Y = g(X,U)\\).\n\nCausality refers to some inherent relationship of cause \\((X, U)\\) and effect \\(Y\\).\n\n\n16.2.2 Correlation\nCorrelation refers to a statistical relationship between \\(X\\) and \\(Y\\). Mathematically, it is\n\\[\\begin{equation*}\n\\rho_{X, Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}.\n\\end{equation*}\\]\n\n\\(\\text{Cov}(X, Y)\\) is the covariance between \\(X\\) and \\(Y\\). It is equal to \\(\\mathbb{E}\\left[(X - \\mu_X)(Y - \\mu_Y)\\right]\\), where \\(\\mu_X\\) and \\(\\mu_Y\\) are the means of \\(X\\) and \\(Y\\). The covariance measures how \\(X\\) varies with \\(Y\\) and how \\(Y\\)varies with \\(X\\).\n\\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\).\n\nBecause the correlation is the covariance divided by the standard deviations, it can be thought of as a rescaled covariance. There are some important properties of correlation.\n\nCorrelation is symmetric. This is one clear reason why the concepts of correlation and causality are separate.\nThe sign of the correlation is equal to the sign of the covariance.\nThe correlation is between \\(-1\\) and \\(1\\). This helps give a sense of the direction and magnitude of the linear relationship between \\(X\\) and \\(Y\\).",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Program Evaluation</span>"
    ]
  },
  {
    "objectID": "16_Program_Evaluation.html#challenges-of-program-evaluation",
    "href": "16_Program_Evaluation.html#challenges-of-program-evaluation",
    "title": "16  Program Evaluation",
    "section": "16.3 Challenges of Program Evaluation",
    "text": "16.3 Challenges of Program Evaluation\nTo understand why program evaluation is challenging, it is useful to define the potential outcomes framework.\n\n16.3.1 Potential Outcomes\nSuppose there is an outcome \\(Y\\). What if we were to “fix” units to some value of \\(X\\) called \\(x\\). Note that this does not mean that we always actually see units with this value of \\(x\\) in real life. It is the thought experiment of setting \\(X\\) to a certain value. Then, the potential outcome is \\(Y(x)\\). Based on the function defined above, this is equal to \\(g(x, U)\\). We can make this more concrete.\nConsider that there is some job training. \\(D\\) indicates whether the individual receives treatment (\\(D = 1\\)) or not (\\(D = 0\\)). The potential outcome \\(Y(1)\\) is the outcome (wage) fixing the individual to receiving the treatment. The potential outcome \\(Y(0)\\) is the outcome fixing the individual to not receiving the treatment.\n\n\n16.3.2 Fundamental Problem of Causal Analysis\nThe potential outcome framework makes it simple to understand why causal analysis is hard. The fundamental problem is that we can never observe \\(Y(1)\\) and \\(Y(0)\\) at the same time for the same unit. There is no way to solve this problem and so we must use techniques in causal analysis. These techniques allow us to speak to causality, but under assumptions. We will never be able to compare, at the individual level, \\(Y(1)\\) and \\(Y(0)\\) from direct observation. The counterfactual is what would have happened in the absence of treatment.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Program Evaluation</span>"
    ]
  },
  {
    "objectID": "16_Program_Evaluation.html#common-threats-to-causal-analysis",
    "href": "16_Program_Evaluation.html#common-threats-to-causal-analysis",
    "title": "16  Program Evaluation",
    "section": "16.4 Common Threats to Causal Analysis",
    "text": "16.4 Common Threats to Causal Analysis\n\n16.4.1 Selection Bias\nThis is a very common issue that arises for program evaluation. If individuals can chose values of \\(X\\), then there may be a relationship between \\(U\\) and \\(Y\\). For example, people who are employed are a selected sample, even after considering observed characteristics.\nSuppose we evaluate a job training program and find that participants have higher wages than non-participants. This does not necessarily mean the program is effective. The people who sign up may already be more motivated or skilled.\n\n\n16.4.2 Omitted Variable Bias\nOmitted variable bias arises when an unobserved factor influences both treatment and the outcome.\nExample: We might observe that students in smaller classes perform better on standardized tests. However, if wealthier districts tend to have smaller classes, the true cause of higher test scores could be better funding, not class size.\n\n\n16.4.3 Reverse Causality\nReverse causality occurs when the outcome influences treatment rather than the other way around.\nExample: A study finds that hospitals with more doctors per patient have higher mortality rates. Does this mean more doctors cause worse health outcomes? Likely not—hospitals with high mortality rates might hire more doctors in response to sicker patients.\n\n\n16.4.4 Measurement Error\nIf variables are measured inaccurately, our estimates of causal effects may be biased.\nExample: If self-reported income is frequently understated, a study examining the effect of education on earnings might misrepresent the true relationship.\n\n\n16.4.5 Attrition\nWhen individuals drop out of a study non-randomly, it can distort results.\nExample: In a medical trial, if only the healthiest patients continue treatment while the sickest drop out, the program may appear more effective than it actually is.\n\n\n16.4.6 Spillover Effects\nSometimes, a treatment affects individuals who are not directly receiving it, making causal estimates difficult.\nExample: If a school introduces a new reading program, students in neighboring schools might also benefit if teachers share materials. The true effect of the program is then underestimated.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Program Evaluation</span>"
    ]
  },
  {
    "objectID": "16_Program_Evaluation.html#further-reading",
    "href": "16_Program_Evaluation.html#further-reading",
    "title": "16  Program Evaluation",
    "section": "16.5 Further Reading",
    "text": "16.5 Further Reading\nThese notes draw from Matt Mastens’s Identification and Causality notes.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Program Evaluation</span>"
    ]
  },
  {
    "objectID": "17_Instrumental_Variables.html",
    "href": "17_Instrumental_Variables.html",
    "title": "17  Instrumental Variables",
    "section": "",
    "text": "17.1 Problem",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "17_Instrumental_Variables.html#review-of-regression-and-assumptions",
    "href": "17_Instrumental_Variables.html#review-of-regression-and-assumptions",
    "title": "17  Instrumental Variables",
    "section": "17.2 Review of Regression and Assumptions",
    "text": "17.2 Review of Regression and Assumptions\nLet there be \\(N\\) units indexed by \\(i = 1, 2, \\ldots, N\\). For example, these could be people, households, firms, states, stocks, etc. For each unit, we observe an outcome variable of interest, \\(Y_i\\), and a regressor \\(X_i\\). We can run the below regression:\n\\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i + U_i.\n\\end{equation}\\]\nRecall that \\(U_i\\) is unobservable and \\(\\beta_0\\) and \\(\\beta_1\\) are the parameters.\nIf the following assumptions hold, then we can interpret the model as causal. That is, increasing \\(X_i\\) by one unit causes \\(Y_i\\) to change by \\(\\beta_1\\) units.\n\nLinear Model.\nStrict Exogeneity: \\(\\mathbb{E}(U \\vert X) = \\mathbb{E}(U) = 0\\).\nNo Perfect Multicollinearity.\nHomoscedasticity.\n\nWe saw this interpretation with randomized controlled trials. Suppose that \\(Y_i\\) is income and \\(X_i\\) is an indicator of whether an individual was randomized to attend job training. That is, if \\(X_i = 1\\) then individual \\(i\\) was randomized into the treatment group and attended the job training. If \\(X_i = 0\\), then individual \\(i\\) was randomized into the control group and did not attend the job training. Then \\(\\beta_1\\) will be the causal effect of job training on income. Note that in this case, \\(X_i\\) is exogenous under the assumption that randomizing means that not only are the observed variables the same between treatment and control, but also the unobserved factors (\\(U_i\\)) are the same between treatment and control.\nWhen exogeneity breaks down, then we cannot interpret the regression causally. There are several reasons why exogeneity may not hold. Regardless of the reason, \\(\\mathbb{E}(U\\vert X) \\neq \\mathbb{E}(U)\\).",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "17_Instrumental_Variables.html#basic-instrumental-variable-setup",
    "href": "17_Instrumental_Variables.html#basic-instrumental-variable-setup",
    "title": "17  Instrumental Variables",
    "section": "17.3 Basic Instrumental Variable Setup",
    "text": "17.3 Basic Instrumental Variable Setup\nSuppose we have the regression model from above:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + U_i.\n\\tag{17.1}\\]\nWe know that \\(X_i\\) is endogeneous, meaning that \\(\\mathbb{E}(U\\vert X) \\neq \\mathbb{E}(U)\\). Suppose we observe \\(Z_i\\), which is a valid instrumental variable if the following conditions hold:\n\nRelevance. \\(X\\) and \\(Z\\) are correlated.\nExogeneity. \\(Z\\) cannot be correlated with \\(U\\).\n\nWe can check for relevance in the data. That is because \\(X\\) and \\(Z\\) are both observed. We can simply calculate the correlation between them to see that they are related. However, we cannot check for exogeneity. This is an assumption on the unobserved term \\(U\\), so we must rely on logical arguments to justify exogeneity.\n\n17.3.1 Two-Stage Least Squares Estimator\nHow do we put into practice the basic instrumental variable setup? We need an estimation strategy. Two-stage least squares (TSLS) is a common approach. There are two stages.\n\nRegress the endogeneous regressor on the instrument:\n\n\\[\\begin{equation}\nX_i = \\pi_0 + \\pi_1 Z_i + V_i.\n\\end{equation}\\]\nThe unobserved variable \\(V_i\\) is the component of \\(X_i\\) not explained by the instrument. From this regression, we can predict the regressor. That means calculating \\(\\hat{X}_i\\) for all units. As long as \\(Z\\) satisfies the two assumptions, then we can interpret \\(\\hat{X}\\) as exogeneous to \\(Y\\).\n\nRegress the outcome on the predicted regressor:\n\n\\[\\begin{equation}\nY_i = \\alpha_0 + \\alpha_1 \\hat{X}_i + W_i.\n\\end{equation}\\]\nThen, \\(\\hat{\\alpha}_0\\) and \\(\\hat{\\alpha}_1\\) are the TSLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) from Equation Equation 17.1.\n\n17.3.1.1 R Implementation\nSuppose a policymaker wants to decrease smoking. They suggest increasing taxes on cigarettes to raise the price. In order to know the effect this policy will have on cigarette consumption, we need to know: what is the causal effect of increasing the price of cigarettes on the demand for cigarettes?\nWhy is this a difficult question to analyze? Demand and supply are determined simultaneously. It is possible that both the demand for cigarettes affects the price, and that the price affects the demand. This general source of endogeneity is called simultaneity.\nWe can use taxes on cigarettes as instruments for the price. Let us evaluate the two assumptions for this instrument.\n\nRelevance. Taxes affect the prices of cigarettes.\nExogeneity. Taxes indirectly affect the demand for cigarettes through prices. However, there are not other plausible channels for how taxes may affect the demand for cigarettes through other channels. This assumption can always be debated, but the argument of exogeneity is that apart from the effect through prices, there is no other effect.\n\nLet us run the analysis in R. We will use data compiled in Stock and Watson (2015). We can do some usual commands to explore the data.\n\n# Load this package so we can access the data\nlibrary(AER) \n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\n# Load other packages that wil be useful\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(magrittr)\n\n# Data on cigarette consumption for the 48 continental US from 1985-1995\ndata(\"CigarettesSW\") \n\n# Number of rows and columns\ndim(CigarettesSW)\n\n[1] 96  9\n\n# Names of variables\nnames(CigarettesSW)\n\n[1] \"state\"      \"year\"       \"cpi\"        \"population\" \"packs\"     \n[6] \"income\"     \"tax\"        \"price\"      \"taxs\"      \n\n# Summary statistics of all variables\nsummary(CigarettesSW)\n\n     state      year         cpi          population           packs       \n AL     : 2   1985:48   Min.   :1.076   Min.   :  478447   Min.   : 49.27  \n AR     : 2   1995:48   1st Qu.:1.076   1st Qu.: 1622606   1st Qu.: 92.45  \n AZ     : 2             Median :1.300   Median : 3697472   Median :110.16  \n CA     : 2             Mean   :1.300   Mean   : 5168866   Mean   :109.18  \n CO     : 2             3rd Qu.:1.524   3rd Qu.: 5901500   3rd Qu.:123.52  \n CT     : 2             Max.   :1.524   Max.   :31493524   Max.   :197.99  \n (Other):84                                                                \n     income               tax            price             taxs       \n Min.   :  6887097   Min.   :18.00   Min.   : 84.97   Min.   : 21.27  \n 1st Qu.: 25520384   1st Qu.:31.00   1st Qu.:102.71   1st Qu.: 34.77  \n Median : 61661644   Median :37.00   Median :137.72   Median : 41.05  \n Mean   : 99878736   Mean   :42.68   Mean   :143.45   Mean   : 48.33  \n 3rd Qu.:127313964   3rd Qu.:50.88   3rd Qu.:176.15   3rd Qu.: 59.48  \n Max.   :771470144   Max.   :99.00   Max.   :240.85   Max.   :112.63  \n                                                                      \n\n\nWe want to run this regression:\n\\[\\begin{equation}\n\\log(Q_i) = \\beta_0 + \\beta_1 \\log(P_i) + U_i.\n\\end{equation}\\]\nThe index \\(i\\) corresponds to states. Here, \\(Q_i\\) is the number of cigarette packs sold per capita and \\(P_i\\) is the average real price (after tax) per pack of cigarettes. The instrument is \\(S_i\\), which is the real price per pack. We are going to focus on one year of data (1995). After exploring the data, the next step is to clean the data for this analysis.\n\n# Limit to 1995\ncig &lt;- CigarettesSW %&gt;%\n  filter(year == 1995)\n\n# Calculate real prices\ncig &lt;- cig %&gt;%\n  mutate(P = price / cpi, # Real price \n         S = (taxs - tax) / cpi) # Real sales tax\n\n# Summarize variables we will use in the analysis\nsummary(cig[c(\"P\", \"S\", \"packs\")])\n\n       P                S              packs       \n Min.   : 95.79   Min.   : 0.000   Min.   : 49.27  \n 1st Qu.:109.32   1st Qu.: 4.641   1st Qu.: 80.23  \n Median :115.59   Median : 5.634   Median : 92.84  \n Mean   :120.24   Mean   : 5.364   Mean   : 96.33  \n 3rd Qu.:130.26   3rd Qu.: 7.280   3rd Qu.:109.27  \n Max.   :158.04   Max.   :10.264   Max.   :172.65  \n\n\nNow, we can perform the first stage of the TSLS estimator:\n\\[\\begin{equation}\n\\log(P_i) = \\pi_0 + \\pi_1 S_i + V_i.\n\\end{equation}\\]\n\ntsls_s1 &lt;- lm(log(P) ~ S,\n              data = cig)\n\nsummary(tsls_s1)\n\n\nCall:\nlm(formula = log(P) ~ S, data = cig)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.221027 -0.044324  0.000111  0.063730  0.210717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.616546   0.029108   158.6  &lt; 2e-16 ***\nS           0.030729   0.004802     6.4 7.27e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09394 on 46 degrees of freedom\nMultiple R-squared:  0.471, Adjusted R-squared:  0.4595 \nF-statistic: 40.96 on 1 and 46 DF,  p-value: 7.271e-08\n\n\nThe interpretation is that increasing the sales tax by 1 dollar per pack increases the per pack price by 3.07%. This is because the outcome variable is logged. We can use this regression to evaluate the first assumption of the instrument (relevance). All of the below criteria are related, but can be useful to check to understand the first stage.s\n\nSignificance of the estimated coefficients. We see that the coefficients are significantly different than 0. This means that the relationship between the sales tax and the price is statistically significant.\nMagnitude of the estimated coefficients. We determined that there is a statistical significance, but what about an economic significance? We can see this subjectively from the magnitude of the estimated coefficients. Increasing a price by approximately 3% seems like a substantial change in prices. Often this is discussed alongside other estimates or papers. For reference, consider that inflation was 2.9% in 2024.\n\\(R^2\\). We can see how much of the observed variation of the price is explained by the sales tax by examining the \\(R^2\\). We find that 47% of the variation is explained.\n\n\nsummary(tsls_s1)$r.squared\n\n[1] 0.4709961\n\n\n\n\\(F\\) statistic. The \\(F\\) statistic tells us the overall significance of the model. A common, but arbitrary, rule-of-thumb is that an instrument satisfies the relevance assumption if the \\(F\\) statistic is at least 10.\n\n\nsummary(tsls_s1)$fstatistic\n\n   value    numdf    dendf \n40.95588  1.00000 46.00000 \n\n\nWe can be satisfied that the instrument is relevant. From the regression, we calculate the fitted values. Note that I add it to the data.\n\ncig &lt;- cig %&gt;%\n  mutate(logP_pred = tsls_s1$fitted.values)\n\nThen, we use this to run the second stage:\n\\[\\begin{equation}\n\\log(Q_i) = \\alpha_0 + \\alpha_1 \\widehat{log({P}_i)} + W_i\n\\end{equation}\\]\n\ntsls_s2 &lt;- lm(log(packs) ~ logP_pred,\n              data = cig)\n\nsummary(tsls_s2)\n\n\nCall:\nlm(formula = log(packs) ~ logP_pred, data = cig)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63180 -0.15802  0.00524  0.13574  0.61434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.7199     1.8012   5.396  2.3e-06 ***\nlogP_pred    -1.0836     0.3766  -2.877  0.00607 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2264 on 46 degrees of freedom\nMultiple R-squared:  0.1525,    Adjusted R-squared:  0.1341 \nF-statistic: 8.277 on 1 and 46 DF,  p-value: 0.006069\n\n\nThe interpretation is that increasing the price per pack of cigarettes by 1% causally reduces cigarette demand by 1.08%. This holds under the assumptions above. If there are other threats to exogeneity, such as omitted variable bias, then these estimates can no longer be interpreted causally.\nWe can use the variable income to control for the tax policies and general economic condition of states. Let us see how including a covariate changes the process.\n\n# Create per capita real income\ncig &lt;- cig %&gt;%\n  mutate(real_inc = (income / population) / cpi)\n\n# First stage\ntsls_X_s1 &lt;- lm(log(P) ~ log(real_inc) + S, data = cig)\n\nsummary(tsls_X_s1)\n\n\nCall:\nlm(formula = log(P) ~ log(real_inc) + S, data = cig)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.163799 -0.033049  0.001907  0.049322  0.185542 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.590811   0.225558  15.920  &lt; 2e-16 ***\nlog(real_inc) 0.389283   0.085104   4.574 3.74e-05 ***\nS             0.027395   0.004077   6.720 2.65e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07848 on 45 degrees of freedom\nMultiple R-squared:  0.6389,    Adjusted R-squared:  0.6228 \nF-statistic: 39.81 on 2 and 45 DF,  p-value: 1.114e-10\n\nsummary(tsls_X_s1)$fstatistic\n\n value  numdf  dendf \n39.809  2.000 45.000 \n\nsummary(tsls_X_s1)$r.squared\n\n[1] 0.6388965\n\ncig &lt;- cig %&gt;%\n  mutate(logP_X_pred = tsls_X_s1$fitted.values)\n\n\n# Second stage\ntsls_X_s2 &lt;- lm(log(packs) ~ log(real_inc) + logP_X_pred, data = cig)\nsummary(tsls_X_s2)\n\n\nCall:\nlm(formula = log(packs) ~ log(real_inc) + logP_X_pred, data = cig)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67048 -0.13306  0.00598  0.13361  0.58044 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     9.4307     1.6247   5.805 6.09e-07 ***\nlog(real_inc)   0.2145     0.3212   0.668   0.5077    \nlogP_X_pred    -1.1434     0.4300  -2.659   0.0108 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2267 on 45 degrees of freedom\nMultiple R-squared:  0.1687,    Adjusted R-squared:  0.1318 \nF-statistic: 4.567 on 2 and 45 DF,  p-value: 0.01564\n\n\nThe AER package has the function ivreg, which is a one-stop function for the above protocol. The first equation is our desired equation. After the midline (\\(\\vert\\)), put the right-hand side of the first stage. You can see that the output is what we have above from our more manual TSLS approach.\n\nivreg(log(packs) ~ log(P) + log(real_inc) | log(real_inc) + S,\n      data = cig) %&gt;%\n  summary()\n\n\nCall:\nivreg(formula = log(packs) ~ log(P) + log(real_inc) | log(real_inc) + \n    S, data = cig)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.611000 -0.086072  0.009423  0.106912  0.393159 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     9.4307     1.3584   6.943 1.24e-08 ***\nlog(P)         -1.1434     0.3595  -3.181  0.00266 ** \nlog(real_inc)   0.2145     0.2686   0.799  0.42867    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1896 on 45 degrees of freedom\nMultiple R-Squared: 0.4189, Adjusted R-squared: 0.3931 \nWald test: 6.534 on 2 and 45 DF,  p-value: 0.003227",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "17_Instrumental_Variables.html#further-reading",
    "href": "17_Instrumental_Variables.html#further-reading",
    "title": "17  Instrumental Variables",
    "section": "17.4 Further Reading",
    "text": "17.4 Further Reading\nSee Stock and Watson (2020) for more details on IV regression. I adapted parts of chapter 12 from Hanck et al. (2018) for these notes. The example is from Stock and Watson (2020).\n\n17.4.1 References\n\n\n\n\n\nHanck, Cristoph, Martin Arnold, Alexander Gerber, and Martin Schmelzer. 2018. Introduction to Econometrics with R. https://bookdown.org/machar1991/ITER/.\n\n\nStock, James H., and Mark W. Watson. 2020. Introduction to Econometrics. 4th ed. New York, NY: Pearson. https://www.pearson.com/en-us/pearsonplus/p/9780136879787?srsltid=AfmBOoq1UPESY9Ez-JJYsWKIOM907u7A75_4qkiJiqT6ZPyAdlBH_5b4.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "18_Discontinuity_Designs.html",
    "href": "18_Discontinuity_Designs.html",
    "title": "18  Discontinuity Designs",
    "section": "",
    "text": "18.1 Defining a Discontinuity\nRandomized controlled trials are considered the “gold standard.” This means that when possible, a randomized controlled trial can produce plausible and useful causal estimates for the question at hand. However, randomization is not always possible, desirable, or ethical. There are some situations where there is “quasi-randomness.” This means that there is an argument that something about the intervention occurred as if it were random. Discontinuity designs are one example of a quasi-random design.\nSuppose treatment variable is \\(D_i\\). If \\(D_i = 1\\), then unit \\(i\\) is treated. If \\(D_i = 0\\), then unit \\(i\\) is in the control group. However, in this case, treatment was not randomly assigned. Instead, there is an underlying, continuous variable, \\(W_i\\), that determines \\(D_i\\):\n\\[\\begin{equation}\nD_i = \\begin{cases} 1 & W_i \\geq c \\\\ 0 W_i &lt; c. \\end{cases}\n\\end{equation}\\]\nGiven some threshold \\(c\\), any value of \\(W_i\\) above the threshold implies treatment status and any value below implies control status. This setup allows us to use a regression discontinuity design.\nThe general idea is that units just below and just above the cutoff are similar, both in observed and unobserved ways. The average treatment effect for units for which \\(W_i = c\\) will provide a good approximation to the treatment effect.\nThe regression model is:\n\\[\nY_i = \\beta_0 + \\beta_1 D_i + \\beta_2 W_i + U_i.\n\\tag{18.1}\\]\nThis is sharp RDD because it is deterministic. As long as we know \\(W_i\\) we know with certainty what \\(D_i\\) will be.\nThe identifying assumption is that units cannot alter \\(W_i\\) with precision around the cut-off. For example, suppose \\(W_i\\) is a test score and the cutoff \\(c\\) determines if a student enters a gifted program. If students can study before the test, take the test multiple times, or appeal to slightly change their score, then the randomness is not as plausible. Students who partake in these “manipulations” may be unobservably different than students who do not.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discontinuity Designs</span>"
    ]
  },
  {
    "objectID": "18_Discontinuity_Designs.html#illustration-with-simulated-data",
    "href": "18_Discontinuity_Designs.html#illustration-with-simulated-data",
    "title": "18  Discontinuity Designs",
    "section": "18.2 Illustration with Simulated Data",
    "text": "18.2 Illustration with Simulated Data\nSimulate and visualize the data. There is a cutoff at \\(W = 0\\).\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n# Set seed\nset.seed(470500)\n\n# Sample data\nW &lt;-runif(1000, -1, 1)\nY &lt;- 3 + 2 * W + 10 * (W &gt;= 0) + rnorm(1000)\n\n# Construct the data\ndf &lt;- data.frame(Y = Y, W = W) %&gt;%\n  as_tibble()\n\ndf &lt;- df %&gt;%\n  mutate(D = ifelse(W &gt;= 0, 1, 0))\n\n# Plot the data\nggplot(data = df) +\n  geom_point(aes(x = W, y = Y, color = as.factor(D))) +\n  geom_vline(xintercept = 0) +\n  labs(color = \"Treatment Status\")\n\n\n\n\n\n\n\n\nThe simplest way to estimate the regression is taking Equation Equation 18.1 at face value and estimating that. We are interested in the coefficient on \\(D\\). That is, given a value of \\(W\\), the effect of treatment is 10.137 units.\n\\[\nY = \\alpha + \\tau D + \\epsilon\n\\]\n\nrdd &lt;- lm(formula = Y ~ D + W,\n   data = df)\n\nsummary(rdd)\n\n\nCall:\nlm(formula = Y ~ D + W, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2590 -0.6539 -0.0217  0.6412  2.9480 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.93787    0.07114   41.30   &lt;2e-16 ***\nD           10.13690    0.12467   81.31   &lt;2e-16 ***\nW            1.82822    0.10773   16.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9982 on 997 degrees of freedom\nMultiple R-squared:  0.9732,    Adjusted R-squared:  0.9731 \nF-statistic: 1.807e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nThis model linearly extrapolates across the threshold. It imposes that both the treatment and control group have the same slope (the coefficient on \\(W\\)). Notice that the lines below are parallel so we can believe it, but it is not always the case. In those settings, researchers use other models.\n\nggplot(data = df) +\n  geom_point(aes(x = W, y = Y, color = as.factor(D))) +\n  geom_vline(xintercept = 0) +\n  geom_smooth(aes(x = W, y = Y, group = D), method = \"lm\", se = FALSE) +\n  labs(color = \"Treatment Status\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere is an example with more complex data.\n\n# Set seed\nset.seed(470500)\n\n# Sample data\nW &lt;-runif(1000, -1, 1)\nY &lt;- 3 + 2 * W + 15 * (W &gt;= 0) + 3 * W^2 + rnorm(1000)\n\n# Construct the data\ndf &lt;- data.frame(Y = Y, W = W) %&gt;%\n  as_tibble()\n\ndf &lt;- df %&gt;%\n  mutate(D = ifelse(W &gt;= 0, 1, 0))\n\n# Plot the data\nggplot(data = df) +\n  geom_point(aes(x = W, y = Y, color = as.factor(D))) +\n  geom_vline(xintercept = 0)  +\n  geom_smooth(aes(x = W, y = Y, group = D), method = \"lm\", se = FALSE) +\n  labs(color = \"Treatment Status\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere it is clear there are different slopes above and below the threshold. We can interact the running variable (\\(W\\)) with the treatment variable (\\(D\\)).\n\nlm(formula = Y ~ D + W:D,\n   data = df) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ D + W:D, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4561 -0.6953 -0.0607  0.6742  3.1522 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.04936    0.04739   64.35   &lt;2e-16 ***\nD           14.57101    0.10195  142.92   &lt;2e-16 ***\nD:W          4.73620    0.15891   29.80   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.043 on 997 degrees of freedom\nMultiple R-squared:  0.9852,    Adjusted R-squared:  0.9852 \nF-statistic: 3.321e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nOften, researchers add a quadratic term to relax the assumption that the fit is linear.\n\nlm(formula = Y ~ D + W:D + I(W^2) + I(W^2):D,\n   data = df) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ D + W:D + I(W^2) + I(W^2):D, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.13868 -0.67245 -0.02409  0.63367  2.96582 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.64185    0.06809  38.802  &lt; 2e-16 ***\nD           15.48166    0.14458 107.077  &lt; 2e-16 ***\nI(W^2)       1.17407    0.14629   8.025 2.84e-15 ***\nD:W          1.65227    0.59487   2.778  0.00558 ** \nD:I(W^2)     1.94160    0.59915   3.241  0.00123 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9979 on 995 degrees of freedom\nMultiple R-squared:  0.9865,    Adjusted R-squared:  0.9864 \nF-statistic: 1.815e+04 on 4 and 995 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discontinuity Designs</span>"
    ]
  },
  {
    "objectID": "18_Discontinuity_Designs.html#examples-of-rdd",
    "href": "18_Discontinuity_Designs.html#examples-of-rdd",
    "title": "18  Discontinuity Designs",
    "section": "18.3 Examples of RDD",
    "text": "18.3 Examples of RDD\nSee example papers.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discontinuity Designs</span>"
    ]
  },
  {
    "objectID": "18_Discontinuity_Designs.html#further-reading",
    "href": "18_Discontinuity_Designs.html#further-reading",
    "title": "18  Discontinuity Designs",
    "section": "18.4 Further Reading",
    "text": "18.4 Further Reading\n\n18.4.1 References",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discontinuity Designs</span>"
    ]
  },
  {
    "objectID": "19_Difference_in_Differences.html",
    "href": "19_Difference_in_Differences.html",
    "title": "19  Fixed Effects and Difference-in-Differences",
    "section": "",
    "text": "19.1 Panel Data\nBefore discussing difference-in-differences, it is important to understand the terminology for data that extends over time.\nLike before, we use \\(i\\) to index the units of the dataset (individuals, households, firms, counties, countries, etc.). Unlike before, we also have to index time periods, \\(t\\). Time periods can be anything: years, seconds, decades, etc. It is convenient to consider there are \\(N\\) individuals, so that \\(i = 1, 2, \\ldots, N\\), and \\(T\\) time periods, so that \\(t = 1, 2, \\ldots, T\\).\nThe advantage of panel data is that it allows us to consider a “before and after” something changes. We can then see the effect of a policy intervention by comparing what happens after the intervention to what happens before the intervention.\nWe can load an example panel dataset from the AER package.\nlibrary(AER)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(magrittr)\nlibrary(tidyr)\n\n\nAttaching package: 'tidyr'\n\n\nThe following object is masked from 'package:magrittr':\n\n    extract\n\ndata(Fatalities)\n\n# Explore the data (commented to be concise)\n# summary(Fatalities)\nThis dataset contains the number of deaths from traffic accidents in the continental U.S. annually from 1982 to 1988. Before we dive in, answer the following questions about the data.\nA policymaker might be interested in the relationship between traffic fatalities and taxes on alcohol. We can use this dataset to see the relationship. To begin, let us look at a naive correlation between the fatality rate and the tax on beer.\n# Calculate the fatality rate per 10,000 people\ndf &lt;- Fatalities %&gt;%\n  mutate(frate = fatal / pop * 10000)\n\nsummary(df$frate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.8212  1.6237  1.9560  2.0404  2.4179  4.2178 \n\n# Correlation in 1982\nlm(frate ~ beertax, \n   data = filter(df, year == 1982)) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = frate ~ beertax, data = filter(df, year == 1982))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9356 -0.4480 -0.1068  0.2295  2.1716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.0104     0.1391  14.455   &lt;2e-16 ***\nbeertax       0.1485     0.1884   0.788    0.435    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6705 on 46 degrees of freedom\nMultiple R-squared:  0.01332,   Adjusted R-squared:  -0.008126 \nF-statistic: 0.6212 on 1 and 46 DF,  p-value: 0.4347\n\n# Correlation in 1988\nlm(frate ~ beertax, \n   data = filter(df, year == 1988)) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = frate ~ beertax, data = filter(df, year == 1988))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.72931 -0.36028 -0.07132  0.39938  1.35783 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.8591     0.1060  17.540   &lt;2e-16 ***\nbeertax       0.4388     0.1645   2.668   0.0105 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4903 on 46 degrees of freedom\nMultiple R-squared:  0.134, Adjusted R-squared:  0.1152 \nF-statistic: 7.118 on 1 and 46 DF,  p-value: 0.0105\nOur results are counter-intuitive. It seems that increasing taxes actually increases fatality rate. While this may be the case, we cannot conclude anything causal due to omitted variable biases. We have some covariates in this dataset that my help us control for economic conditions, demographic factors, etc., but we may always be concerned about unobservable factors that determine traffic fatalities, some of which may be related to alcohol taxes. Fixed effects can help us!",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fixed Effects and Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "19_Difference_in_Differences.html#panel-data",
    "href": "19_Difference_in_Differences.html#panel-data",
    "title": "19  Fixed Effects and Difference-in-Differences",
    "section": "",
    "text": "Cross-sectional data. In this type of data, there is one observation per unit. This is the type of data we have been considering in the class so far.\nPanel data. In this type of data, there are multiple observations per unit corresponding to multiple time periods. This is the type of data we will consider for difference-in-differences. Another name for panel data is longitudinal data.\n\n\n\n\n\n\n\nWhat is the unit of analysis in the data?\nWhat is the unit of time period in the data?\nHow many observations and variables are there?\nIs the dataset in long or wide format?",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fixed Effects and Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "19_Difference_in_Differences.html#fixed-effects",
    "href": "19_Difference_in_Differences.html#fixed-effects",
    "title": "19  Fixed Effects and Difference-in-Differences",
    "section": "19.2 Fixed Effects",
    "text": "19.2 Fixed Effects\nWe can consider all the observable and unobservable factors of unit that do not vary over time using unit fixed effects. These are just indicator variables for each of the states.\nIf \\(Y_{it}\\) is the fatality rate for state \\(i\\) in year \\(t\\) and \\(X_{it}\\) is the tax for beer, then we can think of the following regressions for 1982 and 1988 with fixed effects:\n\\[\\begin{align*}\nY_{i1982} = \\beta_0 + \\beta_1X_{i1982} + \\beta_2 Z_i + U_{i1982} \\\\\nY_{i1988} = \\beta_0 + \\beta_1X_{i1988} + \\beta_2 Z_i + U_{i1988}.\n\\end{align*}\\]\nNotice that because the fixed effects do not vary over time, if we take the difference of the two equations then the fixed effects will cancel out. This will help us capture the correlation between changes in taxes and changes in traffic fatalities:\n\\[\\begin{equation*}\nY_{i1988} - Y_{i1982} = \\beta_1(X_{i1988} - X_{i1982}) + (U_{i1988} - U_{i1982})\n\\end{equation*}\\]\nWe can implement this in the data. We allow for an intercept which is the mean change in fatality rate if there were no change in the tax on beer.\n\n# Reshape data from long to wide\ndf_wide &lt;- df %&gt;%\n  pivot_wider(id_cols = state,\n              names_from = year,\n              values_from = c(frate, beertax))\n\nhead(df_wide)\n\n# A tibble: 6 × 15\n  state frate_1982 frate_1983 frate_1984 frate_1985 frate_1986 frate_1987\n  &lt;fct&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 al          2.13       2.35       2.34       2.19       2.67       2.72\n2 az          2.50       2.27       2.83       2.80       3.07       2.77\n3 ar          2.38       2.40       2.24       2.26       2.54       2.68\n4 ca          1.86       1.81       1.95       1.88       1.95       1.99\n5 co          2.17       2.05       1.91       1.79       1.85       1.79\n6 ct          1.65       1.39       1.49       1.41       1.41       1.40\n# ℹ 8 more variables: frate_1988 &lt;dbl&gt;, beertax_1982 &lt;dbl&gt;, beertax_1983 &lt;dbl&gt;,\n#   beertax_1984 &lt;dbl&gt;, beertax_1985 &lt;dbl&gt;, beertax_1986 &lt;dbl&gt;,\n#   beertax_1987 &lt;dbl&gt;, beertax_1988 &lt;dbl&gt;\n\n# Calculate differences\ndf_wide &lt;- df_wide %&gt;%\n  mutate(frate_diff = frate_1988 - frate_1982,\n         beertax_diff = beertax_1988 - beertax_1982)\n\n# Regression\nlm(frate_diff ~ beertax_diff, data = df_wide) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = frate_diff ~ beertax_diff, data = df_wide)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22715 -0.09619  0.09212  0.22290  0.67745 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -0.07204    0.06064  -1.188   0.2410  \nbeertax_diff -1.04097    0.41723  -2.495   0.0162 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.394 on 46 degrees of freedom\nMultiple R-squared:  0.1192,    Adjusted R-squared:    0.1 \nF-statistic: 6.225 on 1 and 46 DF,  p-value: 0.01625\n\n\nNow we see that when we increase the beer tax, that is correlated with a change in fatality rates. Increasing the tax on a case of beer by one dollar results in 1.04 deaths per 10,000 people on average. We can evaluate the magnitude of this by looking at the summary statistics of the outcome variable and considering the data. This effect seems pretty large. Even with taking the differences to remove the time-invariant, state-level factors, there could be other omitted variables that make the change in beer tax endogenous.\n\nggplot(data = df_wide) +\n  geom_point(aes(x = beertax_diff, y = frate_diff)) +\n  geom_smooth(aes(x = beertax_diff, y = frate_diff), \n              method = \"lm\", se = FALSE) +\n  labs(x = \"Change in Beer Tax (1988 - 1982)\",\n       y = \"Change in Fatality Rate (per 10,000, 1988 - 1982)\") +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsummary(df_wide[c(\"frate_1982\", \"frate_1988\", \"frate_diff\")])\n\n   frate_1982      frate_1988      frate_diff      \n Min.   :1.101   Min.   :1.231   Min.   :-1.30657  \n 1st Qu.:1.618   1st Qu.:1.628   1st Qu.:-0.13319  \n Median :2.046   Median :1.998   Median : 0.04075  \n Mean   :2.089   Mean   :2.070   Mean   :-0.01951  \n 3rd Qu.:2.317   3rd Qu.:2.468   3rd Qu.: 0.23626  \n Max.   :4.218   Max.   :3.236   Max.   : 0.71697  \n\n\nWe can also consider regressions with the fixed effects without taking the differences between time periods. We call these panel regression models:\n\\[\\begin{equation*}\nY_{it} = \\beta_0 + \\beta_1 X_{it} + \\beta_2 Z_i + U_{it}.\n\\end{equation*}\\]\nNotice that we have the state fixed effects (\\(Z_i\\)). You can imagine this regression for each \\(t\\) that we have in the data. The basic idea here is that each state has its own intercept (\\(\\beta_2 Z_i\\)) in addition to the common intercept (\\(\\beta_0\\)). The notation above is a bit of a shorthand for including indicators for each of the units except (arbitrarily) the first one:\n\\[\\begin{equation*}\nY_{it} = \\beta_0 + \\beta_1 X_{it} + \\alpha_2 D2_i + \\alpha_3 D3_i + \\ldots + \\alpha_N DN_i + U_{it}.\n\\end{equation*}\\]\nIf we remove the common intercept, then we can also include the intercept for the first unit.\nLet us estimate the model with traffic fatalities and the beer tax. Notice that we want to use the long data. It is useful to compare with and without intercept. We see the coefficients of the fixed effects, but we are only interested in the estimated coefficient on the beer tax: \\(-0.657\\).\n\n# With Intercept\nlm(frate ~ beertax + state, data = df) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = frate ~ beertax + state, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58696 -0.08284 -0.00127  0.07955  0.89780 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.47763    0.31336  11.098  &lt; 2e-16 ***\nbeertax     -0.65587    0.18785  -3.491 0.000556 ***\nstateaz     -0.56773    0.26667  -2.129 0.034107 *  \nstatear     -0.65495    0.21902  -2.990 0.003028 ** \nstateca     -1.50947    0.30435  -4.960 1.21e-06 ***\nstateco     -1.48428    0.28735  -5.165 4.50e-07 ***\nstatect     -1.86226    0.28053  -6.638 1.58e-10 ***\nstatede     -1.30760    0.29395  -4.448 1.24e-05 ***\nstatefl     -0.26813    0.13933  -1.924 0.055284 .  \nstatega      0.52460    0.18395   2.852 0.004661 ** \nstateid     -0.66902    0.25797  -2.593 0.009989 ** \nstateil     -1.96162    0.29150  -6.730 9.23e-11 ***\nstatein     -1.46154    0.27254  -5.363 1.69e-07 ***\nstateia     -1.54393    0.25344  -6.092 3.58e-09 ***\nstateks     -1.22322    0.24544  -4.984 1.08e-06 ***\nstateky     -1.21752    0.28717  -4.240 3.02e-05 ***\nstatela     -0.84712    0.18867  -4.490 1.03e-05 ***\nstateme     -1.10795    0.19112  -5.797 1.78e-08 ***\nstatemd     -1.70644    0.28322  -6.025 5.17e-09 ***\nstatema     -2.10975    0.27610  -7.641 3.24e-13 ***\nstatemi     -1.48453    0.23602  -6.290 1.18e-09 ***\nstatemn     -1.89721    0.26509  -7.157 6.92e-12 ***\nstatems     -0.02908    0.14845  -0.196 0.844839    \nstatemo     -1.29626    0.26669  -4.861 1.93e-06 ***\nstatemt     -0.36039    0.26396  -1.365 0.173225    \nstatene     -1.52218    0.24928  -6.106 3.30e-09 ***\nstatenv     -0.60077    0.28595  -2.101 0.036517 *  \nstatenh     -1.25445    0.20968  -5.983 6.53e-09 ***\nstatenj     -2.10575    0.30720  -6.855 4.37e-11 ***\nstatenm      0.42637    0.25432   1.677 0.094724 .  \nstateny     -2.18667    0.29890  -7.316 2.57e-12 ***\nstatenc     -0.29047    0.11984  -2.424 0.015979 *  \nstatend     -1.62344    0.25381  -6.396 6.45e-10 ***\nstateoh     -1.67442    0.25381  -6.597 2.02e-10 ***\nstateok     -0.54506    0.16912  -3.223 0.001415 ** \nstateor     -1.16800    0.28572  -4.088 5.65e-05 ***\nstatepa     -1.76747    0.27610  -6.402 6.26e-10 ***\nstateri     -2.26505    0.29376  -7.711 2.07e-13 ***\nstatesc      0.55717    0.11000   5.065 7.30e-07 ***\nstatesd     -1.00372    0.20962  -4.788 2.70e-06 ***\nstatetn     -0.87566    0.26802  -3.267 0.001218 ** \nstatetx     -0.91747    0.24556  -3.736 0.000225 ***\nstateut     -1.16395    0.19642  -5.926 8.88e-09 ***\nstatevt     -0.96604    0.21113  -4.576 7.08e-06 ***\nstateva     -1.29018    0.20416  -6.320 9.99e-10 ***\nstatewa     -1.65952    0.28346  -5.854 1.31e-08 ***\nstatewv     -0.89675    0.24661  -3.636 0.000328 ***\nstatewi     -1.75927    0.29395  -5.985 6.44e-09 ***\nstatewy     -0.22850    0.31290  -0.730 0.465811    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1899 on 287 degrees of freedom\nMultiple R-squared:  0.905, Adjusted R-squared:  0.8891 \nF-statistic: 56.97 on 48 and 287 DF,  p-value: &lt; 2.2e-16\n\n# Without Intercept\nlm(frate ~ beertax + state - 1, data = df) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = frate ~ beertax + state - 1, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58696 -0.08284 -0.00127  0.07955  0.89780 \n\nCoefficients:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nbeertax -0.65587    0.18785  -3.491 0.000556 ***\nstateal  3.47763    0.31336  11.098  &lt; 2e-16 ***\nstateaz  2.90990    0.09254  31.445  &lt; 2e-16 ***\nstatear  2.82268    0.13213  21.364  &lt; 2e-16 ***\nstateca  1.96816    0.07401  26.594  &lt; 2e-16 ***\nstateco  1.99335    0.08037  24.802  &lt; 2e-16 ***\nstatect  1.61537    0.08391  19.251  &lt; 2e-16 ***\nstatede  2.17003    0.07746  28.016  &lt; 2e-16 ***\nstatefl  3.20950    0.22151  14.489  &lt; 2e-16 ***\nstatega  4.00223    0.46403   8.625 4.43e-16 ***\nstateid  2.80861    0.09877  28.437  &lt; 2e-16 ***\nstateil  1.51601    0.07848  19.318  &lt; 2e-16 ***\nstatein  2.01609    0.08867  22.736  &lt; 2e-16 ***\nstateia  1.93370    0.10222  18.918  &lt; 2e-16 ***\nstateks  2.25441    0.10863  20.753  &lt; 2e-16 ***\nstateky  2.26011    0.08046  28.089  &lt; 2e-16 ***\nstatela  2.63051    0.16266  16.171  &lt; 2e-16 ***\nstateme  2.36968    0.16006  14.805  &lt; 2e-16 ***\nstatemd  1.77119    0.08246  21.480  &lt; 2e-16 ***\nstatema  1.36788    0.08648  15.818  &lt; 2e-16 ***\nstatemi  1.99310    0.11663  17.089  &lt; 2e-16 ***\nstatemn  1.58042    0.09363  16.880  &lt; 2e-16 ***\nstatems  3.44855    0.20936  16.472  &lt; 2e-16 ***\nstatemo  2.18137    0.09252  23.576  &lt; 2e-16 ***\nstatemt  3.11724    0.09441  33.017  &lt; 2e-16 ***\nstatene  1.95545    0.10551  18.534  &lt; 2e-16 ***\nstatenv  2.87686    0.08106  35.492  &lt; 2e-16 ***\nstatenh  2.22318    0.14114  15.751  &lt; 2e-16 ***\nstatenj  1.37188    0.07333  18.709  &lt; 2e-16 ***\nstatenm  3.90401    0.10154  38.449  &lt; 2e-16 ***\nstateny  1.29096    0.07563  17.070  &lt; 2e-16 ***\nstatenc  3.18717    0.25173  12.661  &lt; 2e-16 ***\nstatend  1.85419    0.10193  18.191  &lt; 2e-16 ***\nstateoh  1.80321    0.10193  17.691  &lt; 2e-16 ***\nstateok  2.93257    0.18428  15.913  &lt; 2e-16 ***\nstateor  2.30963    0.08117  28.453  &lt; 2e-16 ***\nstatepa  1.71016    0.08648  19.776  &lt; 2e-16 ***\nstateri  1.21258    0.07753  15.640  &lt; 2e-16 ***\nstatesc  4.03480    0.35479  11.372  &lt; 2e-16 ***\nstatesd  2.47391    0.14121  17.519  &lt; 2e-16 ***\nstatetn  2.60197    0.09162  28.398  &lt; 2e-16 ***\nstatetx  2.56016    0.10853  23.589  &lt; 2e-16 ***\nstateut  2.31368    0.15453  14.972  &lt; 2e-16 ***\nstatevt  2.51159    0.13973  17.975  &lt; 2e-16 ***\nstateva  2.18745    0.14664  14.917  &lt; 2e-16 ***\nstatewa  1.81811    0.08233  22.084  &lt; 2e-16 ***\nstatewv  2.58088    0.10767  23.971  &lt; 2e-16 ***\nstatewi  1.71836    0.07746  22.185  &lt; 2e-16 ***\nstatewy  3.24913    0.07233  44.922  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1899 on 287 degrees of freedom\nMultiple R-squared:  0.9931,    Adjusted R-squared:  0.992 \nF-statistic: 847.8 on 49 and 287 DF,  p-value: &lt; 2.2e-16\n\n\nWhile we can always use lm(), we can also use a package called plm to more conveniently estimate fixed effect regressions. The model is called “within” because we want there to be unit fixed effects. We get the same result but it does not show all the estimated coefficients for the fixed effects.\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nplm(frate ~ beertax, \n    data = df,\n    index = c(\"state\", \"year\"),\n    model = \"within\") %&gt;%\n  summary()\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = frate ~ beertax, data = df, model = \"within\", index = c(\"state\", \n    \"year\"))\n\nBalanced Panel: n = 48, T = 7, N = 336\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.5869619 -0.0828376 -0.0012701  0.0795454  0.8977960 \n\nCoefficients:\n        Estimate Std. Error t-value Pr(&gt;|t|)    \nbeertax -0.65587    0.18785 -3.4915 0.000556 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    10.785\nResidual Sum of Squares: 10.345\nR-Squared:      0.040745\nAdj. R-Squared: -0.11969\nF-statistic: 12.1904 on 1 and 287 DF, p-value: 0.00055597\n\n\nWe still may not be satisfied with these estimates! What if there are factors that are changing over time, not just over states? We can include time fixed effects for this. I will use \\(\\delta_t\\) to stand in for all the indicators:\n\\[\\begin{equation*}\nY_{it} = \\beta_0 + \\beta_1 X_{it} + \\delta_t + U_{it}.\n\\end{equation*}\\]\nWe can run the below regressions because both state and year are factor variables. If they are not factor variables, then we need to convert them before running the regression.\n\n# State fixed effects\nlm(frate ~ beertax + state - 1, data = df) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = frate ~ beertax + state - 1, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58696 -0.08284 -0.00127  0.07955  0.89780 \n\nCoefficients:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nbeertax -0.65587    0.18785  -3.491 0.000556 ***\nstateal  3.47763    0.31336  11.098  &lt; 2e-16 ***\nstateaz  2.90990    0.09254  31.445  &lt; 2e-16 ***\nstatear  2.82268    0.13213  21.364  &lt; 2e-16 ***\nstateca  1.96816    0.07401  26.594  &lt; 2e-16 ***\nstateco  1.99335    0.08037  24.802  &lt; 2e-16 ***\nstatect  1.61537    0.08391  19.251  &lt; 2e-16 ***\nstatede  2.17003    0.07746  28.016  &lt; 2e-16 ***\nstatefl  3.20950    0.22151  14.489  &lt; 2e-16 ***\nstatega  4.00223    0.46403   8.625 4.43e-16 ***\nstateid  2.80861    0.09877  28.437  &lt; 2e-16 ***\nstateil  1.51601    0.07848  19.318  &lt; 2e-16 ***\nstatein  2.01609    0.08867  22.736  &lt; 2e-16 ***\nstateia  1.93370    0.10222  18.918  &lt; 2e-16 ***\nstateks  2.25441    0.10863  20.753  &lt; 2e-16 ***\nstateky  2.26011    0.08046  28.089  &lt; 2e-16 ***\nstatela  2.63051    0.16266  16.171  &lt; 2e-16 ***\nstateme  2.36968    0.16006  14.805  &lt; 2e-16 ***\nstatemd  1.77119    0.08246  21.480  &lt; 2e-16 ***\nstatema  1.36788    0.08648  15.818  &lt; 2e-16 ***\nstatemi  1.99310    0.11663  17.089  &lt; 2e-16 ***\nstatemn  1.58042    0.09363  16.880  &lt; 2e-16 ***\nstatems  3.44855    0.20936  16.472  &lt; 2e-16 ***\nstatemo  2.18137    0.09252  23.576  &lt; 2e-16 ***\nstatemt  3.11724    0.09441  33.017  &lt; 2e-16 ***\nstatene  1.95545    0.10551  18.534  &lt; 2e-16 ***\nstatenv  2.87686    0.08106  35.492  &lt; 2e-16 ***\nstatenh  2.22318    0.14114  15.751  &lt; 2e-16 ***\nstatenj  1.37188    0.07333  18.709  &lt; 2e-16 ***\nstatenm  3.90401    0.10154  38.449  &lt; 2e-16 ***\nstateny  1.29096    0.07563  17.070  &lt; 2e-16 ***\nstatenc  3.18717    0.25173  12.661  &lt; 2e-16 ***\nstatend  1.85419    0.10193  18.191  &lt; 2e-16 ***\nstateoh  1.80321    0.10193  17.691  &lt; 2e-16 ***\nstateok  2.93257    0.18428  15.913  &lt; 2e-16 ***\nstateor  2.30963    0.08117  28.453  &lt; 2e-16 ***\nstatepa  1.71016    0.08648  19.776  &lt; 2e-16 ***\nstateri  1.21258    0.07753  15.640  &lt; 2e-16 ***\nstatesc  4.03480    0.35479  11.372  &lt; 2e-16 ***\nstatesd  2.47391    0.14121  17.519  &lt; 2e-16 ***\nstatetn  2.60197    0.09162  28.398  &lt; 2e-16 ***\nstatetx  2.56016    0.10853  23.589  &lt; 2e-16 ***\nstateut  2.31368    0.15453  14.972  &lt; 2e-16 ***\nstatevt  2.51159    0.13973  17.975  &lt; 2e-16 ***\nstateva  2.18745    0.14664  14.917  &lt; 2e-16 ***\nstatewa  1.81811    0.08233  22.084  &lt; 2e-16 ***\nstatewv  2.58088    0.10767  23.971  &lt; 2e-16 ***\nstatewi  1.71836    0.07746  22.185  &lt; 2e-16 ***\nstatewy  3.24913    0.07233  44.922  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1899 on 287 degrees of freedom\nMultiple R-squared:  0.9931,    Adjusted R-squared:  0.992 \nF-statistic: 847.8 on 49 and 287 DF,  p-value: &lt; 2.2e-16\n\n# Time fixed effects\nlm(frate ~ beertax + year - 1, data = df) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = frate ~ beertax + year - 1, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.06068 -0.38427 -0.09853  0.26781  2.23447 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)    \nbeertax   0.36634    0.06260   5.852 1.18e-08 ***\nyear1982  1.89485    0.08566  22.121  &lt; 2e-16 ***\nyear1983  1.81281    0.08571  21.151  &lt; 2e-16 ***\nyear1984  1.82311    0.08564  21.288  &lt; 2e-16 ***\nyear1985  1.78430    0.08534  20.909  &lt; 2e-16 ***\nyear1986  1.87873    0.08514  22.065  &lt; 2e-16 ***\nyear1987  1.87931    0.08483  22.154  &lt; 2e-16 ***\nyear1988  1.89382    0.08448  22.416  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5471 on 328 degrees of freedom\nMultiple R-squared:  0.9349,    Adjusted R-squared:  0.9333 \nF-statistic: 588.7 on 8 and 328 DF,  p-value: &lt; 2.2e-16\n\n# State fixed effects and time fixed effects\nlm(frate ~ beertax + state + year - 1, data = df) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = frate ~ beertax + state + year - 1, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59556 -0.08096  0.00143  0.08234  0.83883 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)    \nbeertax  -0.63998    0.19738  -3.242  0.00133 ** \nstateal   3.51137    0.33250  10.560  &lt; 2e-16 ***\nstateaz   2.96451    0.09933  29.846  &lt; 2e-16 ***\nstatear   2.87284    0.14162  20.286  &lt; 2e-16 ***\nstateca   2.02618    0.07857  25.787  &lt; 2e-16 ***\nstateco   2.04984    0.08594  23.851  &lt; 2e-16 ***\nstatect   1.67125    0.08989  18.592  &lt; 2e-16 ***\nstatede   2.22711    0.08264  26.951  &lt; 2e-16 ***\nstatefl   3.25132    0.23590  13.782  &lt; 2e-16 ***\nstatega   4.02300    0.49087   8.196 8.92e-15 ***\nstateid   2.86242    0.10606  26.990  &lt; 2e-16 ***\nstateil   1.57287    0.08380  18.769  &lt; 2e-16 ***\nstatein   2.07123    0.09512  21.775  &lt; 2e-16 ***\nstateia   1.98709    0.10976  18.103  &lt; 2e-16 ***\nstateks   2.30707    0.11663  19.781  &lt; 2e-16 ***\nstateky   2.31659    0.08604  26.923  &lt; 2e-16 ***\nstatela   2.67772    0.17390  15.398  &lt; 2e-16 ***\nstateme   2.41713    0.17116  14.122  &lt; 2e-16 ***\nstatemd   1.82731    0.08828  20.700  &lt; 2e-16 ***\nstatema   1.42335    0.09272  15.352  &lt; 2e-16 ***\nstatemi   2.04488    0.12516  16.338  &lt; 2e-16 ***\nstatemn   1.63488    0.10051  16.266  &lt; 2e-16 ***\nstatems   3.49146    0.22311  15.649  &lt; 2e-16 ***\nstatemo   2.23598    0.09931  22.515  &lt; 2e-16 ***\nstatemt   3.17160    0.10136  31.291  &lt; 2e-16 ***\nstatene   2.00846    0.11329  17.729  &lt; 2e-16 ***\nstatenv   2.93322    0.08671  33.827  &lt; 2e-16 ***\nstatenh   2.27245    0.15116  15.033  &lt; 2e-16 ***\nstatenj   1.43016    0.07773  18.399  &lt; 2e-16 ***\nstatenm   3.95748    0.10903  36.296  &lt; 2e-16 ***\nstateny   1.34849    0.08051  16.748  &lt; 2e-16 ***\nstatenc   3.22630    0.26770  12.052  &lt; 2e-16 ***\nstatend   1.90762    0.10945  17.428  &lt; 2e-16 ***\nstateoh   1.85664    0.10945  16.963  &lt; 2e-16 ***\nstateok   2.97776    0.19670  15.139  &lt; 2e-16 ***\nstateor   2.36597    0.08684  27.244  &lt; 2e-16 ***\nstatepa   1.76563    0.09272  19.044  &lt; 2e-16 ***\nstateri   1.26964    0.08272  15.348  &lt; 2e-16 ***\nstatesc   4.06496    0.37606  10.809  &lt; 2e-16 ***\nstatesd   2.52317    0.15123  16.684  &lt; 2e-16 ***\nstatetn   2.65670    0.09833  27.017  &lt; 2e-16 ***\nstatetx   2.61282    0.11653  22.423  &lt; 2e-16 ***\nstateut   2.36165    0.16532  14.286  &lt; 2e-16 ***\nstatevt   2.56100    0.14966  17.112  &lt; 2e-16 ***\nstateva   2.23618    0.15698  14.245  &lt; 2e-16 ***\nstatewa   1.87424    0.08813  21.266  &lt; 2e-16 ***\nstatewv   2.63364    0.11560  22.782  &lt; 2e-16 ***\nstatewi   1.77545    0.08264  21.485  &lt; 2e-16 ***\nstatewy   3.30791    0.07641  43.291  &lt; 2e-16 ***\nyear1983 -0.07990    0.03835  -2.083  0.03813 *  \nyear1984 -0.07242    0.03835  -1.888  0.06001 .  \nyear1985 -0.12398    0.03844  -3.225  0.00141 ** \nyear1986 -0.03786    0.03859  -0.981  0.32731    \nyear1987 -0.05090    0.03897  -1.306  0.19260    \nyear1988 -0.05180    0.03962  -1.307  0.19215    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1879 on 281 degrees of freedom\nMultiple R-squared:  0.9934,    Adjusted R-squared:  0.9921 \nF-statistic: 771.5 on 55 and 281 DF,  p-value: &lt; 2.2e-16\n\n\nFor these regressions, we need to be careful with the standard errors. Normally, we assume homoskedasticity, and for longitudinal data, we do not want there to be correlation between the unobserved variables. However, \\(U_{it}\\) will naturally be correlated with \\(U_{it+1}\\) because they are both for the same unit. Similarly, \\(U_{it}\\) will be correlated with \\(U_{jt}\\) because they are both for the same time period.\nWe can cluster the standard errors to allow for heteroskedasticity and autocorrelated errors within the cluster, but not across clusters. To do, we can use plm so that we have a plm object rather than an lm object. Then, we can use coeftest() with the sandwich package to calculate the clustered standard errors.\n\nlibrary(sandwich)\n\nplm_out &lt;- plm(frate ~ beertax,\n               data = df,\n               index = c(\"state\", \"year\"),\n               model = \"within\",\n               effect = \"twoways\") # This does state and year fixed effects\n\ncoeftest(plm_out, vcov = vcovHC, type = \"HC1\")\n\n\nt test of coefficients:\n\n        Estimate Std. Error t value Pr(&gt;|t|)  \nbeertax -0.63998    0.35015 -1.8277  0.06865 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fixed Effects and Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "19_Difference_in_Differences.html#difference-in-differences",
    "href": "19_Difference_in_Differences.html#difference-in-differences",
    "title": "19  Fixed Effects and Difference-in-Differences",
    "section": "19.3 Difference-in-Differences",
    "text": "19.3 Difference-in-Differences\nNow we are comfortable with panel data and fixed effects. In some cases, you may be comfortable assuming that the fixed effects account for all possible unobserved heterogeneity that could be correlated with the covariate of interest (e.g., beer taxes), and interpret the above estimates causally. This is hard to argue though, and there may be something better we can do: difference-in-differences.\nSuppose there is a large change in the taxes in Texas but a small change in the taxes in Oklahoma. If we compare Texas to Oklahoma in 1988, we will see the difference between the two states after the policy, but what if this is all due to differences between the states? If we compare Texas in 1982 to Texas in 1988, we may see a difference but what if this is all due to differences between the years? We can take the “difference-in-differences” to account for both possibilities and just see the change due to the policy change. Here are the means for each of the groups before (pre) and after (post) the policy:\n\\[\\begin{equation*}\n(Y_{\\text{Treatment, Post}} - Y_{\\text{Treatment, Pre}}) - (Y_{\\text{Control, Post}} - Y_{\\text{Control, Pre}})\n\\end{equation*}\\]\nHere is an illustration of the logic from Hanck et al. (2018).\n\n# Source:\n# https://bookdown.org/machar1991/ITER/13-4-quasi-experiments.html\nplot(c(0, 1), c(6, 8), \n     type = \"p\",\n     ylim = c(5, 12),\n     xlim = c(-0.3, 1.3),\n     main = \"The Differences-in-Differences Estimator\",\n     xlab = \"Period\",\n     ylab = \"Y\",\n     col = \"steelblue\",\n     pch = 20,\n     xaxt = \"n\",\n     yaxt = \"n\")\n\naxis(1, at = c(0, 1), labels = c(\"before\", \"after\"))\naxis(2, at = c(0, 13))\n\n# add treatment group\npoints(c(0, 1, 1), c(7, 9, 11), \n       col = \"darkred\",\n       pch = 20)\n\n# add line segments\nlines(c(0, 1), c(7, 11), col = \"darkred\")\nlines(c(0, 1), c(6, 8), col = \"steelblue\")\nlines(c(0, 1), c(7, 9), col = \"darkred\", lty = 2)\nlines(c(1, 1), c(9, 11), col = \"black\", lty = 2, lwd = 2)\n\n# add annotations\ntext(1, 10, expression(hat(beta)[1]^{DID}), cex = 0.8, pos = 4)\ntext(0, 5.5, \"s. mean control\", cex = 0.8 , pos = 4)\ntext(0, 6.8, \"s. mean treatment\", cex = 0.8 , pos = 4)\ntext(1, 7.9, \"s. mean control\", cex = 0.8 , pos = 4)\ntext(1, 11.1, \"s. mean treatment\", cex = 0.8 , pos = 4)\n\n\n\n\n\n\n\n\nLet us simulate some data to see how we estimate difference-in-differences.\n\n# Source:\n# https://bookdown.org/machar1991/ITER/13-4-quasi-experiments.html\n\nset.seed(470500)\n\n# Sample size\nN &lt;- 200\n\n# Treatment effect\nTEffect &lt;- 4\n\n# Generate treatment dummy\nTDummy &lt;- c(rep(0, N/2), rep(1, N/2))\n\n# Simulate pre- and post-treatment values of the dependent variable\ny_pre &lt;- 7 + rnorm(N)\ny_pre[1:N/2] &lt;- y_pre[1:N/2] - 1\ny_post &lt;- 7 + 2 + TEffect * TDummy + rnorm(N)\ny_post[1:N/2] &lt;- y_post[1:N/2] - 1 \n\nWe can compute the difference-in-difference estimator by hand.\n\nmean(y_post[TDummy == 1]) - mean(y_pre[TDummy == 1]) - \n(mean(y_post[TDummy == 0]) - mean(y_pre[TDummy == 0]))\n\n[1] 3.947658\n\n\nWe can use a linear model:\n\\[\\begin{equation*}\n\\Delta Y_i = \\beta_0 + \\beta_1 D_i + U_i.\n\\end{equation*}\\]\nNotice that we do not need to have everything in a dataset to use lm(). We can also rely on standalone vectors.\n\nlm(I(y_post - y_pre) ~ TDummy) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = I(y_post - y_pre) ~ TDummy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1025 -0.9670 -0.0932  0.9622  4.4390 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.0414     0.1466   13.92   &lt;2e-16 ***\nTDummy        3.9477     0.2074   19.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.466 on 198 degrees of freedom\nMultiple R-squared:  0.6467,    Adjusted R-squared:  0.6449 \nF-statistic: 362.4 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also use this equation:\n\\[\\begin{equation*}\nY_{it} = \\beta_0 + \\beta_1 D_i + \\beta_2 Post_t + \\beta (Post_t \\times D_i) + U_{it}\n\\end{equation*}\\]\n\ndf &lt;- tibble(Y = c(y_pre, y_post),\n             D = rep(TDummy, 2), \n             Post = c(rep(\"1\", N), rep(\"2\", N)))\n\nlm(Y ~ D * Post, data = df) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = Y ~ D * Post, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8788 -0.6071 -0.0357  0.6399  3.8692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.9209     0.1020  58.044   &lt;2e-16 ***\nD             1.2918     0.1443   8.955   &lt;2e-16 ***\nPost2         2.0414     0.1443  14.151   &lt;2e-16 ***\nD:Post2       3.9477     0.2040  19.350   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.02 on 396 degrees of freedom\nMultiple R-squared:  0.8816,    Adjusted R-squared:  0.8807 \nF-statistic: 982.9 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\nThe key assumption is commonly called “parallel trends.” This boils down to: in the absence of treatment, the trends of the treatment and the control group would be the same. We can see this most easily in the graph. This means that there cannot be unobserved factors that vary by time and by unit that are correlated with treatment.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fixed Effects and Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "19_Difference_in_Differences.html#further-reading",
    "href": "19_Difference_in_Differences.html#further-reading",
    "title": "19  Fixed Effects and Difference-in-Differences",
    "section": "19.4 Further Reading",
    "text": "19.4 Further Reading\nThese notes are based on chapters 10 and 13 of Hanck et al. (2018). See that source for more information.\n\n19.4.1 References\n\n\n\n\n\nHanck, Cristoph, Martin Arnold, Alexander Gerber, and Martin Schmelzer. 2018. Introduction to Econometrics with R. https://bookdown.org/machar1991/ITER/.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fixed Effects and Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "20_EventStudies.html",
    "href": "20_EventStudies.html",
    "title": "20  Event Studies",
    "section": "",
    "text": "20.1 Notation\nFor this chapter, download the following libraries.\nIn the examples of difference-in-differences that we saw before, we compared outcomes before and after a treatment for treatment and control groups. The treatment group all started experiencing the treatment at the same time. Event studies is the name for the approach when treated units receive treatment at different times.Event studies estimate dynamic treatment effects by comparing outcomes relative to the timing of the event.\nWe define event time as the number of periods before or after treatment:\n\\[\\begin{equation}\nY_{it} = \\sum_{k \\neq -1} \\beta_k \\cdot \\mathbb{1}\\{ \\text{event\\_time}_{it} = k \\} + \\alpha_i + \\gamma t + \\varepsilon_{it}\n\\end{equation}\\]\nEach \\(\\beta_k\\) tells us the average difference in the outcome \\(k\\) periods before or after treatment, relative to the omitted period (\\(k = -1\\)).",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "20_EventStudies.html#notation",
    "href": "20_EventStudies.html#notation",
    "title": "20  Event Studies",
    "section": "",
    "text": "The reference period is ( k = -1 ), the period just before treatment.\n\\(\\alpha_i\\): individual fixed effects\n\\(\\gamma_t\\): time fixed effects\n\n\n\n20.1.1 Event Study in Practice: Union Entry and Wages\nWe’ll use the wagepan dataset from the wooldridge package. This dataset tracks 545 men from 1980 to 1987, recording their wages and union status each year.\n\ndata(wagepan)\nhead(wagepan)\n\n  nr year agric black bus construc ent exper fin hisp poorhlth hours manuf\n1 13 1980     0     0   1        0   0     1   0    0        0  2672     0\n2 13 1981     0     0   0        0   0     2   0    0        0  2320     0\n3 13 1982     0     0   1        0   0     3   0    0        0  2940     0\n4 13 1983     0     0   1        0   0     4   0    0        0  2960     0\n5 13 1984     0     0   0        0   0     5   0    0        0  3071     0\n6 13 1985     0     0   1        0   0     6   0    0        0  2864     0\n  married min nrthcen nrtheast occ1 occ2 occ3 occ4 occ5 occ6 occ7 occ8 occ9 per\n1       0   0       0        1    0    0    0    0    0    0    0    0    1   0\n2       0   0       0        1    0    0    0    0    0    0    0    0    1   1\n3       0   0       0        1    0    0    0    0    0    0    0    0    1   0\n4       0   0       0        1    0    0    0    0    0    0    0    0    1   0\n5       0   0       0        1    0    0    0    0    1    0    0    0    0   1\n6       0   0       0        1    0    1    0    0    0    0    0    0    0   0\n  pro pub rur south educ tra trad union    lwage d81 d82 d83 d84 d85 d86 d87\n1   0   0   0     0   14   0    0     0 1.197540   0   0   0   0   0   0   0\n2   0   0   0     0   14   0    0     1 1.853060   1   0   0   0   0   0   0\n3   0   0   0     0   14   0    0     0 1.344462   0   1   0   0   0   0   0\n4   0   0   0     0   14   0    0     0 1.433213   0   0   1   0   0   0   0\n5   0   0   0     0   14   0    0     0 1.568125   0   0   0   1   0   0   0\n6   0   0   0     0   14   0    0     0 1.699891   0   0   0   0   1   0   0\n  expersq\n1       1\n2       4\n3       9\n4      16\n5      25\n6      36\n\n\nWe define the “event” as the first year a person joins a union.\n\n# Identify first year of union membership\nfirst_union &lt;- wagepan %&gt;%\n  group_by(nr) %&gt;%\n  filter(union == 1) %&gt;%\n  summarise(first_union_year = min(year), .groups = \"drop\")\n\n# Merge with main data and compute event time\nwagepan_evt &lt;- wagepan %&gt;%\n  left_join(first_union, by = \"nr\") %&gt;%\n  mutate(event_time = ifelse(!is.na(first_union_year), year - first_union_year, NA))\n\nWe’ll keep only individuals who ever joined a union to estimate dynamic effects.\n\nwage_evt &lt;- wagepan_evt %&gt;%\n  filter(!is.na(event_time))\n\n\n\n20.1.2 Estimate the Event Study Model\nWe’ll use fixest::feols() to estimate the event study, including individual and year fixed effects.\n\nevent_model &lt;- feols(lwage ~ i(event_time, ref = -1) | nr + year, data = wage_evt)\n\nThe variable 'event_time::7' has been removed because of collinearity (see $collin.var).\n\nsummary(event_model)\n\nOLS estimation, Dep. Var.: lwage\nObservations: 2,240\nFixed-effects: nr: 280,  year: 8\nStandard-errors: Clustered (nr) \n                Estimate Std. Error   t value Pr(&gt;|t|)    \nevent_time::-7  0.053626   0.181664  0.295191 0.768067    \nevent_time::-6  0.261184   0.119646  2.182970 0.029872 *  \nevent_time::-5  0.096592   0.099593  0.969867 0.332953    \nevent_time::-4 -0.100424   0.096855 -1.036849 0.300704    \nevent_time::-3  0.038853   0.066285  0.586150 0.558248    \nevent_time::-2 -0.070702   0.050560 -1.398389 0.163107    \nevent_time::0   0.099590   0.042999  2.316111 0.021277 *  \nevent_time::1   0.080911   0.040040  2.020729 0.044263 *  \nevent_time::2   0.054518   0.039461  1.381556 0.168214    \nevent_time::3   0.040313   0.039699  1.015461 0.310766    \nevent_time::4   0.032080   0.043211  0.742404 0.458467    \nevent_time::5   0.071639   0.036762  1.948740 0.052329 .  \nevent_time::6   0.071525   0.034204  2.091129 0.037421 *  \n... 1 variable was removed because of collinearity (event_time::7)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.32362     Adj. R2: 0.538958\n                Within R2: 0.020262\n\n\n\n\n20.1.3 Plot the Event Study\n\niplot(event_model, ci = TRUE, ref.line = 0, main = \"Event Study: Union Entry and Log Wages\")\n\n\n\n\n\n\n\n\n\n\n20.1.4 Interpreting the Plot\n\nPre-trends: Are the coefficients before treatment close to zero? This supports the parallel trends assumption.\nPost-treatment effects: Do wages rise or fall after joining a union?\nStatistical significance: Look at the confidence intervals.\n\nEvent studies are a powerful tool for visualizing causal effects over time—but only if the identifying assumptions hold!\n\n\n20.1.5 Resources\nI closely followed Tilburg Science Hub for this exercise, which is based on the original paper.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "20_EventStudies.html#activity-event-studies",
    "href": "20_EventStudies.html#activity-event-studies",
    "title": "20  Event Studies",
    "section": "20.2 Activity: Event Studies",
    "text": "20.2 Activity: Event Studies\n\nRead the first two pages of the paper “Punishment to promote prosocial behavior: a field experiment” posted to Canvas &gt; Modules &gt; Module 2 &gt; VS2024.pdf. Summarize in a few sentences the idea behind the experiment in that paper.\nGo to Canvas. Download waste.dta from Modules &gt; Module 2 &gt; Datasets. Save the dataset to a convenient folder and open it in your script.\nExplore the data. Get a sense for the units of analysis, the variables, whether the data are wide or long, and how many units are in the treatment and control groups. We are interested in the otucome residual_weight, which measures the amount of garbage. For a given route, the variable treatment “turns on” (that is, goes from 0 to 1) when the households on the route receive the announcement of the inspections.\nWe want to create a variable called weekstart that contains the value of calendar_week for which a particular route went from 0 to 1. Create this variable. This is tricky! Hint: Think about first constructing a variable that is like calendar_week but for all rows when treatment is 0, it takes a very large value. Then, create weekstart by taking the minimum of that variable.\nCreate a variable called eventtime that is centered (i.e., 0) around when treatment begins, by route. It should be 0 one period before treatment begins (the variable treatment goes from 0 to 1).\nGraph the variable eventtime. You can use the below code. Notice that we observe many observations between -20 and 20, but there are some observations observed beyond those periods. This can pose an issue because we want to have enough data at each of our event times to be able to estimate the regression well.\n\n\nggplot(data = waste) + \n  geom_bar(aes(x=eventtime)) + \n  labs(x = \"Event Time\", y = \"Count\") +\n  theme_bw()\n\n\nBin event time. You can use cut or ifelse. Create a bin for values less than or equal to -37. Create a bin for greater than or equal to 28. Everything else should remain as it is. That is, the values should be: -37, -36, -35, …, 26, 27, 28. Once you create the variable, turn it into a factor and use relevel to change the reference point to 0. We want to do this because we want to exclude 0 from the regression below. That is, we want all estimated values relative to the value at event time 0.\nYou should estimate this equation:\n\n\\[\\begin{equation*}\nY_{it} = \\beta_0 + \\sum_{\\tau = -37}^{-1} \\alpha_\\tau W_{i\\tau} + \\sum_{\\tau=1}^{28} \\alpha_\\tau W_{i\\tau} + \\lambda_i + \\mu_t + \\varepsilon_{it}.\n\\end{equation*}\\]\n\n\\(Y_{it}\\): this is the outcome variable and should be residual_weight\n\\(\\tau\\): event time, at \\(\\tau = 1\\), treatment begins\n\\(W_{i\\tau}\\): indicator that is 1 if route \\(i\\) has event time \\(\\tau\\)\n\\(\\lambda_i\\): unit fixed effects\n\\(\\mu_t\\): time fixed effects\n\nTo do this, use your usual lm(). The right hand side should include eventtime_bin, unit, and time fixed effects.\n\nLook at your estimates for eventtime_bin. What do you notice about the estimates as the event time goes from negative to positive?\nThis is going to be way more striking as a plot. Use the below code to extract the coefficients from the regression into a manageable dataset. There are complex data operations here that we have not learned about in class. If you are interested, try to figure out what each line does. Note that you need the tibble and stringr packages. The object regout is the object from summarizing the output of question 8. Using the critical value 1.96, create two variables in this dataset that take the lower and upper values of the 95% confidence interval.\n\n\nlibrary(tibble)\nlibrary(stringr)\n\nregout &lt;- output$coefficients[str_detect(rownames(output$coefficients), \"eventtime_bin\"),] %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(var = \"var\") %&gt;%\n  as_tibble() %&gt;%\n  mutate(eventtime = str_extract(var, \"-??[:digit:]{1,2}?$\"),\n         eventtime = as.numeric(eventtime))\n\nnames(regout) &lt;- c(\"varname\", \"estimate\", \"stderror\", \"tvalue\", \"pvalue\", \"eventtime\")\n\n\nCreate a plot with event time as the horizontal axis and the estimated coefficient as the vertical axis. There should be a point for every estimate at each event time. Use geom_errorbar() to add the 95% confidence intervalus. Note that in aesthetics you will need to specify x as event time, ymin (the minimum y value) and ymax (the maximum y value). You can go to the help for that function and scroll to the bottom to see examples.\nWhat is your takeaway about the effectiveness of this type of intervention to induce people to separate their trash and recycling?",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Event Studies</span>"
    ]
  },
  {
    "objectID": "21_MachineLearning.html",
    "href": "21_MachineLearning.html",
    "title": "21  Machine Learning Basics",
    "section": "",
    "text": "21.1 Supervised Learning\nMachine learning (sometimes abbreviated to ML) is a class of statistical tools that help analysts find patterns in data. Basically, ML helps with prediction problems. Many core methods in ML are modern, with new developments everyday, but there are also fundamental components that have been around for a long time and now can be applied thanks to the larger availability of large datasets and computing power. Academic researchers use ML but it is also commonly used in the public and private sectors, and is a core part of recent advances in AI tools. We will go through some ML basics and see how to implement them in R. You will have practice doing so with an activity.\nThere are two broad categories of ML.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Machine Learning Basics</span>"
    ]
  },
  {
    "objectID": "21_MachineLearning.html#supervised-learning",
    "href": "21_MachineLearning.html#supervised-learning",
    "title": "21  Machine Learning Basics",
    "section": "",
    "text": "21.1.1 Linear Regression\nYou already are familiar with a method that can be used for prediction. Before, we were using linear regression to understand the relationship between the outcome variable (\\(Y\\)) and covariates (\\(X_1, X_2, \\ldots\\)). Now, instead of focusing on interpreting the coefficients, we focus on minimizing prediction error. Specifically, linear regression minimizes the mean squared error (MSE) between the predicted and observed outcomes. If the fitted value is \\(\\hat{Y}_i\\), the MSE is\n\\[\\begin{equation*}\n\\text{MSE} = \\frac{1}{N} \\sum_{i =1}^N (Y_i - \\hat{Y}_i)^2.\n\\end{equation*}\\]\nConsider two datasets. One is the training dataset. We use this dataset to estimate the linear regression. From this dataset, we get the estimated coefficients. In the testing dataset we predict the outcome but using the estimated coefficients from the training dataset. Because we actually observe the outcome in tthe testing dataset as well, we can compute the squared prediction error. Suppose we have one dataset that we partition into training \\((i = 1, 2, \\ldots, N)\\) and testing \\((i = N+1, N+2, \\ldots, M)\\) datasets.\nLet us see how we implement this in R. We will use a dataset from the ggplot2 package called diamonds. This is a very clean dataset with the price and attribute of diamonds. We will try to predict the price of diamonds based on their attributes. You can think of this exercise extending to any situation where you want to predict prices.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(magrittr)\n\ndata(diamonds)\nsummary(diamonds)\n\n     carat               cut        color        clarity          depth      \n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n                                    J: 2808   (Other): 2531                  \n     table           price             x                y         \n Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n                                                                  \n       z         \n Min.   : 0.000  \n 1st Qu.: 2.910  \n Median : 3.530  \n Mean   : 3.539  \n 3rd Qu.: 4.040  \n Max.   :31.800  \n                 \n\n# Make the factors unordered (makes the regression easier)\ndiamonds &lt;- diamonds %&gt;%\n  mutate(across(c(cut, color, clarity), ~ factor(.x, ordered = FALSE)))\n\nLet us first define the training and testing datasets. We can randomly divide the data into the two datasets. We use 70% for the training dataset because we have a really big sample here.\n\nset.seed(470500)\n\n# Create an ID variable\ndiamonds &lt;- diamonds %&gt;%\n  mutate(id = 1:nrow(diamonds))\n\n# Randomly select 70% to the training dataset\ntrain_ids &lt;- sample(diamonds$id, size = 0.7 * nrow(diamonds))\n\ntrain &lt;- diamonds[(diamonds$id %in% train_ids), ]\ntest &lt;- diamonds[!(diamonds$id %in% train_ids), ]\n\nNow, we can use the training dataset to estimate the regression of price on characteristics.\n\ntrain_lm &lt;- lm(price ~ carat + cut + color + \n                 clarity + depth + table + x + y + z,\n               data = diamonds)\n\nsummary(train_lm)\n\n\nCall:\nlm(formula = price ~ carat + cut + color + clarity + depth + \n    table + x + y + z, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21376.0   -592.4   -183.5    376.4  10694.2 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2184.477    408.197   5.352 8.76e-08 ***\ncarat        11256.978     48.628 231.494  &lt; 2e-16 ***\ncutGood        579.751     33.592  17.259  &lt; 2e-16 ***\ncutVery Good   726.783     32.241  22.542  &lt; 2e-16 ***\ncutPremium     762.144     32.228  23.649  &lt; 2e-16 ***\ncutIdeal       832.912     33.407  24.932  &lt; 2e-16 ***\ncolorE        -209.118     17.893 -11.687  &lt; 2e-16 ***\ncolorF        -272.854     18.093 -15.081  &lt; 2e-16 ***\ncolorG        -482.039     17.716 -27.209  &lt; 2e-16 ***\ncolorH        -980.267     18.836 -52.043  &lt; 2e-16 ***\ncolorI       -1466.244     21.162 -69.286  &lt; 2e-16 ***\ncolorJ       -2369.398     26.131 -90.674  &lt; 2e-16 ***\nclaritySI2    2702.586     43.818  61.677  &lt; 2e-16 ***\nclaritySI1    3665.472     43.634  84.005  &lt; 2e-16 ***\nclarityVS2    4267.224     43.853  97.306  &lt; 2e-16 ***\nclarityVS1    4578.398     44.546 102.779  &lt; 2e-16 ***\nclarityVVS2   4950.814     45.855 107.967  &lt; 2e-16 ***\nclarityVVS1   5007.759     47.160 106.187  &lt; 2e-16 ***\nclarityIF     5345.102     51.024 104.757  &lt; 2e-16 ***\ndepth          -63.806      4.535 -14.071  &lt; 2e-16 ***\ntable          -26.474      2.912  -9.092  &lt; 2e-16 ***\nx            -1008.261     32.898 -30.648  &lt; 2e-16 ***\ny                9.609     19.333   0.497    0.619    \nz              -50.119     33.486  -1.497    0.134    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1130 on 53916 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9198 \nF-statistic: 2.688e+04 on 23 and 53916 DF,  p-value: &lt; 2.2e-16\n\n\nTo test our model, we can use predict() to predict the price in the testing data with our estimated coefficients.\n\ntest_pred &lt;- predict(train_lm, newdata = test)\nsummary(test_pred)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -4308    1089    2819    3937    5879   39394 \n\n(test$price - test_pred)^2 %&gt;%\n  mean()\n\n[1] 1261473\n\n\nThis does not mean too much to us unless we compare to another model.\n\ntrain_lm2 &lt;- lm(price ~ carat + cut + color + clarity,\n               data = diamonds)\n\nsummary(train_lm2)\n\n\nCall:\nlm(formula = price ~ carat + cut + color + clarity, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16813.5   -680.4   -197.6    466.4  10394.9 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -7362.80      51.68 -142.46   &lt;2e-16 ***\ncarat         8886.13      12.03  738.44   &lt;2e-16 ***\ncutGood        655.77      33.63   19.50   &lt;2e-16 ***\ncutVery Good   848.72      31.28   27.14   &lt;2e-16 ***\ncutPremium     869.40      30.93   28.11   &lt;2e-16 ***\ncutIdeal       998.25      30.66   32.56   &lt;2e-16 ***\ncolorE        -211.68      18.32  -11.56   &lt;2e-16 ***\ncolorF        -303.31      18.51  -16.39   &lt;2e-16 ***\ncolorG        -506.20      18.12  -27.93   &lt;2e-16 ***\ncolorH        -978.70      19.27  -50.78   &lt;2e-16 ***\ncolorI       -1440.30      21.65  -66.54   &lt;2e-16 ***\ncolorJ       -2325.22      26.72  -87.01   &lt;2e-16 ***\nclaritySI2    2625.95      44.79   58.63   &lt;2e-16 ***\nclaritySI1    3573.69      44.60   80.13   &lt;2e-16 ***\nclarityVS2    4217.83      44.84   94.06   &lt;2e-16 ***\nclarityVS1    4534.88      45.54   99.59   &lt;2e-16 ***\nclarityVVS2   4967.20      46.89  105.93   &lt;2e-16 ***\nclarityVVS1   5072.03      48.21  105.20   &lt;2e-16 ***\nclarityIF     5419.65      52.14  103.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1157 on 53921 degrees of freedom\nMultiple R-squared:  0.9159,    Adjusted R-squared:  0.9159 \nF-statistic: 3.264e+04 on 18 and 53921 DF,  p-value: &lt; 2.2e-16\n\ntest_pred2 &lt;- predict(train_lm2, newdata = test)\n\n(test$price - test_pred2)^2 %&gt;%\n  mean()\n\n[1] 1328692\n\n\nComparing the MSE between these two models, the first model has better predictive power. But what about other models? What about all the possible combinations of covariates? What about adding polynomials of those covariates? ML methods can help with that too (below).\n\n\n21.1.2 Logistic Regression\nWe can apply the same logic when the outcome is binary, but we use logistic regression. The model predicts the probability of success, when the outcome is one. Suppose we want to predict if a diamond has a price below someone’s budget of $1,000.\n\ntrain &lt;- train %&gt;%\n  mutate(inbudget = (price &lt;= 1000))\n\ntest &lt;- test %&gt;%\n  mutate(inbudget = (price &lt;= 1000))\n\n\n# Train\nib_train_lm &lt;- glm(inbudget ~ carat + cut + color + clarity + depth + table + price + x + y + z,\n    data = train,\n    family = \"binomial\")\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(ib_train_lm)\n\n\nCall:\nglm(formula = inbudget ~ carat + cut + color + clarity + depth + \n    table + price + x + y + z, family = \"binomial\", data = train)\n\nCoefficients:\n               Estimate Std. Error    z value Pr(&gt;|z|)    \n(Intercept)   1.199e+16  2.871e+07  417678354   &lt;2e-16 ***\ncarat        -1.145e+15  4.777e+06 -239754796   &lt;2e-16 ***\ncutGood       2.742e+13  2.394e+06   11454293   &lt;2e-16 ***\ncutVery Good -1.798e+13  2.304e+06   -7801079   &lt;2e-16 ***\ncutPremium   -1.965e+14  2.307e+06  -85191457   &lt;2e-16 ***\ncutIdeal     -1.899e+14  2.388e+06  -79502937   &lt;2e-16 ***\ncolorE        1.142e+14  1.271e+06   89862090   &lt;2e-16 ***\ncolorF        2.088e+14  1.282e+06  162806470   &lt;2e-16 ***\ncolorG        3.120e+14  1.266e+06  246437038   &lt;2e-16 ***\ncolorH        4.192e+14  1.367e+06  306732002   &lt;2e-16 ***\ncolorI        6.511e+14  1.563e+06  416413920   &lt;2e-16 ***\ncolorJ        7.919e+14  1.998e+06  396405362   &lt;2e-16 ***\nclaritySI2   -8.918e+14  3.187e+06 -279870015   &lt;2e-16 ***\nclaritySI1   -1.073e+15  3.264e+06 -328657932   &lt;2e-16 ***\nclarityVS2   -1.340e+15  3.346e+06 -400416873   &lt;2e-16 ***\nclarityVS1   -1.424e+15  3.429e+06 -415227268   &lt;2e-16 ***\nclarityVVS2  -1.591e+15  3.565e+06 -446331901   &lt;2e-16 ***\nclarityVVS1  -1.911e+15  3.653e+06 -523092303   &lt;2e-16 ***\nclarityIF    -2.321e+15  3.951e+06 -587473604   &lt;2e-16 ***\ndepth        -3.769e+13  3.159e+05 -119307594   &lt;2e-16 ***\ntable        -1.878e+13  2.080e+05  -90303057   &lt;2e-16 ***\nprice         2.039e+11  3.049e+02  668533089   &lt;2e-16 ***\nx            -1.344e+15  2.194e+06 -612489770   &lt;2e-16 ***\ny             6.368e+13  1.294e+06   49228723   &lt;2e-16 ***\nz            -3.354e+14  2.044e+06 -164050373   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance:  44034  on 37757  degrees of freedom\nResidual deviance: 135524  on 37733  degrees of freedom\nAIC: 135574\n\nNumber of Fisher Scoring iterations: 25\n\n# Predict in testing data\nib_test_pred &lt;- predict(ib_train_lm, newdata = test, type = \"response\")\n\nsummary(ib_test_pred)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2444  0.0000  1.0000 \n\nib_test_pred_binary &lt;- (ib_test_pred &gt; 0.5)\n\n# Confusion matrix \ntable(observed = test$inbudget, predicted = ib_test_pred_binary)\n\n        predicted\nobserved FALSE  TRUE\n   FALSE 11634   213\n   TRUE    593  3742\n\n# Accuracy\nmean(test$inbudget == ib_test_pred_binary)\n\n[1] 0.9501916\n\n\n\n\n21.1.3 LASSO\nUnder that intuition, ML methods can help us find a model (what to put on the right-hand side) for the best prediction (what is on the left-hand side). We will go through the workflow of LASSO, which is one method you can use.\n\nSplit the cleaned data into training and testing. We can use the tidymodels library to split the data into training and testing datsaets.\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tibble       3.2.1\n✔ infer        1.0.8     ✔ tidyr        1.3.1\n✔ modeldata    1.4.0     ✔ tune         1.3.0\n✔ parsnip      1.3.1     ✔ workflows    1.2.0\n✔ purrr        1.0.4     ✔ workflowsets 1.1.0\n✔ recipes      1.2.1     ✔ yardstick    1.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()   masks scales::discard()\n✖ tidyr::extract()   masks magrittr::extract()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ purrr::set_names() masks magrittr::set_names()\n✖ recipes::step()    masks stats::step()\n\nd_split &lt;- initial_split(diamonds) # Creates the split (default is 75/25)\n\nd_train &lt;- training(d_split) # Define training data based on split\nd_test &lt;- testing(d_split) # Definet testing data based on split\n\n\nBuild a “recipe.” This tells R what is the outcome variable and helps format the possible predictors.\n\n\nrecipe(price ~ ., data = d_train) %&gt;%\n  summary()\n\n# A tibble: 11 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 carat    &lt;chr [2]&gt; predictor original\n 2 cut      &lt;chr [3]&gt; predictor original\n 3 color    &lt;chr [3]&gt; predictor original\n 4 clarity  &lt;chr [3]&gt; predictor original\n 5 depth    &lt;chr [2]&gt; predictor original\n 6 table    &lt;chr [2]&gt; predictor original\n 7 x        &lt;chr [2]&gt; predictor original\n 8 y        &lt;chr [2]&gt; predictor original\n 9 z        &lt;chr [2]&gt; predictor original\n10 id       &lt;chr [2]&gt; predictor original\n11 price    &lt;chr [2]&gt; outcome   original\n\n# Account for ID variable and factor variables\n\nd_rec &lt;- recipe(price ~ ., data = d_train) %&gt;%\n  step_dummy(all_nominal_predictors())  %&gt;% \n  update_role(id, new_role = \"ID\") \n\nsummary(d_rec)\n\n# A tibble: 11 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 carat    &lt;chr [2]&gt; predictor original\n 2 cut      &lt;chr [3]&gt; predictor original\n 3 color    &lt;chr [3]&gt; predictor original\n 4 clarity  &lt;chr [3]&gt; predictor original\n 5 depth    &lt;chr [2]&gt; predictor original\n 6 table    &lt;chr [2]&gt; predictor original\n 7 x        &lt;chr [2]&gt; predictor original\n 8 y        &lt;chr [2]&gt; predictor original\n 9 z        &lt;chr [2]&gt; predictor original\n10 id       &lt;chr [2]&gt; ID        original\n11 price    &lt;chr [2]&gt; outcome   original\n\n\n\n“Prep” the recipe. Given the recipe, R then puts together the data inputs. You can explore the outputs of recipe and prep to better understand these steps.\n\n\nd_prep &lt;- d_rec %&gt;%\n  prep()\n\nsummary(d_rec)\n\n# A tibble: 11 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 carat    &lt;chr [2]&gt; predictor original\n 2 cut      &lt;chr [3]&gt; predictor original\n 3 color    &lt;chr [3]&gt; predictor original\n 4 clarity  &lt;chr [3]&gt; predictor original\n 5 depth    &lt;chr [2]&gt; predictor original\n 6 table    &lt;chr [2]&gt; predictor original\n 7 x        &lt;chr [2]&gt; predictor original\n 8 y        &lt;chr [2]&gt; predictor original\n 9 z        &lt;chr [2]&gt; predictor original\n10 id       &lt;chr [2]&gt; ID        original\n11 price    &lt;chr [2]&gt; outcome   original\n\n\n\nSpecify parameters of the model. The basic idea of model selection is that we have an objective function with a penalty term. This penalty term depends on the number of predictors. If we include more predictors, the penalty increases. This prevents us just using every predictor available. Then the question is what parameters do we choose for our objective function? Here is the general format for LASSO:\n\n\\[\\begin{equation}\n\\arg\\min_{\\beta} \\sum_{i = 1}^N (Y_i - \\beta ' X_i)^2 + \\lambda \\sum_{k=1}^K \\vert \\beta_k \\vert\n\\end{equation}\\]\nWe can dissect this term by term. The first term in Equation (1) is the usual OLS estimator. The second term is the penalty. The parameter \\(\\lambda\\) is the first parameter we need to set. This itself is called the penalty term. Large values of \\(\\lambda\\) results in a more parsimonious model (fewer predictors) because the penalty is large. If \\(\\lambda\\) is zero, then the model is the same as OLS. In practice, they can cover a large range. See methods in cross-validation to select \\(\\lambda\\) in a more principled way than just setting it. Then, there is the function on the parameters \\(\\beta\\). Suppose there are \\(K\\) possible predictors. LASSO involves taking the absolute value of the parameters. Other methods, like Ridge regression (not in these notes), use other functions rather than the absolute value (e.g., square the parameters). This function is the second part that the analyst sets.\nIn the code below, the penalty argument is \\(\\lambda\\) and the mixture argument set to 1 means that we are using LASSO. We use set_engine() to tell R which algorithm to use. Because we are using LASSO, we use “glmnet.” This also works for Ridge or similar methods. Other engines include “lm” for OLS or “keras” for Neural Nets.\n\nd_lasso &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\nCreate the workflow. This puts together the model and the recipe.\n\n\nd_wf &lt;- workflow() %&gt;%\n  add_recipe(d_rec)\n\n\nFit the model and investigate the coefficients. If a coefficient is 0, that means that LASSO dropped it because it did not add sufficient predictive power. The interpretation of the coefficients is the same as for standard linear regression.\n\n\n# Run LASSO!\nd_fit &lt;- d_wf %&gt;%\n  add_model(d_lasso) %&gt;%\n  fit(data = d_train)\n\n# Check out coefficients\nd_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() \n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n\n# A tibble: 24 × 3\n   term          estimate penalty\n   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)     2338.      0.1\n 2 carat          11015.      0.1\n 3 depth            -64.7     0.1\n 4 table            -28.5     0.1\n 5 x               -912.      0.1\n 6 y                  0       0.1\n 7 z                -45.9     0.1\n 8 cut_Good         537.      0.1\n 9 cut_Very.Good    690.      0.1\n10 cut_Premium      712.      0.1\n# ℹ 14 more rows\n\n\n\nApply to testing data and calculate the MSE (or other metric). If we were to use other models or tuning parameters, then this MSE can be used to compare the models.\n\n\n# Apply recipe and model to test data\nd_results &lt;- predict(d_fit, new_data = d_test) %&gt;%\n  bind_cols(d_test)\n\n# Assess MSE\nmean((d_results$.pred - d_results$price)^2)\n\n[1] 1263936\n\n\nHere is a visualization of how the coefficients change with different parameters \\(\\lambda\\).\n\n# Raw glmnet object\nglmnet_fit &lt;- d_fit %&gt;%\n  extract_fit_engine() \n\n# Clean coefficient paths\n# Install broom!\nlibrary(broom)\ncoefs_long &lt;- tidy(glmnet_fit, return_zeros = TRUE, matrix = \"beta\")  \n\n# Plot\nggplot(coefs_long, aes(x = log(lambda), y = estimate, color = term)) +\n  geom_line(alpha = 0.8) +\n  labs(title = \"LASSO Coefficient Paths (Workflow Model)\",\n       x = \"log(Lambda)\",\n       y = \"Coefficient Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n21.1.4 Other Tools\nDecision trees and the more general random forest methods provide other ways to predict. There are various packages to implement these methods in R, including rpart. One issue that arises in all these methods is overfitting. This means that the model performs very well in the training data, but not so well in the testing data. Cross-validation is a tool that addresses this issue. The basic idea of cross-validation is that the split of the data is not so simplistic into training/testing. Instead, the data is repeatedly divided into multiple subsets (or “folds”), so that each part of the data is used for both training and testing at different stages. For example, in 5-fold cross-validation, the data is split into five parts: the model is trained on four and tested on the fifth, and this process repeats five times. The average performance across all folds gives a more reliable estimate of how the model is likely to perform on new, unseen data.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Machine Learning Basics</span>"
    ]
  },
  {
    "objectID": "21_MachineLearning.html#unsupervised-learning",
    "href": "21_MachineLearning.html#unsupervised-learning",
    "title": "21  Machine Learning Basics",
    "section": "21.2 Unsupervised Learning",
    "text": "21.2 Unsupervised Learning\nA common application of unsupervised learning is “clustering” or “pattern recognition.” The most foundational approach is the k-means algorithm, which attempts to group observations into \\(k\\) clusters based on similarity. The intuition behind k-means is simple: the algorithm assigns each observation to the nearest cluster center, then updates the cluster centers to be the average of the observations assigned to them. This process repeats until the assignments stop changing. The result is a division of the data into groups that are internally similar but distinct from one another. While it is fast and widely used, it does require the user to specify the number of clusters in advance. K-means and similar tools are useful for summarizing large datasets, detecting outliers, or identifying natural groupings when no outcome variable is available.\n\n# Load built-in iris dataset\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Remove the species label for unsupervised learning\niris_data &lt;- iris[, 1:4]\n\n# Run k-means with 3 clusters (we know there are 3 species)\nset.seed(470500)\niris_kmeans &lt;- kmeans(iris_data, centers = 3, nstart = 20)\n\n# Add the cluster labels to the original data\niris_clustered &lt;- iris %&gt;%\n  mutate(cluster = as.factor(iris_kmeans$cluster))\n\n# Plot the clusters (e.g., using petal length vs. width)\nlibrary(ggplot2)\nggplot(iris_clustered, aes(x = Petal.Length, y = Petal.Width, color = cluster)) +\n  geom_point()",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Machine Learning Basics</span>"
    ]
  },
  {
    "objectID": "21_MachineLearning.html#further-reading",
    "href": "21_MachineLearning.html#further-reading",
    "title": "21  Machine Learning Basics",
    "section": "21.3 Further Reading",
    "text": "21.3 Further Reading\nThere are so many great resources online to learn ML methods. Athey and Imbens (2019) provides a nice overview on the classes of methods. Text analysis is another class of machne learning that is useful. You can use the libraries tidytext or tm for that.\n\n\n\n\n\nAthey, Susan, and Guido W. Imbens. 2019. “Machine Learning Methods That Economists Should Know About.” Annual Review of Economics 11 (1): 685–725. https://doi.org/10.1146/annurev-economics-080217-053433.",
    "crumbs": [
      "Statistical Methods and Econometrics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Machine Learning Basics</span>"
    ]
  }
]